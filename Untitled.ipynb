{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result = pd.read_csv('game_result_20162018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#オールスター除外\n",
    "game_result = game_result[(game_result['home_team'] != 'パシフィック・リーグ') & (game_result['away_team'] != 'パシフィック・リーグ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result['date'] = game_result['date'].str.extract('(.+日)（', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "game_result['date'] = pd.to_datetime(game_result['date'], format='%Y年%m月%d日')\n",
    "game_result['year'] = game_result['date'].dt.strftime(\"%Y\").astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_year_result = pd.read_csv('game_year_result.csv')\n",
    "game_year_result = game_year_result[['year','team','win_rate']]\n",
    "game_year_result['year'] += 1\n",
    "game_result = pd.merge(game_result, game_year_result.rename(columns={'team':'home_team','win_rate':'last_year_home_win_rate'}), on=['year','home_team'], how='left')\n",
    "game_result = pd.merge(game_result, game_year_result.rename(columns={'team':'away_team','win_rate':'last_year_away_win_rate'}), on=['year','away_team'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result['home_all_batter_at_bat'] = game_result['home_batter1_at_bat'] + game_result['home_batter2_at_bat'] + game_result['home_batter3_at_bat']\\\n",
    "+ game_result['home_batter4_at_bat'] + game_result['home_batter5_at_bat'] + game_result['home_batter6_at_bat'] + game_result['home_batter7_at_bat']\\\n",
    " + game_result['home_batter8_at_bat'] + game_result['home_batter9_at_bat'] + game_result['home_other_batters_at_bat']\n",
    "\n",
    "game_result['home_all_batter_hit'] = game_result['home_batter1_hit'] + game_result['home_batter2_hit'] + game_result['home_batter3_hit']\\\n",
    "+ game_result['home_batter4_hit'] + game_result['home_batter5_hit'] + game_result['home_batter6_hit'] + game_result['home_batter7_hit']\\\n",
    " + game_result['home_batter8_hit'] + game_result['home_batter9_hit'] + game_result['home_other_batters_hit']\n",
    "\n",
    "game_result['home_all_batter_two_hit'] = game_result['home_batter1_two_hit'] + game_result['home_batter2_two_hit'] + game_result['home_batter3_two_hit']\\\n",
    "+ game_result['home_batter4_two_hit'] + game_result['home_batter5_two_hit'] + game_result['home_batter6_two_hit'] + game_result['home_batter7_two_hit']\\\n",
    " + game_result['home_batter8_two_hit'] + game_result['home_batter9_two_hit'] + game_result['home_other_batters_two_hit']\n",
    "\n",
    "game_result['home_all_batter_three_hit'] = game_result['home_batter1_three_hit'] + game_result['home_batter2_three_hit'] + game_result['home_batter3_three_hit']\\\n",
    "+ game_result['home_batter4_three_hit'] + game_result['home_batter5_three_hit'] + game_result['home_batter6_three_hit'] + game_result['home_batter7_three_hit']\\\n",
    " + game_result['home_batter8_three_hit'] + game_result['home_batter9_three_hit'] + game_result['home_other_batters_three_hit']\n",
    "\n",
    "game_result['home_all_batter_homerun'] = game_result['home_batter1_homerun'] + game_result['home_batter2_homerun'] + game_result['home_batter3_homerun']\\\n",
    "+ game_result['home_batter4_homerun'] + game_result['home_batter5_homerun'] + game_result['home_batter6_homerun'] + game_result['home_batter7_homerun']\\\n",
    " + game_result['home_batter8_homerun'] + game_result['home_batter9_homerun'] + game_result['home_other_batters_homerun']\n",
    "\n",
    "game_result['home_all_batter_four_balls'] = game_result['home_batter1_four_balls'] + game_result['home_batter2_four_balls'] + game_result['home_batter3_four_balls']\\\n",
    "+ game_result['home_batter4_four_balls'] + game_result['home_batter5_four_balls'] + game_result['home_batter6_four_balls'] + game_result['home_batter7_four_balls']\\\n",
    " + game_result['home_batter8_four_balls'] + game_result['home_batter9_four_balls'] + game_result['home_other_batters_four_balls']\n",
    "\n",
    "game_result['home_all_batter_dead_ball'] = game_result['home_batter1_dead_ball'] + game_result['home_batter2_dead_ball'] + game_result['home_batter3_dead_ball']\\\n",
    "+ game_result['home_batter4_dead_ball'] + game_result['home_batter5_dead_ball'] + game_result['home_batter6_dead_ball'] + game_result['home_batter7_dead_ball']\\\n",
    "+ game_result['home_batter8_dead_ball'] + game_result['home_batter9_dead_ball'] + game_result['home_other_batters_dead_ball']\n",
    "\n",
    "game_result['home_all_batter_steal'] = game_result['home_batter1_steal'] + game_result['home_batter2_steal'] + game_result['home_batter3_steal']\\\n",
    "+ game_result['home_batter4_steal'] + game_result['home_batter5_steal'] + game_result['home_batter6_steal'] + game_result['home_batter7_steal']\\\n",
    "+ game_result['home_batter8_steal'] + game_result['home_batter9_steal'] + game_result['home_other_batters_steal']\n",
    "\n",
    "game_result['home_all_batter_strikeout'] = game_result['home_batter1_strikeout'] + game_result['home_batter2_strikeout'] + game_result['home_batter3_strikeout']\\\n",
    "+ game_result['home_batter4_strikeout'] + game_result['home_batter5_strikeout'] + game_result['home_batter6_strikeout'] + game_result['home_batter7_strikeout']\\\n",
    "+ game_result['home_batter8_strikeout'] + game_result['home_batter9_strikeout'] + game_result['home_other_batters_strikeout']\n",
    "\n",
    "game_result['home_all_batter_sacrifice_hit'] = game_result['home_batter1_sacrifice_hit'] + game_result['home_batter2_sacrifice_hit'] + game_result['home_batter3_sacrifice_hit']\\\n",
    "+ game_result['home_batter4_sacrifice_hit'] + game_result['home_batter5_sacrifice_hit'] + game_result['home_batter6_sacrifice_hit'] + game_result['home_batter7_sacrifice_hit']\\\n",
    "+ game_result['home_batter8_sacrifice_hit'] + game_result['home_batter9_sacrifice_hit'] + game_result['home_other_batters_sacrifice_hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result['away_all_batter_at_bat'] = game_result['away_batter1_at_bat'] + game_result['away_batter2_at_bat'] + game_result['away_batter3_at_bat']\\\n",
    "+ game_result['away_batter4_at_bat'] + game_result['away_batter5_at_bat'] + game_result['away_batter6_at_bat'] + game_result['away_batter7_at_bat']\\\n",
    " + game_result['away_batter8_at_bat'] + game_result['away_batter9_at_bat'] + game_result['away_other_batters_at_bat']\n",
    "\n",
    "game_result['away_all_batter_hit'] = game_result['away_batter1_hit'] + game_result['away_batter2_hit'] + game_result['away_batter3_hit']\\\n",
    "+ game_result['away_batter4_hit'] + game_result['away_batter5_hit'] + game_result['away_batter6_hit'] + game_result['away_batter7_hit']\\\n",
    " + game_result['away_batter8_hit'] + game_result['away_batter9_hit'] + game_result['away_other_batters_hit']\n",
    "\n",
    "game_result['away_all_batter_two_hit'] = game_result['away_batter1_two_hit'] + game_result['away_batter2_two_hit'] + game_result['away_batter3_two_hit']\\\n",
    "+ game_result['away_batter4_two_hit'] + game_result['away_batter5_two_hit'] + game_result['away_batter6_two_hit'] + game_result['away_batter7_two_hit']\\\n",
    " + game_result['away_batter8_two_hit'] + game_result['away_batter9_two_hit'] + game_result['away_other_batters_two_hit']\n",
    "\n",
    "game_result['away_all_batter_three_hit'] = game_result['away_batter1_three_hit'] + game_result['away_batter2_three_hit'] + game_result['away_batter3_three_hit']\\\n",
    "+ game_result['away_batter4_three_hit'] + game_result['away_batter5_three_hit'] + game_result['away_batter6_three_hit'] + game_result['away_batter7_three_hit']\\\n",
    " + game_result['away_batter8_three_hit'] + game_result['away_batter9_three_hit'] + game_result['away_other_batters_three_hit']\n",
    "\n",
    "game_result['away_all_batter_homerun'] = game_result['away_batter1_homerun'] + game_result['away_batter2_homerun'] + game_result['away_batter3_homerun']\\\n",
    "+ game_result['away_batter4_homerun'] + game_result['away_batter5_homerun'] + game_result['away_batter6_homerun'] + game_result['away_batter7_homerun']\\\n",
    " + game_result['away_batter8_homerun'] + game_result['away_batter9_homerun'] + game_result['away_other_batters_homerun']\n",
    "\n",
    "game_result['away_all_batter_four_balls'] = game_result['away_batter1_four_balls'] + game_result['away_batter2_four_balls'] + game_result['away_batter3_four_balls']\\\n",
    "+ game_result['away_batter4_four_balls'] + game_result['away_batter5_four_balls'] + game_result['away_batter6_four_balls'] + game_result['away_batter7_four_balls']\\\n",
    " + game_result['away_batter8_four_balls'] + game_result['away_batter9_four_balls'] + game_result['away_other_batters_four_balls']\n",
    "\n",
    "game_result['away_all_batter_dead_ball'] = game_result['away_batter1_dead_ball'] + game_result['away_batter2_dead_ball'] + game_result['away_batter3_dead_ball']\\\n",
    "+ game_result['away_batter4_dead_ball'] + game_result['away_batter5_dead_ball'] + game_result['away_batter6_dead_ball'] + game_result['away_batter7_dead_ball']\\\n",
    "+ game_result['away_batter8_dead_ball'] + game_result['away_batter9_dead_ball'] + game_result['away_other_batters_dead_ball']\n",
    "\n",
    "game_result['away_all_batter_steal'] = game_result['away_batter1_steal'] + game_result['away_batter2_steal'] + game_result['away_batter3_steal']\\\n",
    "+ game_result['away_batter4_steal'] + game_result['away_batter5_steal'] + game_result['away_batter6_steal'] + game_result['away_batter7_steal']\\\n",
    " + game_result['away_batter8_steal'] + game_result['away_batter9_steal'] + game_result['away_other_batters_steal']\n",
    "\n",
    "game_result['away_all_batter_strikeout'] = game_result['away_batter1_strikeout'] + game_result['away_batter2_strikeout'] + game_result['away_batter3_strikeout']\\\n",
    "+ game_result['away_batter4_strikeout'] + game_result['away_batter5_strikeout'] + game_result['away_batter6_strikeout'] + game_result['away_batter7_strikeout']\\\n",
    " + game_result['away_batter8_strikeout'] + game_result['away_batter9_strikeout'] + game_result['away_other_batters_strikeout']\n",
    "\n",
    "game_result['away_all_batter_sacrifice_hit'] = game_result['away_batter1_sacrifice_hit'] + game_result['away_batter2_sacrifice_hit'] + game_result['away_batter3_sacrifice_hit']\\\n",
    "+ game_result['away_batter4_sacrifice_hit'] + game_result['away_batter5_sacrifice_hit'] + game_result['away_batter6_sacrifice_hit'] + game_result['away_batter7_sacrifice_hit']\\\n",
    " + game_result['away_batter8_sacrifice_hit'] + game_result['away_batter9_sacrifice_hit'] + game_result['away_other_batters_sacrifice_hit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_append_columns = ['last_week_home_all_batter_at_bat', 'last_week_home_all_batter_hit', 'last_week_home_all_batter_two_hit'\n",
    "    , 'last_week_home_all_batter_three_hit', 'last_week_home_all_batter_homerun', 'last_week_home_all_batter_four_balls'\n",
    "    , 'last_week_home_all_batter_dead_ball', 'last_week_home_all_batter_steal', 'last_week_home_all_batter_strikeout'\n",
    "    , 'last_week_home_all_batter_sacrifice_hit']\n",
    "home_append_columns2 = ['last_week_home_victory', 'last_week_home_draw', 'last_week_home_lose', 'last_week_home_runs', 'last_week_home_runs_allowed']\n",
    "home_append_columns3 = ['last_3_week_home_st_APP', 'last_3_week_home_st_ERA', 'last_3_week_home_st_IP_ave', 'last_year_home_st_APP'\n",
    "    , 'last_year_home_st_ERA', 'last_year_home_st_IP_ave', 'last_2_year_home_st_APP', 'last_2_year_home_st_ERA', 'last_2_year_home_st_IP_ave'\n",
    "    , 'last_week_home_re_times', 'last_week_home_re_ERA']\n",
    "for append_column in home_append_columns + home_append_columns2:\n",
    "    game_result[append_column] = 0\n",
    "for append_column in home_append_columns3:\n",
    "    game_result[append_column] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "away_append_columns = ['last_week_away_all_batter_at_bat', 'last_week_away_all_batter_hit', 'last_week_away_all_batter_two_hit'\n",
    "    , 'last_week_away_all_batter_three_hit', 'last_week_away_all_batter_homerun', 'last_week_away_all_batter_four_balls'\n",
    "    , 'last_week_away_all_batter_dead_ball', 'last_week_away_all_batter_steal', 'last_week_away_all_batter_strikeout'\n",
    "    , 'last_week_away_all_batter_sacrifice_hit']\n",
    "away_append_columns2 = ['last_week_away_victory', 'last_week_away_draw', 'last_week_away_lose', 'last_week_away_runs', 'last_week_away_runs_allowed']\n",
    "away_append_columns3 = [ 'last_3_week_away_st_APP', 'last_3_week_away_st_ERA', 'last_3_week_away_st_IP_ave', 'last_year_away_st_APP'\n",
    "    , 'last_year_away_st_ERA', 'last_year_away_st_IP_ave', 'last_2_year_away_st_APP', 'last_2_year_away_st_ERA', 'last_2_year_away_st_IP_ave'\n",
    "    , 'last_week_away_re_times', 'last_week_away_re_ERA','last_week_home_st_ERA','last_week_home_st_IP_ave','last_week_away_st_ERA','last_week_away_st_IP_ave']\n",
    "for append_column in away_append_columns + away_append_columns2:\n",
    "    game_result[append_column] = 0\n",
    "for append_column in away_append_columns3:\n",
    "    game_result[append_column] = 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_achivements = pd.read_csv('personal_achivements2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num(str) -> float:\n",
    "    extr = re.compile('\\d*[.,]?\\d*')\n",
    "    return float(extr.search(str).group())\n",
    "\n",
    "def baseball_times_converse(times) -> float:\n",
    "    after_decimal_point = times - int(times)\n",
    "    return float((int(times) * 3 + int(after_decimal_point * 10)) /3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_ERA = 4\n",
    "for index, row in game_result.iterrows():\n",
    "    now_date = row['date']\n",
    "    now_year = now_date.year\n",
    "    td_week = datetime.timedelta(weeks=1)\n",
    "    td_3_week = datetime.timedelta(weeks=3)\n",
    "    home_team = row['home_team']\n",
    "    away_team = row['away_team']\n",
    "    if game_result.at[index, 'home_runs'] == game_result.at[index, 'away_runs']:\n",
    "        game_result.at[index, 'draw'] = 1\n",
    "\n",
    "    # home_team\n",
    "    game_count = len(game_result[(now_date - td_week <= game_result['date']) & (now_date > game_result['date']) & ((home_team == game_result['home_team']) | (home_team == game_result['away_team']))])\n",
    "    for last_week_index, last_week_row in game_result[(now_date - td_week <= game_result['date']) & (now_date > game_result['date']) & ((home_team == game_result['home_team']) | (home_team == game_result['away_team']))].iterrows():\n",
    "        for home_append_column, away_append_column in zip(home_append_columns, away_append_columns):\n",
    "            if home_team == last_week_row['home_team']:\n",
    "                game_result.at[index, home_append_column] += last_week_row[home_append_column[10:]]\n",
    "            else:\n",
    "                game_result.at[index, home_append_column] += last_week_row[away_append_column[10:]]\n",
    "        if home_team == last_week_row['home_team']:\n",
    "            # win or lose\n",
    "            game_result.at[index, 'last_week_home_victory'] += last_week_row['home_victory']\n",
    "            if int(last_week_row['home_runs']) == int(last_week_row['away_runs']):\n",
    "                game_result.at[index, 'last_week_home_draw'] += 1\n",
    "            else:\n",
    "                game_result.at[index, 'last_week_home_lose'] += abs(last_week_row['home_victory'] - 1)\n",
    "            game_result.at[index, 'last_week_home_runs'] += last_week_row['home_runs']\n",
    "            game_result.at[index, 'last_week_home_runs_allowed'] += last_week_row['away_runs']\n",
    "            # relief\n",
    "            game_result.at[index, 'last_week_home_re_times'] += baseball_times_converse(extract_num(str(last_week_row['home_re_times'])))\n",
    "            game_result.at[index, 'last_week_home_re_ERA'] += int(last_week_row['home_re_responsible_runs'])\n",
    "\n",
    "        else:\n",
    "            # win or lose\n",
    "            game_result.at[index, 'last_week_home_victory'] += abs(last_week_row['home_victory'] - 1)\n",
    "            if int(last_week_row['home_runs']) == int(last_week_row['away_runs']):\n",
    "                game_result.at[index, 'last_week_home_draw'] += 1\n",
    "            else:\n",
    "                game_result.at[index, 'last_week_home_lose'] += last_week_row['home_victory']\n",
    "            game_result.at[index, 'last_week_home_runs'] += last_week_row['away_runs']\n",
    "            game_result.at[index, 'last_week_home_runs_allowed'] += last_week_row['home_runs']\n",
    "            # relief\n",
    "            game_result.at[index, 'last_week_home_re_times'] += baseball_times_converse(extract_num(str(last_week_row['away_re_times'])))\n",
    "            game_result.at[index, 'last_week_home_re_ERA'] += int(last_week_row['away_re_responsible_runs'])\n",
    "\n",
    "    # relief \n",
    "    game_result.at[index, 'last_week_home_re_ERA'] /= (game_result.at[index, 'last_week_home_re_times'] / 9)\n",
    "    \"\"\"\n",
    "    if game_result.at[index, 'last_week_home_re_times'] == 0:\n",
    "        initial_ERA = 4\n",
    "        game_result.at[index, 'last_week_home_re_ERA'] = initial_ERA\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    pitcher_url = row['home_st_url']\n",
    "    for last_week_index, last_week_row in game_result[(now_date - td_week <= game_result['date']) & (now_date > game_result['date']) \n",
    "    & ((pitcher_url == game_result['home_st_url']) | (pitcher_url == game_result['away_st_url']))].iterrows():\n",
    "        if(pitcher_url == last_week_row['home_st_url']):\n",
    "            game_result.at[index, 'last_week_home_st_ERA'] += int(last_week_row['home_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_week_home_st_IP_ave'] += baseball_times_converse(extract_num(last_week_row['home_st_times']))\n",
    "        else:\n",
    "            game_result.at[index, 'last_week_home_st_ERA'] += int(last_week_row['away_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_week_home_st_IP_ave'] += baseball_times_converse(extract_num(last_week_row['away_st_times']))         \n",
    "    game_result.at[index, 'last_week_home_st_ERA'] /= (game_result.at[index, 'last_week_home_st_IP_ave'] / 9) \n",
    "\n",
    "    pitcher_url = row['away_st_url']\n",
    "    for last_week_index, last_week_row in game_result[(now_date - td_week <= game_result['date']) & (now_date > game_result['date']) \n",
    "    & ((pitcher_url == game_result['home_st_url']) | (pitcher_url == game_result['away_st_url']))].iterrows():\n",
    "        if(pitcher_url == last_week_row['away_st_url']):\n",
    "            game_result.at[index, 'last_week_away_st_ERA'] += int(last_week_row['home_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_week_away_st_IP_ave'] += baseball_times_converse(extract_num(last_week_row['home_st_times']))\n",
    "        else:\n",
    "            game_result.at[index, 'last_week_away_st_ERA'] += int(last_week_row['away_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_week_away_st_IP_ave'] += baseball_times_converse(extract_num(last_week_row['away_st_times']))         \n",
    "    game_result.at[index, 'last_week_away_st_ERA'] /= (game_result.at[index, 'last_week_away_st_IP_ave'] / 9) \n",
    "    \"\"\"\n",
    "    \n",
    "    # last year\n",
    "    pitcher_url = row['home_st_url']\n",
    "    pitcher_achievements = personal_achivements[(pitcher_url == personal_achivements['url'])]\n",
    "    if (pitcher_achievements['pitching_appearance_'+str(now_year-1)].values[0] == \"-\") | (pitcher_achievements['pitching_appearance_'+str(now_year-1)].values[0] == \"0\"):\n",
    "        game_result.at[index, 'last_year_home_st_APP'] = 0\n",
    "        game_result.at[index, 'last_year_home_st_ERA'] = initial_ERA\n",
    "        game_result.at[index, 'last_year_home_st_IP_ave'] = 0\n",
    "    else:\n",
    "        game_result.at[index, 'last_year_home_st_IP_ave'] = baseball_times_converse(extract_num(pitcher_achievements['time_'+str(now_year-1)].values[0])) / float(pitcher_achievements['pitching_appearance_'+str(now_year-1)].values[0])\n",
    "        game_result.at[index, 'last_year_home_st_APP'] = pitcher_achievements['pitching_appearance_'+str(now_year-1)]\n",
    "        game_result.at[index, 'last_year_home_st_ERA'] = pitcher_achievements['earned_run_ave_'+str(now_year-1)]\n",
    "    \n",
    "    \n",
    "    # home_teamの過去三週間の集計, pitching\n",
    "    game_count = len(game_result[(now_date - td_3_week <= game_result['date']) & (now_date > game_result['date']) & ((pitcher_url == game_result['home_st_url']) | (pitcher_url == game_result['away_st_url']))])\n",
    "    game_result.at[index, 'last_3_week_home_st_APP'] = game_count\n",
    "    for last_3_week_index, last_3_week_row in game_result[(now_date - td_3_week <= game_result['date']) & (now_date > game_result['date']) \n",
    "    & ((pitcher_url == game_result['home_st_url']) | (pitcher_url == game_result['away_st_url']))].iterrows():\n",
    "        if(pitcher_url == last_3_week_row['home_st_url']):\n",
    "            game_result.at[index, 'last_3_week_home_st_ERA'] += int(last_3_week_row['home_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_3_week_home_st_IP_ave'] += baseball_times_converse(extract_num(last_3_week_row['home_st_times']))\n",
    "        else:\n",
    "            game_result.at[index, 'last_3_week_home_st_ERA'] += int(last_3_week_row['away_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_3_week_home_st_IP_ave'] += baseball_times_converse(extract_num(last_3_week_row['away_st_times']))         \n",
    "    if game_count == 0:\n",
    "        game_result.at[index, 'last_3_week_home_st_ERA'] = game_result.at[index, 'last_year_home_st_ERA']\n",
    "        game_result.at[index, 'last_3_week_home_st_IP_ave'] = 0\n",
    "    else:\n",
    "        game_result.at[index, 'last_3_week_home_st_ERA'] /= (game_result.at[index, 'last_3_week_home_st_IP_ave'] / 9)\n",
    "        game_result.at[index, 'last_3_week_home_st_IP_ave'] /= game_count\n",
    "\n",
    "    \"\"\"\n",
    "    # last 2 year\n",
    "    if (pitcher_achievements['pitching_appearance_'+str(now_year-2)].values[0] == \"-\") | (pitcher_achievements['pitching_appearance_'+str(now_year-2)].values[0] == \"0\"):\n",
    "        game_result.at[index, 'last_2_year_home_st_APP'] = 0\n",
    "        game_result.at[index, 'last_2_year_home_st_ERA'] = initial_ERA\n",
    "        game_result.at[index, 'last_2_year_home_st_IP_ave'] = 0\n",
    "    else:\n",
    "        game_result.at[index, 'last_2_year_home_st_IP_ave'] = baseball_times_converse(extract_num(pitcher_achievements['time_'+str(now_year-2)].values[0])) / float(pitcher_achievements['pitching_appearance_'+str(now_year-2)].values[0])\n",
    "        game_result.at[index, 'last_2_year_home_st_APP'] = pitcher_achievements['pitching_appearance_'+str(now_year-2)]\n",
    "        game_result.at[index, 'last_2_year_home_st_ERA'] = pitcher_achievements['earned_run_ave_'+str(now_year-2)]\n",
    "    \"\"\"\n",
    "\n",
    "    game_count = len(game_result[(now_date - td_week <= game_result['date']) & (now_date > game_result['date']) & ((away_team == game_result['home_team']) | (away_team == game_result['away_team']))])\n",
    "    # away_teamの過去一週間の集計\n",
    "    for last_week_index, last_week_row in game_result[(now_date - td_week <= game_result['date']) & (now_date > game_result['date']) & ((away_team == game_result['home_team']) | (away_team == game_result['away_team']))].iterrows():\n",
    "        # batting\n",
    "        for home_append_column, away_append_column in zip(home_append_columns, away_append_columns):\n",
    "            if away_team == last_week_row['home_team']:\n",
    "                game_result.at[index, away_append_column] += last_week_row[home_append_column[10:]]\n",
    "            else:\n",
    "                game_result.at[index, away_append_column] += last_week_row[away_append_column[10:]]\n",
    "        # win or lose\n",
    "        if away_team == last_week_row['home_team']:\n",
    "            if int(last_week_row['home_runs']) == int(last_week_row['away_runs']):\n",
    "                game_result.at[index, 'last_week_away_draw'] += 1\n",
    "            else:\n",
    "                game_result.at[index, 'last_week_away_victory'] += last_week_row['home_victory']\n",
    "                game_result.at[index, 'last_week_away_lose'] += abs(last_week_row['home_victory'] - 1)\n",
    "            game_result.at[index, 'last_week_away_runs'] += last_week_row['home_runs']\n",
    "            game_result.at[index, 'last_week_away_runs_allowed'] += last_week_row['away_runs']\n",
    "            # relief\n",
    "            game_result.at[index, 'last_week_away_re_times'] += baseball_times_converse(extract_num(str(last_week_row['home_re_times'])))\n",
    "            game_result.at[index, 'last_week_away_re_ERA'] += int(last_week_row['home_re_responsible_runs'])\n",
    "        else:\n",
    "            if int(last_week_row['home_runs']) == int(last_week_row['away_runs']):\n",
    "                game_result.at[index, 'last_week_away_draw'] += 1\n",
    "            else:\n",
    "                game_result.at[index, 'last_week_away_victory'] += abs(last_week_row['home_victory'] - 1)\n",
    "                game_result.at[index, 'last_week_away_lose'] += last_week_row['home_victory']\n",
    "            game_result.at[index, 'last_week_away_runs'] += last_week_row['away_runs']\n",
    "            game_result.at[index, 'last_week_away_runs_allowed'] += last_week_row['home_runs']\n",
    "            # relief\n",
    "            game_result.at[index, 'last_week_away_re_times'] += baseball_times_converse(extract_num(str(last_week_row['away_re_times'])))\n",
    "            game_result.at[index, 'last_week_away_re_ERA'] += int(last_week_row['away_re_responsible_runs'])\n",
    "    game_result.at[index, 'last_week_away_re_ERA'] /= (game_result.at[index, 'last_week_away_re_times'] / 9)\n",
    "    \"\"\"\n",
    "    if game_result.at[index, 'last_week_away_re_times'] == 0:\n",
    "        initial_ERA = 4\n",
    "        game_result.at[index, 'last_week_away_re_ERA'] = initial_ERA\n",
    "    \"\"\"\n",
    "        \n",
    "    # last year\n",
    "    pitcher_url = row['away_st_url']\n",
    "    pitcher_achievements = personal_achivements[(pitcher_url == personal_achivements['url'])]\n",
    "    if (pitcher_achievements['pitching_appearance_'+str(now_year-1)].values[0] == \"-\") | (pitcher_achievements['pitching_appearance_'+str(now_year-1)].values[0] == \"0\"):\n",
    "        game_result.at[index, 'last_year_away_st_APP'] = 0\n",
    "        game_result.at[index, 'last_year_away_st_ERA'] = initial_ERA\n",
    "        game_result.at[index, 'last_year_away_st_IP_ave'] = 0\n",
    "    else:\n",
    "        game_result.at[index, 'last_year_away_st_IP_ave'] = baseball_times_converse(extract_num(pitcher_achievements['time_'+str(now_year-1)].values[0])) / float(pitcher_achievements['pitching_appearance_'+str(now_year-1)].values[0])\n",
    "        game_result.at[index, 'last_year_away_st_APP'] = pitcher_achievements['pitching_appearance_'+str(now_year-1)]\n",
    "        game_result.at[index, 'last_year_away_st_ERA'] = pitcher_achievements['earned_run_ave_'+str(now_year-1)]\n",
    "    \"\"\"\n",
    "    # last 2 year\n",
    "    if (pitcher_achievements['pitching_appearance_'+str(now_year-2)].values[0] == \"-\") | (pitcher_achievements['pitching_appearance_'+str(now_year-2)].values[0] == \"0\"):\n",
    "        game_result.at[index, 'last_2_year_away_st_APP'] = 0\n",
    "        game_result.at[index, 'last_2_year_away_st_ERA'] = initial_ERA\n",
    "        game_result.at[index, 'last_2_year_away_st_IP_ave'] = 0\n",
    "    else:\n",
    "        game_result.at[index, 'last_2_year_away_st_IP_ave'] = baseball_times_converse(extract_num(pitcher_achievements['time_'+str(now_year-2)].values[0])) / float(pitcher_achievements['pitching_appearance_'+str(now_year-2)].values[0])\n",
    "        game_result.at[index, 'last_2_year_away_st_APP'] = pitcher_achievements['pitching_appearance_'+str(now_year-2)]\n",
    "        game_result.at[index, 'last_2_year_away_st_ERA'] = pitcher_achievements['earned_run_ave_'+str(now_year-2)]\n",
    "    \"\"\"\n",
    "    # away_teamの過去三週間の集計, pitching\n",
    "    game_count = len(game_result[(now_date - td_3_week <= game_result['date']) & (now_date > game_result['date']) & ((pitcher_url == game_result['home_st_url']) | (pitcher_url == game_result['away_st_url']))])\n",
    "    game_result.at[index, 'last_3_week_away_st_APP'] = game_count\n",
    "    \n",
    "    for last_3_week_index, last_3_week_row in game_result[(now_date - td_3_week <= game_result['date']) & (now_date > game_result['date']) \n",
    "    & ((pitcher_url == game_result['home_st_url']) | (pitcher_url == game_result['away_st_url']))].iterrows():\n",
    "        if(pitcher_url == last_3_week_row['home_st_url']):\n",
    "            game_result.at[index, 'last_3_week_away_st_ERA'] += int(last_3_week_row['home_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_3_week_away_st_IP_ave'] += baseball_times_converse(extract_num(str(last_3_week_row['home_st_times'])))\n",
    "        else:\n",
    "            game_result.at[index, 'last_3_week_away_st_ERA'] += int(last_3_week_row['away_st_responsible_runs'])\n",
    "            game_result.at[index, 'last_3_week_away_st_IP_ave'] += baseball_times_converse(extract_num(str(last_3_week_row['away_st_times'])))\n",
    "    if game_count == 0:\n",
    "        game_result.at[index, 'last_3_week_away_st_ERA'] = game_result.at[index, 'last_year_away_st_ERA']\n",
    "        game_result.at[index, 'last_3_week_away_st_IP_ave'] = 0\n",
    "    else:\n",
    "        game_result.at[index, 'last_3_week_away_st_ERA'] /= (game_result.at[index, 'last_3_week_away_st_IP_ave'] / 9)\n",
    "        game_result.at[index, 'last_3_week_away_st_IP_ave'] /= game_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 各確率計算\n",
    "game_result['last_week_home_team_batting_average'] = game_result['last_week_home_all_batter_hit'] / game_result['last_week_home_all_batter_at_bat']\n",
    "game_result['last_week_home_team_slugging_average'] = (game_result['last_week_home_all_batter_hit'] + game_result['last_week_home_all_batter_two_hit']\n",
    " + (game_result['last_week_home_all_batter_three_hit'] * 2) + (game_result['last_week_home_all_batter_homerun'] * 3)) / game_result['last_week_home_all_batter_at_bat']\n",
    "game_result['last_week_home_team_OBP'] = (game_result['last_week_home_all_batter_hit'] + game_result['last_week_home_all_batter_four_balls'] + game_result['last_week_home_all_batter_dead_ball']) / (game_result['last_week_home_all_batter_at_bat'] + game_result['last_week_home_all_batter_four_balls'] + game_result['last_week_home_all_batter_dead_ball'] + game_result['last_week_home_all_batter_sacrifice_hit'])\n",
    "game_result['last_week_home_team_strikeout_average'] = game_result['last_week_home_all_batter_strikeout'] / game_result['last_week_home_all_batter_at_bat']\n",
    "game_result['last_week_home_team_OPS'] = game_result['last_week_home_team_OBP'] + game_result['last_week_home_team_slugging_average']\n",
    "game_result['last_week_away_team_batting_average'] = game_result['last_week_away_all_batter_hit'] / game_result['last_week_away_all_batter_at_bat']\n",
    "game_result['last_week_away_team_slugging_average'] = (game_result['last_week_away_all_batter_hit'] + game_result['last_week_away_all_batter_two_hit']\n",
    " + (game_result['last_week_away_all_batter_three_hit'] * 2) + (game_result['last_week_away_all_batter_homerun'] * 3)) / game_result['last_week_away_all_batter_at_bat']\n",
    "game_result['last_week_away_team_OBP'] = (game_result['last_week_away_all_batter_hit'] + game_result['last_week_away_all_batter_four_balls'] + game_result['last_week_away_all_batter_dead_ball']) / (game_result['last_week_away_all_batter_at_bat'] + game_result['last_week_away_all_batter_four_balls'] + game_result['last_week_away_all_batter_dead_ball'] + game_result['last_week_away_all_batter_sacrifice_hit'])\n",
    "game_result['last_week_away_team_strikeout_average'] = game_result['last_week_away_all_batter_strikeout'] / game_result['last_week_away_all_batter_at_bat']\n",
    "game_result['last_week_away_team_OPS'] = game_result['last_week_away_team_OBP'] + game_result['last_week_away_team_slugging_average']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "week_match = 4\n",
    "for index, row in game_result.iterrows():\n",
    "    game_result.at[index, 'last_week_home_win_rate'] = (row['last_week_home_victory'] + ((week_match - (min(week_match, row['last_week_home_victory'] + row['last_week_home_lose']))) / 2)) / max(week_match, (row['last_week_home_victory'] + row['last_week_home_lose']))\n",
    "    game_result.at[index, 'last_week_away_win_rate'] = (row['last_week_away_victory'] + ((week_match - (min(week_match, row['last_week_away_victory'] + row['last_week_away_lose']))) / 2)) / max(week_match, (row['last_week_away_victory'] + row['last_week_away_lose']))\n",
    "\n",
    "average_runs = 3.5\n",
    "for index, row in game_result.iterrows():\n",
    "    # home\n",
    "    match_count = row['last_week_home_victory'] + row['last_week_home_draw'] + row['last_week_home_lose']\n",
    "    game_result.at[index,'last_week_home_runs_ave'] = (row['last_week_home_runs'] + ((week_match - min(week_match, match_count))*average_runs)) / max(week_match,match_count)\n",
    "    game_result.at[index,'last_week_home_runs_allowed_ave'] = (row['last_week_home_runs_allowed'] + ((week_match - min(week_match, match_count))*average_runs)) / max(week_match,match_count)\n",
    "    # away\n",
    "    match_count = row['last_week_away_victory'] + row['last_week_away_draw'] + row['last_week_away_lose']\n",
    "    game_result.at[index,'last_week_away_runs_ave'] = (row['last_week_away_runs'] + ((week_match - min(week_match,match_count))*average_runs)) / max(week_match,match_count)\n",
    "    game_result.at[index,'last_week_away_runs_allowed_ave'] =(row['last_week_away_runs_allowed'] + ((week_match - min(week_match,match_count))*average_runs)) / max(week_match,match_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epsilon = 0.000001\n",
    "\n",
    "game_result['last_week_home_win_rate_ratio'] = (game_result['last_week_home_win_rate'] + epsilon) / (game_result['last_week_home_win_rate'] + game_result['last_week_away_win_rate'] + (2*epsilon))\n",
    "\n",
    "game_result['last_year_home_win_rate_ratio'] = game_result['last_year_home_win_rate'] / (game_result['last_year_home_win_rate'] + game_result['last_year_away_win_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result['last_week_home_OPS_ratio'] = game_result['last_week_home_team_OPS'] / (game_result['last_week_home_team_OPS'] + game_result['last_week_away_team_OPS'])\n",
    "game_result['last_week_home_batting_average_ratio'] = game_result['last_week_home_team_batting_average'] / (game_result['last_week_home_team_batting_average'] + game_result['last_week_away_team_batting_average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 435.25x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"351.528125pt\" version=\"1.1\" viewBox=\"0 0 426.687986 351.528125\" width=\"426.687986pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 351.528125 \nL 426.687986 351.528125 \nL 426.687986 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 313.69375 \nL 344.885486 313.69375 \nL 344.885486 7.2 \nL 30.103125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 76.154959 313.69375 \nL 96.398477 313.69375 \nL 96.398477 269.705769 \nL 76.154959 269.705769 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 96.398477 313.69375 \nL 116.641994 313.69375 \nL 116.641994 289.500361 \nL 96.398477 289.500361 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 116.641994 313.69375 \nL 136.885512 313.69375 \nL 136.885512 200.424699 \nL 116.641994 200.424699 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 136.885512 313.69375 \nL 157.129029 313.69375 \nL 157.129029 130.04393 \nL 136.885512 130.04393 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 157.129029 313.69375 \nL 177.372547 313.69375 \nL 177.372547 82.756851 \nL 157.129029 82.756851 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 177.372547 313.69375 \nL 197.616064 313.69375 \nL 197.616064 56.364062 \nL 177.372547 56.364062 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 197.616064 313.69375 \nL 217.859582 313.69375 \nL 217.859582 48.666165 \nL 197.616064 48.666165 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 217.859582 313.69375 \nL 238.103099 313.69375 \nL 238.103099 167.433714 \nL 217.859582 167.433714 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 238.103099 313.69375 \nL 258.346617 313.69375 \nL 258.346617 245.51238 \nL 238.103099 245.51238 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 258.346617 313.69375 \nL 278.590134 313.69375 \nL 278.590134 299.397656 \nL 258.346617 299.397656 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 278.590134 313.69375 \nL 298.833652 313.69375 \nL 298.833652 280.702764 \nL 278.590134 280.702764 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 76.154959 313.69375 \nL 96.398481 313.69375 \nL 96.398481 266.689574 \nL 76.154959 266.689574 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 96.398481 313.69375 \nL 116.642002 313.69375 \nL 116.642002 290.691706 \nL 96.398481 290.691706 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 116.642002 313.69375 \nL 136.885524 313.69375 \nL 136.885524 239.687175 \nL 116.642002 239.687175 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 136.885524 313.69375 \nL 157.129045 313.69375 \nL 157.129045 172.681221 \nL 136.885524 172.681221 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 157.129045 313.69375 \nL 177.372567 313.69375 \nL 177.372567 54.670736 \nL 157.129045 54.670736 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 177.372567 313.69375 \nL 197.616089 313.69375 \nL 197.616089 49.670292 \nL 177.372567 49.670292 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 197.616089 313.69375 \nL 217.85961 313.69375 \nL 217.85961 38.669314 \nL 197.616089 38.669314 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 217.85961 313.69375 \nL 238.103132 313.69375 \nL 238.103132 172.681221 \nL 217.85961 172.681221 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 238.103132 313.69375 \nL 258.346653 313.69375 \nL 258.346653 231.686464 \nL 238.103132 231.686464 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 258.346653 313.69375 \nL 278.590175 313.69375 \nL 278.590175 289.691617 \nL 258.346653 289.691617 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 278.590175 313.69375 \nL 298.833696 313.69375 \nL 298.833696 263.689307 \nL 278.590175 263.689307 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m342ed370ef\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"31.6189\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −0.2 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(19.477494 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"76.154737\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.0 -->\n      <g transform=\"translate(68.203174 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"120.690573\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.2 -->\n      <g transform=\"translate(112.739011 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.22641\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(157.274847 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.762246\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(201.810684 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"254.298083\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(246.34652 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"298.833919\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(290.882357 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"343.369756\" xlink:href=\"#m342ed370ef\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1.2 -->\n      <g transform=\"translate(335.418193 328.292187)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- last_week_home_win_rate_ratio -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     </defs>\n     <g transform=\"translate(108.332587 341.970312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n      <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n      <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"543.115234\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"606.494141\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"667.675781\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"765.087891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"826.611328\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"876.611328\" xlink:href=\"#DejaVuSans-119\"/>\n      <use x=\"958.398438\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"986.181641\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1049.560547\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"1099.560547\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1140.673828\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1201.953125\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1241.162109\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1302.685547\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"1352.685547\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"1393.798828\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1455.078125\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1494.287109\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1522.070312\" xlink:href=\"#DejaVuSans-111\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m08467d5883\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m08467d5883\" y=\"313.69375\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 317.492969)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m08467d5883\" y=\"250.961029\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 254.760248)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m08467d5883\" y=\"188.228308\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 192.027527)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m08467d5883\" y=\"125.495587\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 129.294806)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m08467d5883\" y=\"62.762866\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 66.562085)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 44.411414 313.151305 \nL 46.664688 312.900343 \nL 48.917962 312.395637 \nL 51.171236 311.555307 \nL 53.42451 310.275204 \nL 55.677784 308.447189 \nL 57.931057 305.986313 \nL 60.184331 302.864493 \nL 62.437605 299.144216 \nL 69.197427 286.724182 \nL 71.450701 283.399275 \nL 73.703975 281.146833 \nL 75.957248 280.240404 \nL 78.210522 280.777937 \nL 80.463796 282.654327 \nL 82.71707 285.574796 \nL 89.476892 295.971176 \nL 91.730166 298.362618 \nL 93.98344 299.562994 \nL 96.236713 299.330011 \nL 98.489987 297.524279 \nL 100.743261 294.099994 \nL 102.996535 289.096993 \nL 105.249809 282.636535 \nL 107.503083 274.9182 \nL 109.756357 266.212574 \nL 114.262904 247.16685 \nL 118.769452 228.204037 \nL 121.022726 219.426876 \nL 123.276 211.299383 \nL 125.529274 203.820956 \nL 127.782548 196.891529 \nL 136.795643 170.751035 \nL 139.048917 163.647689 \nL 141.302191 156.114293 \nL 145.808739 139.990537 \nL 150.315287 123.463703 \nL 152.56856 115.512442 \nL 154.821834 107.981467 \nL 157.075108 100.971736 \nL 159.328382 94.526539 \nL 161.581656 88.630357 \nL 163.83493 83.212654 \nL 168.341478 73.319944 \nL 179.607847 49.685224 \nL 181.861121 45.810473 \nL 184.114395 42.909643 \nL 186.367669 41.30691 \nL 188.620942 41.21722 \nL 190.874216 42.707595 \nL 193.12749 45.702328 \nL 195.380764 50.027304 \nL 197.634038 55.473401 \nL 199.887312 61.852837 \nL 202.140586 69.027876 \nL 204.39386 76.905314 \nL 206.647134 85.405476 \nL 211.153681 103.807567 \nL 217.913503 131.992328 \nL 220.166777 140.708305 \nL 222.420051 148.890445 \nL 226.926598 163.851227 \nL 233.68642 185.320894 \nL 235.939694 192.954399 \nL 240.446242 209.362156 \nL 249.459337 243.461107 \nL 251.712611 251.290172 \nL 253.965885 258.627294 \nL 256.219159 265.45459 \nL 258.472433 271.786709 \nL 260.725707 277.647643 \nL 262.978981 283.049772 \nL 265.232254 287.980176 \nL 267.485528 292.395978 \nL 269.738802 296.227592 \nL 271.992076 299.387378 \nL 274.24535 301.781466 \nL 276.498624 303.323764 \nL 278.751898 303.952302 \nL 281.005172 303.648118 \nL 283.258445 302.455398 \nL 285.511719 300.499067 \nL 287.764993 297.993637 \nL 292.271541 292.581777 \nL 294.524815 290.394089 \nL 296.778089 288.992422 \nL 299.031363 288.594548 \nL 301.284637 289.278249 \nL 303.53791 290.970065 \nL 305.791184 293.465179 \nL 308.044458 296.472888 \nL 312.551006 302.78142 \nL 314.80428 305.571531 \nL 317.057554 307.912496 \nL 319.310828 309.757121 \nL 321.564101 311.125711 \nL 323.817375 312.080396 \nL 326.070649 312.699199 \nL 328.323923 313.054573 \nL 330.577197 313.19815 \nL 330.577197 313.19815 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p78e3833c29)\" d=\"M 46.464682 312.989808 \nL 48.685622 312.709872 \nL 50.906561 312.082291 \nL 53.1275 311.004165 \nL 55.34844 309.340873 \nL 57.569379 306.956568 \nL 59.790318 303.757337 \nL 62.011257 299.741768 \nL 64.232197 295.047047 \nL 68.674075 284.974435 \nL 70.895015 280.594814 \nL 73.115954 277.379198 \nL 75.336893 275.756301 \nL 77.557833 275.940008 \nL 79.778772 277.874171 \nL 81.999711 281.23894 \nL 84.22065 285.516199 \nL 86.44159 290.093075 \nL 88.662529 294.372566 \nL 90.883468 297.862337 \nL 93.104408 300.224427 \nL 95.325347 301.283826 \nL 97.546286 301.006222 \nL 99.767226 299.460618 \nL 101.988165 296.780931 \nL 104.209104 293.134844 \nL 106.430043 288.701818 \nL 108.650983 283.658126 \nL 110.871922 278.165983 \nL 115.313801 266.368468 \nL 128.639436 229.439149 \nL 133.081315 216.896702 \nL 135.302254 210.308012 \nL 137.523194 203.327765 \nL 139.744133 195.815594 \nL 141.965072 187.657111 \nL 144.186012 178.788025 \nL 146.406951 169.212064 \nL 150.848829 148.348246 \nL 157.511647 116.24195 \nL 159.732587 106.540416 \nL 161.953526 97.765804 \nL 164.174465 89.987323 \nL 166.395404 83.081858 \nL 173.058222 64.001519 \nL 175.279162 56.89142 \nL 181.94198 33.771444 \nL 184.162919 27.545237 \nL 186.383858 23.376533 \nL 188.604797 21.79494 \nL 190.825737 22.929734 \nL 193.046676 26.489912 \nL 195.267615 31.86879 \nL 197.488555 38.331529 \nL 208.593251 72.483311 \nL 210.81419 80.028012 \nL 213.03513 88.418932 \nL 215.256069 97.775695 \nL 217.477008 108.053388 \nL 221.918887 130.495076 \nL 226.360766 153.254751 \nL 228.581705 163.921174 \nL 230.802644 173.785169 \nL 233.023583 182.730307 \nL 235.244523 190.760742 \nL 237.465462 197.994414 \nL 241.907341 210.928499 \nL 248.570159 229.785479 \nL 253.012037 243.181757 \nL 259.674855 263.613474 \nL 264.116734 276.431912 \nL 266.337673 282.311763 \nL 268.558612 287.67084 \nL 270.779552 292.352501 \nL 273.000491 296.180703 \nL 275.22143 298.971477 \nL 277.442369 300.550604 \nL 279.663309 300.778962 \nL 281.884248 299.586736 \nL 284.105187 297.014199 \nL 286.326127 293.250752 \nL 288.547066 288.657831 \nL 290.768005 283.759451 \nL 292.988944 279.189543 \nL 295.209884 275.598059 \nL 297.430823 273.533916 \nL 299.651762 273.33537 \nL 301.872702 275.060646 \nL 304.093641 278.481222 \nL 306.31458 283.140467 \nL 312.977398 298.850927 \nL 315.198337 303.123024 \nL 317.419277 306.526831 \nL 319.640216 309.064105 \nL 321.861155 310.8353 \nL 324.082095 311.986035 \nL 326.303034 312.661623 \nL 328.523973 312.975278 \nL 328.523973 312.975278 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 30.103125 313.69375 \nL 30.103125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 30.103125 313.69375 \nL 344.885486 313.69375 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"legend_1\">\n   <g id=\"text_15\">\n    <!-- home_victory -->\n    <defs>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n    </defs>\n    <g transform=\"translate(350.073924 163.70625)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"63.378906\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"124.560547\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"221.972656\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"283.496094\" xlink:href=\"#DejaVuSans-95\"/>\n     <use x=\"333.496094\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"392.675781\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"420.458984\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"475.439453\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"514.648438\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"575.830078\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"616.943359\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"patch_27\">\n    <path d=\"M 366.699705 178.6625 \nL 386.699705 178.6625 \nL 386.699705 171.6625 \nL 366.699705 171.6625 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- 0 -->\n    <g transform=\"translate(394.699705 178.6625)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 366.699705 193.340625 \nL 386.699705 193.340625 \nL 386.699705 186.340625 \nL 366.699705 186.340625 \nz\n\" style=\"fill:#ff7f0e;opacity:0.4;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- 1 -->\n    <g transform=\"translate(394.699705 193.340625)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-49\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p78e3833c29\">\n   <rect height=\"306.49375\" width=\"314.782361\" x=\"30.103125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "home_win_rate_ratio_Fg = sns.FacetGrid(game_result, hue='home_victory', size=5)\n",
    "home_win_rate_ratio_Fg = home_win_rate_ratio_Fg.map(sns.distplot, \"last_week_home_win_rate_ratio\",bins=11).add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result.to_csv(\"game_result_20200910.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result = game_result.fillna(game_result.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result['last_year_home_st_ERA_ratio'] = (game_result['last_year_home_st_ERA'] + epsilon) / (game_result['last_year_home_st_ERA'] + game_result['last_year_away_st_ERA'] + 2*epsilon)\n",
    "game_result['last_3_week_home_st_ERA_ratio'] = (game_result['last_3_week_home_st_ERA'] + epsilon) / (game_result['last_3_week_home_st_ERA'] + game_result['last_3_week_away_st_ERA'] + 2*epsilon)\n",
    "game_result['last_week_home_re_ERA_ratio'] = (game_result['last_week_home_re_ERA'] + epsilon) / (game_result['last_week_home_re_ERA'] + game_result['last_week_away_re_ERA'] + 2*epsilon)\n",
    "game_result['last_week_home_runs_ave_ratio'] = (game_result['last_week_home_runs_ave'] + epsilon) / (game_result['last_week_home_runs_ave'] + game_result['last_week_away_runs_ave'] + 2*epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result_train = game_result[game_result['home_runs'] != game_result['away_runs']][['home_victory',\n",
    "    'last_week_home_win_rate_ratio', 'last_year_home_st_ERA', 'last_3_week_home_st_ERA', 'last_week_home_re_ERA','last_week_home_runs_ave'\n",
    "    , 'last_year_away_st_ERA', 'last_3_week_away_st_ERA', 'last_week_away_re_ERA', 'last_week_away_runs_ave'\n",
    "    , 'last_week_home_team_OPS', 'last_week_away_team_OPS', 'last_week_home_OPS_ratio','last_year_home_win_rate','last_year_away_win_rate'\n",
    "    , 'last_year_home_st_ERA_ratio', 'last_3_week_home_st_ERA_ratio']].copy()\n",
    "\n",
    "X = game_result_train.drop(columns='home_victory')\n",
    "y = game_result_train['home_victory']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "last_week_home_win_rate_ratio    0\nlast_year_home_st_ERA            0\nlast_3_week_home_st_ERA          0\nlast_week_home_re_ERA            0\nlast_week_home_runs_ave          0\nlast_year_away_st_ERA            0\nlast_3_week_away_st_ERA          0\nlast_week_away_re_ERA            0\nlast_week_away_runs_ave          0\nlast_week_home_team_OPS          0\nlast_week_away_team_OPS          0\nlast_week_home_OPS_ratio         0\nlast_year_home_win_rate          0\nlast_year_away_win_rate          0\ndtype: int64"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaler.transform(X)\n",
    "X_std = pd.DataFrame(scaler.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result_train_nn.to_csv('game_result_train_nn.csv')\n",
    "X_std.to_csv('game_result_train_nn_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"264.131875pt\" version=\"1.1\" viewBox=\"0 0 402.459062 264.131875\" width=\"402.459062pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 264.131875 \n",
       "L 402.459062 264.131875 \n",
       "L 402.459062 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 60.459063 224.64 \n",
       "L 395.259063 224.64 \n",
       "L 395.259063 7.2 \n",
       "L 60.459063 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m2c565e7b61\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"75.677244\" xlink:href=\"#m2c565e7b61\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(72.495994 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.404517\" xlink:href=\"#m2c565e7b61\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 2 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(123.223267 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"177.13179\" xlink:href=\"#m2c565e7b61\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 4 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(173.95054 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.859062\" xlink:href=\"#m2c565e7b61\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 6 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(224.677812 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"278.586335\" xlink:href=\"#m2c565e7b61\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 8 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(275.405085 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"329.313608\" xlink:href=\"#m2c565e7b61\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(322.951108 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"380.040881\" xlink:href=\"#m2c565e7b61\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 12 -->\n",
       "      <g transform=\"translate(373.678381 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- Features -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 51.703125 72.90625 \n",
       "L 51.703125 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.109375 \n",
       "L 48.578125 43.109375 \n",
       "L 48.578125 34.8125 \n",
       "L 19.671875 34.8125 \n",
       "L 19.671875 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-70\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(201.607188 254.43625)scale(0.12 -0.12)\">\n",
       "      <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "      <use x=\"57.441406\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"118.964844\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"180.244141\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"219.453125\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "      <use x=\"282.832031\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"323.914062\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"385.4375\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m693ea5d044\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"216.421953\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- −0.03 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.59375 35.5 \n",
       "L 73.1875 35.5 \n",
       "L 73.1875 27.203125 \n",
       "L 10.59375 27.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-8722\"/>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(22.81375 220.221172)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"188.386839\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- −0.02 -->\n",
       "      <g transform=\"translate(22.81375 192.186058)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"160.351725\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- −0.01 -->\n",
       "      <g transform=\"translate(22.81375 164.150944)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-8722\"/>\n",
       "       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"132.316611\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(31.193438 136.11583)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"104.281497\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.01 -->\n",
       "      <g transform=\"translate(31.193438 108.080716)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"76.246383\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.02 -->\n",
       "      <g transform=\"translate(31.193438 80.045602)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"48.211269\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.03 -->\n",
       "      <g transform=\"translate(31.193438 52.010488)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.459063\" xlink:href=\"#m693ea5d044\" y=\"20.176155\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.04 -->\n",
       "      <g transform=\"translate(31.193438 23.975374)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Coefficient -->\n",
       "     <defs>\n",
       "      <path d=\"M 64.40625 67.28125 \n",
       "L 64.40625 56.890625 \n",
       "Q 59.421875 61.53125 53.78125 63.8125 \n",
       "Q 48.140625 66.109375 41.796875 66.109375 \n",
       "Q 29.296875 66.109375 22.65625 58.46875 \n",
       "Q 16.015625 50.828125 16.015625 36.375 \n",
       "Q 16.015625 21.96875 22.65625 14.328125 \n",
       "Q 29.296875 6.6875 41.796875 6.6875 \n",
       "Q 48.140625 6.6875 53.78125 8.984375 \n",
       "Q 59.421875 11.28125 64.40625 15.921875 \n",
       "L 64.40625 5.609375 \n",
       "Q 59.234375 2.09375 53.4375 0.328125 \n",
       "Q 47.65625 -1.421875 41.21875 -1.421875 \n",
       "Q 24.65625 -1.421875 15.125 8.703125 \n",
       "Q 5.609375 18.84375 5.609375 36.375 \n",
       "Q 5.609375 53.953125 15.125 64.078125 \n",
       "Q 24.65625 74.21875 41.21875 74.21875 \n",
       "Q 47.75 74.21875 53.53125 72.484375 \n",
       "Q 59.328125 70.75 64.40625 67.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-67\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 37.109375 75.984375 \n",
       "L 37.109375 68.5 \n",
       "L 28.515625 68.5 \n",
       "Q 23.6875 68.5 21.796875 66.546875 \n",
       "Q 19.921875 64.59375 19.921875 59.515625 \n",
       "L 19.921875 54.6875 \n",
       "L 34.71875 54.6875 \n",
       "L 34.71875 47.703125 \n",
       "L 19.921875 47.703125 \n",
       "L 19.921875 0 \n",
       "L 10.890625 0 \n",
       "L 10.890625 47.703125 \n",
       "L 2.296875 47.703125 \n",
       "L 2.296875 54.6875 \n",
       "L 10.890625 54.6875 \n",
       "L 10.890625 58.5 \n",
       "Q 10.890625 67.625 15.140625 71.796875 \n",
       "Q 19.390625 75.984375 28.609375 75.984375 \n",
       "z\n",
       "\" id=\"DejaVuSans-102\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(16.318125 148.176562)rotate(-90)scale(0.12 -0.12)\">\n",
       "      <use xlink:href=\"#DejaVuSans-67\"/>\n",
       "      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"192.529297\" xlink:href=\"#DejaVuSans-102\"/>\n",
       "      <use x=\"227.734375\" xlink:href=\"#DejaVuSans-102\"/>\n",
       "      <use x=\"262.939453\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"290.722656\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"345.703125\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"373.486328\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"435.009766\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "      <use x=\"498.388672\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path clip-path=\"url(#p5ad4930614)\" d=\"M 75.677244 97.249022 \n",
       "L 101.040881 164.855471 \n",
       "L 126.404517 117.024324 \n",
       "L 151.768153 102.277411 \n",
       "L 177.13179 94.637198 \n",
       "L 202.495426 130.857218 \n",
       "L 227.859062 194.592346 \n",
       "L 253.222699 101.012273 \n",
       "L 278.586335 89.111149 \n",
       "L 303.949972 214.756364 \n",
       "L 329.313608 160.409608 \n",
       "L 354.677244 198.206164 \n",
       "L 380.040881 17.083636 \n",
       "\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path clip-path=\"url(#p5ad4930614)\" d=\"M 75.677244 96.262034 \n",
       "L 101.040881 164.571788 \n",
       "L 126.404517 117.200043 \n",
       "L 151.768153 103.528014 \n",
       "L 177.13179 95.723222 \n",
       "L 202.495426 131.010305 \n",
       "L 227.859062 192.851139 \n",
       "L 253.222699 101.254099 \n",
       "L 278.586335 89.288402 \n",
       "L 303.949972 213.014427 \n",
       "L 329.313608 159.027466 \n",
       "L 354.677244 197.642546 \n",
       "L 380.040881 19.436765 \n",
       "\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_18\">\n",
       "    <path clip-path=\"url(#p5ad4930614)\" d=\"M 75.677244 90.421168 \n",
       "L 101.040881 162.310893 \n",
       "L 126.404517 118.541478 \n",
       "L 151.768153 112.131876 \n",
       "L 177.13179 103.277762 \n",
       "L 202.495426 131.674845 \n",
       "L 227.859062 181.182186 \n",
       "L 253.222699 103.174166 \n",
       "L 278.586335 90.904191 \n",
       "L 303.949972 200.565864 \n",
       "L 329.313608 149.298845 \n",
       "L 354.677244 193.552987 \n",
       "L 380.040881 35.981193 \n",
       "\" style=\"fill:none;stroke:#0000ff;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 60.459063 224.64 \n",
       "L 60.459063 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 395.259063 224.64 \n",
       "L 395.259063 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 60.459063 224.64 \n",
       "L 395.259063 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 60.459063 7.2 \n",
       "L 395.259063 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 67.459063 59.234375 \n",
       "L 154.646563 59.234375 \n",
       "Q 156.646563 59.234375 156.646563 57.234375 \n",
       "L 156.646563 14.2 \n",
       "Q 156.646563 12.2 154.646563 12.2 \n",
       "L 67.459063 12.2 \n",
       "Q 65.459063 12.2 65.459063 14.2 \n",
       "L 65.459063 57.234375 \n",
       "Q 65.459063 59.234375 67.459063 59.234375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 69.459063 20.298438 \n",
       "L 89.459063 20.298438 \n",
       "\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:1.5,2.475;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\"/>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- alpha=0 -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "      <path d=\"M 10.59375 45.40625 \n",
       "L 73.1875 45.40625 \n",
       "L 73.1875 37.203125 \n",
       "L 10.59375 37.203125 \n",
       "z\n",
       "M 10.59375 25.484375 \n",
       "L 73.1875 25.484375 \n",
       "L 73.1875 17.1875 \n",
       "L 10.59375 17.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-61\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(97.459063 23.798438)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "      <use x=\"215.917969\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"277.197266\" xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use x=\"360.986328\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_21\">\n",
       "     <path d=\"M 69.459063 34.976562 \n",
       "L 89.459063 34.976562 \n",
       "\" style=\"fill:none;stroke:#008000;stroke-dasharray:9.6,2.4,1.5,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_22\"/>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- alpha=10 -->\n",
       "     <g transform=\"translate(97.459063 38.476562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "      <use x=\"215.917969\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"277.197266\" xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use x=\"360.986328\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "      <use x=\"424.609375\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_23\">\n",
       "     <path d=\"M 69.459063 49.654688 \n",
       "L 89.459063 49.654688 \n",
       "\" style=\"fill:none;stroke:#0000ff;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\"/>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- alpha=100 -->\n",
       "     <g transform=\"translate(97.459063 53.154688)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "      <use x=\"215.917969\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"277.197266\" xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use x=\"360.986328\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "      <use x=\"424.609375\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      <use x=\"488.232422\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5ad4930614\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"60.459063\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from matplotlib import pyplot as plt\n",
    "K = 5\n",
    "kf =  StratifiedKFold(n_splits=K, shuffle=True, random_state=114)\n",
    "oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "historys = []\n",
    "for train_index, test_index in kf.split(X_std,y):\n",
    "    X_train, X_val = X_std.iloc[train_index], X_std.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    opt = optimizers.Adam(learning_rate=0.0001,epsilon=0.0001)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    batch_size = 1\n",
    "    epochs = 30\n",
    "    history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs= epochs,\n",
    "                    batch_size= batch_size,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)\n",
    "    pred_i = model.predict(X_val)[:,0]\n",
    "    oof_pred[test_index] = pred_i\n",
    "    historys.append(history)\n",
    "\n",
    "clf1 = LinearRegression().fit(X_std, y)\n",
    "clf2 = Ridge(alpha=10).fit(X_std, y)\n",
    "clf3 = Ridge(alpha=100).fit(X_std, y)\n",
    "\n",
    "plt.plot(clf1.coef_, label='alpha=0', color='r', linestyle=':')\n",
    "plt.plot(clf2.coef_, label='alpha=10', color='g', linestyle='-.')\n",
    "plt.plot(clf3.coef_, label='alpha=100', color='b', linestyle='--')\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Coefficient', fontsize=12)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "p - loss: 0.6886 - accuracy: 0.5279 - val_loss: 0.6911 - val_accuracy: 0.5426\nEpoch 14/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6913 - accuracy: 0.5391 - val_loss: 0.6900 - val_accuracy: 0.5349\nEpoch 15/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6907 - accuracy: 0.5232 - val_loss: 0.6903 - val_accuracy: 0.5349\nEpoch 16/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6923 - accuracy: 0.5327 - val_loss: 0.6900 - val_accuracy: 0.5349\nEpoch 17/30\n2326/2326 [==============================] - 2s 986us/step - loss: 0.6907 - accuracy: 0.5391 - val_loss: 0.6926 - val_accuracy: 0.5116\nEpoch 18/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6910 - accuracy: 0.5181 - val_loss: 0.6895 - val_accuracy: 0.5349\nEpoch 19/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6912 - accuracy: 0.5267 - val_loss: 0.6894 - val_accuracy: 0.5349\nEpoch 20/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6907 - accuracy: 0.5340 - val_loss: 0.6907 - val_accuracy: 0.5349\nEpoch 21/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6898 - accuracy: 0.5284 - val_loss: 0.6898 - val_accuracy: 0.5349\nEpoch 22/30\n2326/2326 [==============================] - 2s 994us/step - loss: 0.6905 - accuracy: 0.5348 - val_loss: 0.6897 - val_accuracy: 0.5349\nEpoch 23/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6893 - accuracy: 0.5348 - val_loss: 0.6882 - val_accuracy: 0.5349\nEpoch 24/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6882 - accuracy: 0.5357 - val_loss: 0.6875 - val_accuracy: 0.5349\nEpoch 25/30\n2326/2326 [==============================] - 2s 985us/step - loss: 0.6916 - accuracy: 0.5297 - val_loss: 0.6892 - val_accuracy: 0.5349\nEpoch 26/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6894 - accuracy: 0.5374 - val_loss: 0.6870 - val_accuracy: 0.5349\nEpoch 27/30\n2326/2326 [==============================] - 2s 983us/step - loss: 0.6910 - accuracy: 0.5241 - val_loss: 0.6891 - val_accuracy: 0.5349\nEpoch 28/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6881 - accuracy: 0.5327 - val_loss: 0.6874 - val_accuracy: 0.5349\nEpoch 29/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6889 - accuracy: 0.5408 - val_loss: 0.6873 - val_accuracy: 0.5349\nEpoch 30/30\n2326/2326 [==============================] - 2s 988us/step - loss: 0.6887 - accuracy: 0.5318 - val_loss: 0.6887 - val_accuracy: 0.5504\nEpoch 1/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.7092 - accuracy: 0.5120 - val_loss: 0.6927 - val_accuracy: 0.5349\nEpoch 2/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6943 - accuracy: 0.5211 - val_loss: 0.6966 - val_accuracy: 0.5349\nEpoch 3/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6922 - accuracy: 0.5232 - val_loss: 0.6937 - val_accuracy: 0.5271\nEpoch 4/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6912 - accuracy: 0.5374 - val_loss: 0.6921 - val_accuracy: 0.5388\nEpoch 5/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6918 - accuracy: 0.5305 - val_loss: 0.6957 - val_accuracy: 0.5349\nEpoch 6/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6914 - accuracy: 0.5331 - val_loss: 0.6921 - val_accuracy: 0.5426\nEpoch 7/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6922 - accuracy: 0.5331 - val_loss: 0.6925 - val_accuracy: 0.5349\nEpoch 8/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6890 - accuracy: 0.5215 - val_loss: 0.6927 - val_accuracy: 0.5310\nEpoch 9/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6914 - accuracy: 0.5340 - val_loss: 0.6921 - val_accuracy: 0.5349\nEpoch 10/30\n2326/2326 [==============================] - 2s 987us/step - loss: 0.6879 - accuracy: 0.5469 - val_loss: 0.6918 - val_accuracy: 0.5426\nEpoch 11/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6907 - accuracy: 0.5378 - val_loss: 0.6918 - val_accuracy: 0.5349\nEpoch 12/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6872 - accuracy: 0.5439 - val_loss: 0.6919 - val_accuracy: 0.5504\nEpoch 13/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6888 - accuracy: 0.5413 - val_loss: 0.6917 - val_accuracy: 0.5426\nEpoch 14/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6891 - accuracy: 0.5370 - val_loss: 0.6910 - val_accuracy: 0.5388\nEpoch 15/30\n2326/2326 [==============================] - 2s 998us/step - loss: 0.6891 - accuracy: 0.5400 - val_loss: 0.6910 - val_accuracy: 0.5465\nEpoch 16/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6881 - accuracy: 0.5335 - val_loss: 0.6909 - val_accuracy: 0.5349\nEpoch 17/30\n2326/2326 [==============================] - 2s 987us/step - loss: 0.6882 - accuracy: 0.5357 - val_loss: 0.6902 - val_accuracy: 0.5581\nEpoch 18/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6863 - accuracy: 0.5404 - val_loss: 0.6906 - val_accuracy: 0.5426\nEpoch 19/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6904 - accuracy: 0.5271 - val_loss: 0.6924 - val_accuracy: 0.5504\nEpoch 20/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6884 - accuracy: 0.5430 - val_loss: 0.6904 - val_accuracy: 0.5465\nEpoch 21/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6887 - accuracy: 0.5297 - val_loss: 0.6911 - val_accuracy: 0.5349\nEpoch 22/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6858 - accuracy: 0.5365 - val_loss: 0.6927 - val_accuracy: 0.5349\nEpoch 23/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6865 - accuracy: 0.5490 - val_loss: 0.6909 - val_accuracy: 0.5426\nEpoch 24/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6887 - accuracy: 0.5451 - val_loss: 0.6906 - val_accuracy: 0.5349\nEpoch 25/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6906 - accuracy: 0.5318 - val_loss: 0.6927 - val_accuracy: 0.4884\nEpoch 26/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6902 - accuracy: 0.5340 - val_loss: 0.6900 - val_accuracy: 0.5465\nEpoch 27/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6839 - accuracy: 0.5482 - val_loss: 0.6934 - val_accuracy: 0.5426\nEpoch 28/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6877 - accuracy: 0.5357 - val_loss: 0.6917 - val_accuracy: 0.5388\nEpoch 29/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6878 - accuracy: 0.5477 - val_loss: 0.6915 - val_accuracy: 0.5620\nEpoch 30/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6830 - accuracy: 0.5374 - val_loss: 0.6933 - val_accuracy: 0.5116\nEpoch 1/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.7094 - accuracy: 0.5279 - val_loss: 0.6955 - val_accuracy: 0.4690\nEpoch 2/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6963 - accuracy: 0.5168 - val_loss: 0.6925 - val_accuracy: 0.5116\nEpoch 3/30\n2326/2326 [==============================] - 2s 969us/step - loss: 0.6943 - accuracy: 0.5232 - val_loss: 0.6879 - val_accuracy: 0.5581\nEpoch 4/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6941 - accuracy: 0.5271 - val_loss: 0.6934 - val_accuracy: 0.5078\nEpoch 5/30\n2326/2326 [==============================] - 2s 971us/step - loss: 0.6947 - accuracy: 0.5181 - val_loss: 0.6895 - val_accuracy: 0.5349\nEpoch 6/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6932 - accuracy: 0.5292 - val_loss: 0.6894 - val_accuracy: 0.5349\nEpoch 7/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6924 - accuracy: 0.5288 - val_loss: 0.6882 - val_accuracy: 0.5349\nEpoch 8/30\n2326/2326 [==============================] - 2s 985us/step - loss: 0.6933 - accuracy: 0.5262 - val_loss: 0.6887 - val_accuracy: 0.5349\nEpoch 9/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6923 - accuracy: 0.5258 - val_loss: 0.6885 - val_accuracy: 0.5349\nEpoch 10/30\n2326/2326 [==============================] - 2s 1000us/step - loss: 0.6904 - accuracy: 0.5314 - val_loss: 0.6874 - val_accuracy: 0.5349\nEpoch 11/30\n2326/2326 [==============================] - 2s 975us/step - loss: 0.6893 - accuracy: 0.5331 - val_loss: 0.6879 - val_accuracy: 0.5349\nEpoch 12/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6883 - accuracy: 0.5348 - val_loss: 0.6848 - val_accuracy: 0.5349\nEpoch 13/30\n2326/2326 [==============================] - 2s 967us/step - loss: 0.6906 - accuracy: 0.5267 - val_loss: 0.6877 - val_accuracy: 0.5426\nEpoch 14/30\n2326/2326 [==============================] - 2s 957us/step - loss: 0.6887 - accuracy: 0.5215 - val_loss: 0.6845 - val_accuracy: 0.5388\nEpoch 15/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6909 - accuracy: 0.5301 - val_loss: 0.6860 - val_accuracy: 0.5349\nEpoch 16/30\n2326/2326 [==============================] - 2s 985us/step - loss: 0.6898 - accuracy: 0.5353 - val_loss: 0.6881 - val_accuracy: 0.5465\nEpoch 17/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6901 - accuracy: 0.5297 - val_loss: 0.6873 - val_accuracy: 0.5543\nEpoch 18/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6886 - accuracy: 0.5232 - val_loss: 0.6845 - val_accuracy: 0.5310\nEpoch 19/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6885 - accuracy: 0.5430 - val_loss: 0.6875 - val_accuracy: 0.5349\nEpoch 20/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6902 - accuracy: 0.5340 - val_loss: 0.6877 - val_accuracy: 0.5465\nEpoch 21/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6890 - accuracy: 0.5370 - val_loss: 0.6865 - val_accuracy: 0.5465\nEpoch 22/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6885 - accuracy: 0.5361 - val_loss: 0.6864 - val_accuracy: 0.5543\nEpoch 23/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6911 - accuracy: 0.5318 - val_loss: 0.6877 - val_accuracy: 0.5426\nEpoch 24/30\n2326/2326 [==============================] - 2s 985us/step - loss: 0.6894 - accuracy: 0.5353 - val_loss: 0.6879 - val_accuracy: 0.5349\nEpoch 25/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6890 - accuracy: 0.5284 - val_loss: 0.6861 - val_accuracy: 0.5349\nEpoch 26/30\n2326/2326 [==============================] - 2s 984us/step - loss: 0.6873 - accuracy: 0.5327 - val_loss: 0.6843 - val_accuracy: 0.5349\nEpoch 27/30\n2326/2326 [==============================] - 2s 968us/step - loss: 0.6870 - accuracy: 0.5305 - val_loss: 0.6861 - val_accuracy: 0.5426\nEpoch 28/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6921 - accuracy: 0.5284 - val_loss: 0.6891 - val_accuracy: 0.5388\nEpoch 29/30\n2326/2326 [==============================] - 2s 997us/step - loss: 0.6861 - accuracy: 0.5374 - val_loss: 0.6853 - val_accuracy: 0.5349\nEpoch 30/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6896 - accuracy: 0.5314 - val_loss: 0.6859 - val_accuracy: 0.5349\nEpoch 1/30\n2326/2326 [==============================] - 2s 989us/step - loss: 0.7044 - accuracy: 0.5262 - val_loss: 0.6913 - val_accuracy: 0.5388\nEpoch 2/30\n2326/2326 [==============================] - 2s 945us/step - loss: 0.6936 - accuracy: 0.5413 - val_loss: 0.6927 - val_accuracy: 0.4884\nEpoch 3/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6925 - accuracy: 0.5292 - val_loss: 0.6955 - val_accuracy: 0.4767\nEpoch 4/30\n2326/2326 [==============================] - 2s 983us/step - loss: 0.6910 - accuracy: 0.5292 - val_loss: 0.6906 - val_accuracy: 0.5349\nEpoch 5/30\n2326/2326 [==============================] - 2s 991us/step - loss: 0.6903 - accuracy: 0.5310 - val_loss: 0.6901 - val_accuracy: 0.5349\nEpoch 6/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6934 - accuracy: 0.5228 - val_loss: 0.6893 - val_accuracy: 0.5349\nEpoch 7/30\n2326/2326 [==============================] - 2s 968us/step - loss: 0.6902 - accuracy: 0.5305 - val_loss: 0.6889 - val_accuracy: 0.5349\nEpoch 8/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6921 - accuracy: 0.5310 - val_loss: 0.6876 - val_accuracy: 0.5349\nEpoch 9/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6917 - accuracy: 0.5262 - val_loss: 0.6878 - val_accuracy: 0.5349\nEpoch 10/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6891 - accuracy: 0.5348 - val_loss: 0.6873 - val_accuracy: 0.5271\nEpoch 11/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6912 - accuracy: 0.5305 - val_loss: 0.6876 - val_accuracy: 0.5349\nEpoch 12/30\n2326/2326 [==============================] - 2s 982us/step - loss: 0.6906 - accuracy: 0.5335 - val_loss: 0.6887 - val_accuracy: 0.5349\nEpoch 13/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6914 - accuracy: 0.5249 - val_loss: 0.6867 - val_accuracy: 0.5349\nEpoch 14/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6893 - accuracy: 0.5396 - val_loss: 0.6882 - val_accuracy: 0.5310\nEpoch 15/30\n2326/2326 [==============================] - 2s 965us/step - loss: 0.6861 - accuracy: 0.5451 - val_loss: 0.6823 - val_accuracy: 0.5349\nEpoch 16/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6875 - accuracy: 0.5383 - val_loss: 0.6850 - val_accuracy: 0.5349\nEpoch 17/30\n2326/2326 [==============================] - 2s 981us/step - loss: 0.6916 - accuracy: 0.5404 - val_loss: 0.6867 - val_accuracy: 0.5349\nEpoch 18/30\n2326/2326 [==============================] - 2s 965us/step - loss: 0.6891 - accuracy: 0.5322 - val_loss: 0.6851 - val_accuracy: 0.5349\nEpoch 19/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6875 - accuracy: 0.5383 - val_loss: 0.6862 - val_accuracy: 0.5271\nEpoch 20/30\n2326/2326 [==============================] - 2s 972us/step - loss: 0.6898 - accuracy: 0.5284 - val_loss: 0.6874 - val_accuracy: 0.5349\nEpoch 21/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6882 - accuracy: 0.5374 - val_loss: 0.6854 - val_accuracy: 0.5426\nEpoch 22/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6891 - accuracy: 0.5413 - val_loss: 0.6868 - val_accuracy: 0.5349\nEpoch 23/30\n2326/2326 [==============================] - 2s 973us/step - loss: 0.6894 - accuracy: 0.5301 - val_loss: 0.6855 - val_accuracy: 0.5388\nEpoch 24/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6879 - accuracy: 0.5365 - val_loss: 0.6845 - val_accuracy: 0.5349\nEpoch 25/30\n2326/2326 [==============================] - 2s 972us/step - loss: 0.6884 - accuracy: 0.5396 - val_loss: 0.6880 - val_accuracy: 0.5310\nEpoch 26/30\n2326/2326 [==============================] - 2s 962us/step - loss: 0.6898 - accuracy: 0.5327 - val_loss: 0.6866 - val_accuracy: 0.5349\nEpoch 27/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6885 - accuracy: 0.5486 - val_loss: 0.6882 - val_accuracy: 0.5349\nEpoch 28/30\n2326/2326 [==============================] - 2s 957us/step - loss: 0.6905 - accuracy: 0.5365 - val_loss: 0.6879 - val_accuracy: 0.5543\nEpoch 29/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6879 - accuracy: 0.5464 - val_loss: 0.6870 - val_accuracy: 0.5504\nEpoch 30/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6908 - accuracy: 0.5391 - val_loss: 0.6888 - val_accuracy: 0.5504\nEpoch 1/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.7225 - accuracy: 0.5095 - val_loss: 0.6905 - val_accuracy: 0.5349\nEpoch 2/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6945 - accuracy: 0.5344 - val_loss: 0.6906 - val_accuracy: 0.5388\nEpoch 3/30\n2326/2326 [==============================] - 2s 950us/step - loss: 0.6901 - accuracy: 0.5249 - val_loss: 0.6913 - val_accuracy: 0.5349\nEpoch 4/30\n2326/2326 [==============================] - 2s 959us/step - loss: 0.6881 - accuracy: 0.5322 - val_loss: 0.6906 - val_accuracy: 0.5310\nEpoch 5/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6907 - accuracy: 0.5353 - val_loss: 0.6910 - val_accuracy: 0.5271\nEpoch 6/30\n2326/2326 [==============================] - 2s 948us/step - loss: 0.6880 - accuracy: 0.5408 - val_loss: 0.6919 - val_accuracy: 0.5349\nEpoch 7/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6899 - accuracy: 0.5310 - val_loss: 0.6962 - val_accuracy: 0.5271\nEpoch 8/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6887 - accuracy: 0.5439 - val_loss: 0.6948 - val_accuracy: 0.5233\nEpoch 9/30\n2326/2326 [==============================] - 2s 973us/step - loss: 0.6894 - accuracy: 0.5387 - val_loss: 0.6946 - val_accuracy: 0.5155\nEpoch 10/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6868 - accuracy: 0.5353 - val_loss: 0.6955 - val_accuracy: 0.5078\nEpoch 11/30\n2326/2326 [==============================] - 2s 976us/step - loss: 0.6874 - accuracy: 0.5439 - val_loss: 0.6925 - val_accuracy: 0.5233\nEpoch 12/30\n2326/2326 [==============================] - 2s 963us/step - loss: 0.6870 - accuracy: 0.5447 - val_loss: 0.6912 - val_accuracy: 0.5349\nEpoch 13/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6851 - accuracy: 0.5473 - val_loss: 0.6941 - val_accuracy: 0.5310\nEpoch 14/30\n2326/2326 [==============================] - 2s 944us/step - loss: 0.6878 - accuracy: 0.5357 - val_loss: 0.6931 - val_accuracy: 0.5349\nEpoch 15/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6841 - accuracy: 0.5494 - val_loss: 0.6949 - val_accuracy: 0.5078\nEpoch 16/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6854 - accuracy: 0.5456 - val_loss: 0.6922 - val_accuracy: 0.5388\nEpoch 17/30\n2326/2326 [==============================] - 2s 974us/step - loss: 0.6863 - accuracy: 0.5447 - val_loss: 0.6923 - val_accuracy: 0.5271\nEpoch 18/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6854 - accuracy: 0.5456 - val_loss: 0.6929 - val_accuracy: 0.5465\nEpoch 19/30\n2326/2326 [==============================] - 2s 994us/step - loss: 0.6839 - accuracy: 0.5516 - val_loss: 0.6942 - val_accuracy: 0.5271\nEpoch 20/30\n2326/2326 [==============================] - 2s 983us/step - loss: 0.6859 - accuracy: 0.5396 - val_loss: 0.6927 - val_accuracy: 0.5388\nEpoch 21/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6878 - accuracy: 0.5284 - val_loss: 0.6926 - val_accuracy: 0.5426\nEpoch 22/30\n2326/2326 [==============================] - 2s 978us/step - loss: 0.6850 - accuracy: 0.5361 - val_loss: 0.6944 - val_accuracy: 0.5233\nEpoch 23/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6827 - accuracy: 0.5456 - val_loss: 0.6925 - val_accuracy: 0.5388\nEpoch 24/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6861 - accuracy: 0.5327 - val_loss: 0.6913 - val_accuracy: 0.5504\nEpoch 25/30\n2326/2326 [==============================] - 2s 995us/step - loss: 0.6858 - accuracy: 0.5383 - val_loss: 0.6928 - val_accuracy: 0.5465\nEpoch 26/30\n2326/2326 [==============================] - 3s 1ms/step - loss: 0.6859 - accuracy: 0.5383 - val_loss: 0.6925 - val_accuracy: 0.5465\nEpoch 27/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6836 - accuracy: 0.5335 - val_loss: 0.6942 - val_accuracy: 0.5271\nEpoch 28/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6859 - accuracy: 0.5482 - val_loss: 0.6941 - val_accuracy: 0.5349\nEpoch 29/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6851 - accuracy: 0.5357 - val_loss: 0.6923 - val_accuracy: 0.5233\nEpoch 30/30\n2326/2326 [==============================] - 2s 1ms/step - loss: 0.6833 - accuracy: 0.5473 - val_loss: 0.6949 - val_accuracy: 0.5039\n"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "K = 10\n",
    "kf =  StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "historys = []\n",
    "for train_index, test_index in kf.split(X_std,y):\n",
    "    X_train, X_val = X_std.iloc[train_index], X_std.iloc[test_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "    opt = optimizers.(learning_rate=0.01)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(X.shape[1],)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    batch_size = 1\n",
    "    epochs = 30\n",
    "    history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs= epochs,\n",
    "                    batch_size= batch_size,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)\n",
    "    pred_i = model.predict(X_val)[:,0]\n",
    "    oof_pred[test_index] = pred_i\n",
    "    historys.append(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5259287925696594\n"
    }
   ],
   "source": [
    "print(accuracy_score(y, (oof_pred > 0.50).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(oof_pred < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5477707006369427\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 393.783366 277.314375\" width=\"393.783366pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 277.314375 \n",
       "L 393.783366 277.314375 \n",
       "L 393.783366 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"mb5e76f1e97\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#mb5e76f1e97\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(62.180682 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"117.838421\" xlink:href=\"#mb5e76f1e97\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(114.657171 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.31491\" xlink:href=\"#mb5e76f1e97\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(163.95241 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"222.791399\" xlink:href=\"#mb5e76f1e97\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(216.428899 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"275.267888\" xlink:href=\"#mb5e76f1e97\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(268.905388 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"327.744377\" xlink:href=\"#mb5e76f1e97\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(321.381877 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"380.220866\" xlink:href=\"#mb5e76f1e97\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 30 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(373.858366 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- Epoch -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 55.90625 72.90625 \n",
       "L 55.90625 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.015625 \n",
       "L 54.390625 43.015625 \n",
       "L 54.390625 34.71875 \n",
       "L 19.671875 34.71875 \n",
       "L 19.671875 8.296875 \n",
       "L 56.78125 8.296875 \n",
       "L 56.78125 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-69\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(202.232813 268.034687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-69\"/>\n",
       "      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m81d09493de\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m81d09493de\" y=\"213.368852\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.49 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "       <path d=\"M 10.984375 1.515625 \n",
       "L 10.984375 10.5 \n",
       "Q 14.703125 8.734375 18.5 7.8125 \n",
       "Q 22.3125 6.890625 25.984375 6.890625 \n",
       "Q 35.75 6.890625 40.890625 13.453125 \n",
       "Q 46.046875 20.015625 46.78125 33.40625 \n",
       "Q 43.953125 29.203125 39.59375 26.953125 \n",
       "Q 35.25 24.703125 29.984375 24.703125 \n",
       "Q 19.046875 24.703125 12.671875 31.3125 \n",
       "Q 6.296875 37.9375 6.296875 49.421875 \n",
       "Q 6.296875 60.640625 12.9375 67.421875 \n",
       "Q 19.578125 74.21875 30.609375 74.21875 \n",
       "Q 43.265625 74.21875 49.921875 64.515625 \n",
       "Q 56.59375 54.828125 56.59375 36.375 \n",
       "Q 56.59375 19.140625 48.40625 8.859375 \n",
       "Q 40.234375 -1.421875 26.421875 -1.421875 \n",
       "Q 22.703125 -1.421875 18.890625 -0.6875 \n",
       "Q 15.09375 0.046875 10.984375 1.515625 \n",
       "z\n",
       "M 30.609375 32.421875 \n",
       "Q 37.25 32.421875 41.125 36.953125 \n",
       "Q 45.015625 41.5 45.015625 49.421875 \n",
       "Q 45.015625 57.28125 41.125 61.84375 \n",
       "Q 37.25 66.40625 30.609375 66.40625 \n",
       "Q 23.96875 66.40625 20.09375 61.84375 \n",
       "Q 16.21875 57.28125 16.21875 49.421875 \n",
       "Q 16.21875 41.5 20.09375 36.953125 \n",
       "Q 23.96875 32.421875 30.609375 32.421875 \n",
       "z\n",
       "\" id=\"DejaVuSans-57\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 217.168071)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-57\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m81d09493de\" y=\"182.473044\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.50 -->\n",
       "      <g transform=\"translate(20.878125 186.272263)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m81d09493de\" y=\"151.577236\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.51 -->\n",
       "      <g transform=\"translate(20.878125 155.376455)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m81d09493de\" y=\"120.681428\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.52 -->\n",
       "      <g transform=\"translate(20.878125 124.480647)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m81d09493de\" y=\"89.78562\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.53 -->\n",
       "      <g transform=\"translate(20.878125 93.584839)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m81d09493de\" y=\"58.889812\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.54 -->\n",
       "      <g transform=\"translate(20.878125 62.689031)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m81d09493de\" y=\"27.994004\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.55 -->\n",
       "      <g transform=\"translate(20.878125 31.793223)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- Accuracy -->\n",
       "     <defs>\n",
       "      <path d=\"M 34.1875 63.1875 \n",
       "L 20.796875 26.90625 \n",
       "L 47.609375 26.90625 \n",
       "z\n",
       "M 28.609375 72.90625 \n",
       "L 39.796875 72.90625 \n",
       "L 67.578125 0 \n",
       "L 57.328125 0 \n",
       "L 50.6875 18.703125 \n",
       "L 17.828125 18.703125 \n",
       "L 11.1875 0 \n",
       "L 0.78125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-65\"/>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 32.171875 -5.078125 \n",
       "Q 28.375 -14.84375 24.75 -17.8125 \n",
       "Q 21.140625 -20.796875 15.09375 -20.796875 \n",
       "L 7.90625 -20.796875 \n",
       "L 7.90625 -13.28125 \n",
       "L 13.1875 -13.28125 \n",
       "Q 16.890625 -13.28125 18.9375 -11.515625 \n",
       "Q 21 -9.765625 23.484375 -3.21875 \n",
       "L 25.09375 0.875 \n",
       "L 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 11.921875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-121\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 153.952969)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use x=\"68.392578\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"123.373047\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"178.353516\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "      <use x=\"241.732422\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"282.845703\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"344.125\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"399.105469\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path clip-path=\"url(#p2b3e64d64a)\" d=\"M 65.361932 207.078733 \n",
       "L 75.85723 162.333664 \n",
       "L 86.352527 154.731739 \n",
       "L 96.847825 145.351003 \n",
       "L 107.343123 125.193152 \n",
       "L 117.838421 142.201428 \n",
       "L 128.333719 104.642281 \n",
       "L 138.829016 97.476395 \n",
       "L 149.324314 86.755575 \n",
       "L 159.819612 85.851382 \n",
       "L 170.31491 82.745452 \n",
       "L 180.810208 74.689294 \n",
       "L 191.305505 100.620409 \n",
       "L 201.800803 75.579271 \n",
       "L 212.296101 72.450431 \n",
       "L 222.791399 49.649409 \n",
       "L 233.286697 88.55247 \n",
       "L 243.781995 65.747544 \n",
       "L 254.277292 57.247429 \n",
       "L 264.77259 62.166755 \n",
       "L 275.267888 49.636113 \n",
       "L 285.763186 46.507937 \n",
       "L 296.258484 59.032391 \n",
       "L 306.753781 38.012647 \n",
       "L 317.249079 42.935803 \n",
       "L 327.744377 32.201761 \n",
       "L 338.239675 38.015888 \n",
       "L 348.734973 38.904981 \n",
       "L 359.23027 50.093587 \n",
       "L 369.725568 38.019129 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path clip-path=\"url(#p2b3e64d64a)\" d=\"M 65.361932 229.874489 \n",
       "L 75.85723 224.459754 \n",
       "L 86.352527 174.428949 \n",
       "L 96.847825 160.063955 \n",
       "L 107.343123 120.779641 \n",
       "L 117.838421 102.910319 \n",
       "L 128.333719 97.58383 \n",
       "L 138.829016 99.338591 \n",
       "L 149.324314 58.154348 \n",
       "L 159.819612 52.80705 \n",
       "L 170.31491 65.360159 \n",
       "L 180.810208 65.308264 \n",
       "L 191.305505 67.104496 \n",
       "L 201.800803 65.31342 \n",
       "L 212.296101 52.791507 \n",
       "L 222.791399 43.830973 \n",
       "L 233.286697 40.243628 \n",
       "L 243.781995 38.462939 \n",
       "L 254.277292 40.243628 \n",
       "L 264.77259 38.468132 \n",
       "L 275.267888 42.050283 \n",
       "L 285.763186 42.050283 \n",
       "L 296.258484 40.264401 \n",
       "L 306.753781 40.264401 \n",
       "L 317.249079 40.264401 \n",
       "L 327.744377 38.468132 \n",
       "L 338.239675 38.468132 \n",
       "L 348.734973 38.468132 \n",
       "L 359.23027 33.079326 \n",
       "L 369.725568 34.875595 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- Model accuracy -->\n",
       "    <defs>\n",
       "     <path d=\"M 9.8125 72.90625 \n",
       "L 24.515625 72.90625 \n",
       "L 43.109375 23.296875 \n",
       "L 61.8125 72.90625 \n",
       "L 76.515625 72.90625 \n",
       "L 76.515625 0 \n",
       "L 66.890625 0 \n",
       "L 66.890625 64.015625 \n",
       "L 48.09375 14.015625 \n",
       "L 38.1875 14.015625 \n",
       "L 19.390625 64.015625 \n",
       "L 19.390625 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-77\"/>\n",
       "     <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "     <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "     <path id=\"DejaVuSans-32\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(170.549688 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"393.310547\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"448.291016\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"503.271484\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "     <use x=\"566.650391\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"607.763672\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"669.042969\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"724.023438\" xlink:href=\"#DejaVuSans-121\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 57.14375 59.674375 \n",
       "L 114.584375 59.674375 \n",
       "Q 116.584375 59.674375 116.584375 57.674375 \n",
       "L 116.584375 29.318125 \n",
       "Q 116.584375 27.318125 114.584375 27.318125 \n",
       "L 57.14375 27.318125 \n",
       "Q 55.14375 27.318125 55.14375 29.318125 \n",
       "L 55.14375 57.674375 \n",
       "Q 55.14375 59.674375 57.14375 59.674375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 59.14375 35.416562 \n",
       "L 79.14375 35.416562 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\"/>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- Train -->\n",
       "     <defs>\n",
       "      <path d=\"M -0.296875 72.90625 \n",
       "L 61.375 72.90625 \n",
       "L 61.375 64.59375 \n",
       "L 35.5 64.59375 \n",
       "L 35.5 0 \n",
       "L 25.59375 0 \n",
       "L 25.59375 64.59375 \n",
       "L -0.296875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-84\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(87.14375 38.916562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-84\"/>\n",
       "      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"163.257812\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"191.041016\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\">\n",
       "     <path d=\"M 59.14375 50.094687 \n",
       "L 79.14375 50.094687 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\"/>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- Val -->\n",
       "     <defs>\n",
       "      <path d=\"M 28.609375 0 \n",
       "L 0.78125 72.90625 \n",
       "L 11.078125 72.90625 \n",
       "L 34.1875 11.53125 \n",
       "L 57.328125 72.90625 \n",
       "L 67.578125 72.90625 \n",
       "L 39.796875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-86\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(87.14375 53.594687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-86\"/>\n",
       "      <use x=\"68.298828\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"129.578125\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p2b3e64d64a\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 393.783366 277.314375\" width=\"393.783366pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 277.314375 \n",
       "L 393.783366 277.314375 \n",
       "L 393.783366 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "L 50.14375 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"ma285ed6338\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#ma285ed6338\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(62.180682 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"117.838421\" xlink:href=\"#ma285ed6338\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(114.657171 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.31491\" xlink:href=\"#ma285ed6338\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 10 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(163.95241 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"222.791399\" xlink:href=\"#ma285ed6338\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(216.428899 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"275.267888\" xlink:href=\"#ma285ed6338\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(268.905388 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"327.744377\" xlink:href=\"#ma285ed6338\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 25 -->\n",
       "      <g transform=\"translate(321.381877 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"380.220866\" xlink:href=\"#ma285ed6338\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 30 -->\n",
       "      <defs>\n",
       "       <path d=\"M 40.578125 39.3125 \n",
       "Q 47.65625 37.796875 51.625 33 \n",
       "Q 55.609375 28.21875 55.609375 21.1875 \n",
       "Q 55.609375 10.40625 48.1875 4.484375 \n",
       "Q 40.765625 -1.421875 27.09375 -1.421875 \n",
       "Q 22.515625 -1.421875 17.65625 -0.515625 \n",
       "Q 12.796875 0.390625 7.625 2.203125 \n",
       "L 7.625 11.71875 \n",
       "Q 11.71875 9.328125 16.59375 8.109375 \n",
       "Q 21.484375 6.890625 26.8125 6.890625 \n",
       "Q 36.078125 6.890625 40.9375 10.546875 \n",
       "Q 45.796875 14.203125 45.796875 21.1875 \n",
       "Q 45.796875 27.640625 41.28125 31.265625 \n",
       "Q 36.765625 34.90625 28.71875 34.90625 \n",
       "L 20.21875 34.90625 \n",
       "L 20.21875 43.015625 \n",
       "L 29.109375 43.015625 \n",
       "Q 36.375 43.015625 40.234375 45.921875 \n",
       "Q 44.09375 48.828125 44.09375 54.296875 \n",
       "Q 44.09375 59.90625 40.109375 62.90625 \n",
       "Q 36.140625 65.921875 28.71875 65.921875 \n",
       "Q 24.65625 65.921875 20.015625 65.03125 \n",
       "Q 15.375 64.15625 9.8125 62.3125 \n",
       "L 9.8125 71.09375 \n",
       "Q 15.4375 72.65625 20.34375 73.4375 \n",
       "Q 25.25 74.21875 29.59375 74.21875 \n",
       "Q 40.828125 74.21875 47.359375 69.109375 \n",
       "Q 53.90625 64.015625 53.90625 55.328125 \n",
       "Q 53.90625 49.265625 50.4375 45.09375 \n",
       "Q 46.96875 40.921875 40.578125 39.3125 \n",
       "z\n",
       "\" id=\"DejaVuSans-51\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(373.858366 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-51\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- Epoch -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 55.90625 72.90625 \n",
       "L 55.90625 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.015625 \n",
       "L 54.390625 43.015625 \n",
       "L 54.390625 34.71875 \n",
       "L 19.671875 34.71875 \n",
       "L 19.671875 8.296875 \n",
       "L 56.78125 8.296875 \n",
       "L 56.78125 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-69\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(202.232813 268.034687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-69\"/>\n",
       "      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m4ab9bc412d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"217.798438\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.70 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "       <path d=\"M 8.203125 72.90625 \n",
       "L 55.078125 72.90625 \n",
       "L 55.078125 68.703125 \n",
       "L 28.609375 0 \n",
       "L 18.3125 0 \n",
       "L 43.21875 64.59375 \n",
       "L 8.203125 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-55\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 221.597657)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"193.351268\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.72 -->\n",
       "      <g transform=\"translate(20.878125 197.150486)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"168.904097\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.74 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 172.703316)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"144.456927\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.76 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 148.256145)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"120.009756\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.78 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 123.808975)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"95.562585\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.80 -->\n",
       "      <g transform=\"translate(20.878125 99.361804)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"71.115415\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_15\">\n",
       "      <!-- 0.82 -->\n",
       "      <g transform=\"translate(20.878125 74.914633)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_8\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4ab9bc412d\" y=\"46.668244\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_16\">\n",
       "      <!-- 0.84 -->\n",
       "      <g transform=\"translate(20.878125 50.467463)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- Loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 19.671875 72.90625 \n",
       "L 19.671875 8.296875 \n",
       "L 55.171875 8.296875 \n",
       "L 55.171875 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-76\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 142.092031)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use x=\"55.697266\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"116.878906\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"168.978516\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path clip-path=\"url(#p87d0975da7)\" d=\"M 65.361932 32.201761 \n",
       "L 75.85723 42.965329 \n",
       "L 86.352527 95.065386 \n",
       "L 96.847825 113.187563 \n",
       "L 107.343123 126.099065 \n",
       "L 117.838421 152.358634 \n",
       "L 128.333719 164.934841 \n",
       "L 138.829016 133.765501 \n",
       "L 149.324314 159.063195 \n",
       "L 159.819612 192.503527 \n",
       "L 170.31491 188.151631 \n",
       "L 180.810208 209.74514 \n",
       "L 191.305505 197.105691 \n",
       "L 201.800803 201.311506 \n",
       "L 212.296101 209.415165 \n",
       "L 222.791399 203.076279 \n",
       "L 233.286697 209.19914 \n",
       "L 243.781995 213.320002 \n",
       "L 254.277292 221.236852 \n",
       "L 264.77259 226.1032 \n",
       "L 275.267888 211.265108 \n",
       "L 285.763186 219.599654 \n",
       "L 296.258484 216.565036 \n",
       "L 306.753781 219.033983 \n",
       "L 317.249079 226.172343 \n",
       "L 327.744377 226.787558 \n",
       "L 338.239675 221.929661 \n",
       "L 348.734973 220.3482 \n",
       "L 359.23027 229.874489 \n",
       "L 369.725568 226.7759 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path clip-path=\"url(#p87d0975da7)\" d=\"M 65.361932 171.553648 \n",
       "L 75.85723 192.468409 \n",
       "L 86.352527 200.647403 \n",
       "L 96.847825 206.807787 \n",
       "L 107.343123 210.459369 \n",
       "L 117.838421 213.265869 \n",
       "L 128.333719 215.006816 \n",
       "L 138.829016 216.143988 \n",
       "L 149.324314 216.02042 \n",
       "L 159.819612 217.903223 \n",
       "L 170.31491 218.643681 \n",
       "L 180.810208 220.339676 \n",
       "L 191.305505 221.669412 \n",
       "L 201.800803 221.947584 \n",
       "L 212.296101 222.946835 \n",
       "L 222.791399 223.475786 \n",
       "L 233.286697 223.540266 \n",
       "L 243.781995 224.576091 \n",
       "L 254.277292 225.286168 \n",
       "L 264.77259 225.879088 \n",
       "L 275.267888 226.039522 \n",
       "L 285.763186 226.608181 \n",
       "L 296.258484 227.025294 \n",
       "L 306.753781 227.705936 \n",
       "L 317.249079 227.68131 \n",
       "L 327.744377 228.016603 \n",
       "L 338.239675 228.54672 \n",
       "L 348.734973 228.837279 \n",
       "L 359.23027 229.084414 \n",
       "L 369.725568 229.059861 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 50.14375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 384.94375 239.758125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 239.758125 \n",
       "L 384.94375 239.758125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 22.318125 \n",
       "L 384.94375 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- Model loss -->\n",
       "    <defs>\n",
       "     <path d=\"M 9.8125 72.90625 \n",
       "L 24.515625 72.90625 \n",
       "L 43.109375 23.296875 \n",
       "L 61.8125 72.90625 \n",
       "L 76.515625 72.90625 \n",
       "L 76.515625 0 \n",
       "L 66.890625 0 \n",
       "L 66.890625 64.015625 \n",
       "L 48.09375 14.015625 \n",
       "L 38.1875 14.015625 \n",
       "L 19.390625 64.015625 \n",
       "L 19.390625 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-77\"/>\n",
       "     <path d=\"M 45.40625 46.390625 \n",
       "L 45.40625 75.984375 \n",
       "L 54.390625 75.984375 \n",
       "L 54.390625 0 \n",
       "L 45.40625 0 \n",
       "L 45.40625 8.203125 \n",
       "Q 42.578125 3.328125 38.25 0.953125 \n",
       "Q 33.9375 -1.421875 27.875 -1.421875 \n",
       "Q 17.96875 -1.421875 11.734375 6.484375 \n",
       "Q 5.515625 14.40625 5.515625 27.296875 \n",
       "Q 5.515625 40.1875 11.734375 48.09375 \n",
       "Q 17.96875 56 27.875 56 \n",
       "Q 33.9375 56 38.25 53.625 \n",
       "Q 42.578125 51.265625 45.40625 46.390625 \n",
       "z\n",
       "M 14.796875 27.296875 \n",
       "Q 14.796875 17.390625 18.875 11.75 \n",
       "Q 22.953125 6.109375 30.078125 6.109375 \n",
       "Q 37.203125 6.109375 41.296875 11.75 \n",
       "Q 45.40625 17.390625 45.40625 27.296875 \n",
       "Q 45.40625 37.203125 41.296875 42.84375 \n",
       "Q 37.203125 48.484375 30.078125 48.484375 \n",
       "Q 22.953125 48.484375 18.875 42.84375 \n",
       "Q 14.796875 37.203125 14.796875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-100\"/>\n",
       "     <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "     <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "     <path id=\"DejaVuSans-32\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(186.031563 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-77\"/>\n",
       "     <use x=\"86.279297\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"147.460938\" xlink:href=\"#DejaVuSans-100\"/>\n",
       "     <use x=\"210.9375\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"272.460938\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"300.244141\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"332.03125\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"359.814453\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"420.996094\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"473.095703\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 57.14375 59.674375 \n",
       "L 114.584375 59.674375 \n",
       "Q 116.584375 59.674375 116.584375 57.674375 \n",
       "L 116.584375 29.318125 \n",
       "Q 116.584375 27.318125 114.584375 27.318125 \n",
       "L 57.14375 27.318125 \n",
       "Q 55.14375 27.318125 55.14375 29.318125 \n",
       "L 55.14375 57.674375 \n",
       "Q 55.14375 59.674375 57.14375 59.674375 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 59.14375 35.416562 \n",
       "L 79.14375 35.416562 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_19\"/>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- Train -->\n",
       "     <defs>\n",
       "      <path d=\"M -0.296875 72.90625 \n",
       "L 61.375 72.90625 \n",
       "L 61.375 64.59375 \n",
       "L 35.5 64.59375 \n",
       "L 35.5 0 \n",
       "L 25.59375 0 \n",
       "L 25.59375 64.59375 \n",
       "L -0.296875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-84\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(87.14375 38.916562)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-84\"/>\n",
       "      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"163.257812\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"191.041016\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_20\">\n",
       "     <path d=\"M 59.14375 50.094687 \n",
       "L 79.14375 50.094687 \n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_21\"/>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- Val -->\n",
       "     <defs>\n",
       "      <path d=\"M 28.609375 0 \n",
       "L 0.78125 72.90625 \n",
       "L 11.078125 72.90625 \n",
       "L 34.1875 11.53125 \n",
       "L 57.328125 72.90625 \n",
       "L 67.578125 72.90625 \n",
       "L 39.796875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-86\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(87.14375 53.594687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-86\"/>\n",
       "      <use x=\"68.298828\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"129.578125\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p87d0975da7\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 277.314375 \n",
       "L 385.78125 277.314375 \n",
       "L 385.78125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 239.758125 \n",
       "L 378.58125 239.758125 \n",
       "L 378.58125 22.318125 \n",
       "L 43.78125 22.318125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m581ed9702d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m581ed9702d\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(51.047869 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.872159\" xlink:href=\"#m581ed9702d\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.2 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(111.920597 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.744886\" xlink:href=\"#m581ed9702d\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.4 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(172.793324 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.617614\" xlink:href=\"#m581ed9702d\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.6 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(233.666051 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.490341\" xlink:href=\"#m581ed9702d\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.8 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(294.538778 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m581ed9702d\" y=\"239.758125\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1.0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(355.411506 254.356562)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- False Positive Rate -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.8125 72.90625 \n",
       "L 51.703125 72.90625 \n",
       "L 51.703125 64.59375 \n",
       "L 19.671875 64.59375 \n",
       "L 19.671875 43.109375 \n",
       "L 48.578125 43.109375 \n",
       "L 48.578125 34.8125 \n",
       "L 19.671875 34.8125 \n",
       "L 19.671875 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-70\"/>\n",
       "      <path d=\"M 34.28125 27.484375 \n",
       "Q 23.390625 27.484375 19.1875 25 \n",
       "Q 14.984375 22.515625 14.984375 16.5 \n",
       "Q 14.984375 11.71875 18.140625 8.90625 \n",
       "Q 21.296875 6.109375 26.703125 6.109375 \n",
       "Q 34.1875 6.109375 38.703125 11.40625 \n",
       "Q 43.21875 16.703125 43.21875 25.484375 \n",
       "L 43.21875 27.484375 \n",
       "z\n",
       "M 52.203125 31.203125 \n",
       "L 52.203125 0 \n",
       "L 43.21875 0 \n",
       "L 43.21875 8.296875 \n",
       "Q 40.140625 3.328125 35.546875 0.953125 \n",
       "Q 30.953125 -1.421875 24.3125 -1.421875 \n",
       "Q 15.921875 -1.421875 10.953125 3.296875 \n",
       "Q 6 8.015625 6 15.921875 \n",
       "Q 6 25.140625 12.171875 29.828125 \n",
       "Q 18.359375 34.515625 30.609375 34.515625 \n",
       "L 43.21875 34.515625 \n",
       "L 43.21875 35.40625 \n",
       "Q 43.21875 41.609375 39.140625 45 \n",
       "Q 35.0625 48.390625 27.6875 48.390625 \n",
       "Q 23 48.390625 18.546875 47.265625 \n",
       "Q 14.109375 46.140625 10.015625 43.890625 \n",
       "L 10.015625 52.203125 \n",
       "Q 14.9375 54.109375 19.578125 55.046875 \n",
       "Q 24.21875 56 28.609375 56 \n",
       "Q 40.484375 56 46.34375 49.84375 \n",
       "Q 52.203125 43.703125 52.203125 31.203125 \n",
       "z\n",
       "\" id=\"DejaVuSans-97\"/>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path id=\"DejaVuSans-32\"/>\n",
       "      <path d=\"M 19.671875 64.796875 \n",
       "L 19.671875 37.40625 \n",
       "L 32.078125 37.40625 \n",
       "Q 38.96875 37.40625 42.71875 40.96875 \n",
       "Q 46.484375 44.53125 46.484375 51.125 \n",
       "Q 46.484375 57.671875 42.71875 61.234375 \n",
       "Q 38.96875 64.796875 32.078125 64.796875 \n",
       "z\n",
       "M 9.8125 72.90625 \n",
       "L 32.078125 72.90625 \n",
       "Q 44.34375 72.90625 50.609375 67.359375 \n",
       "Q 56.890625 61.8125 56.890625 51.125 \n",
       "Q 56.890625 40.328125 50.609375 34.8125 \n",
       "Q 44.34375 29.296875 32.078125 29.296875 \n",
       "L 19.671875 29.296875 \n",
       "L 19.671875 0 \n",
       "L 9.8125 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-80\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 9.421875 54.6875 \n",
       "L 18.40625 54.6875 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 64.59375 \n",
       "L 9.421875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-105\"/>\n",
       "      <path d=\"M 18.3125 70.21875 \n",
       "L 18.3125 54.6875 \n",
       "L 36.8125 54.6875 \n",
       "L 36.8125 47.703125 \n",
       "L 18.3125 47.703125 \n",
       "L 18.3125 18.015625 \n",
       "Q 18.3125 11.328125 20.140625 9.421875 \n",
       "Q 21.96875 7.515625 27.59375 7.515625 \n",
       "L 36.8125 7.515625 \n",
       "L 36.8125 0 \n",
       "L 27.59375 0 \n",
       "Q 17.1875 0 13.234375 3.875 \n",
       "Q 9.28125 7.765625 9.28125 18.015625 \n",
       "L 9.28125 47.703125 \n",
       "L 2.6875 47.703125 \n",
       "L 2.6875 54.6875 \n",
       "L 9.28125 54.6875 \n",
       "L 9.28125 70.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-116\"/>\n",
       "      <path d=\"M 2.984375 54.6875 \n",
       "L 12.5 54.6875 \n",
       "L 29.59375 8.796875 \n",
       "L 46.6875 54.6875 \n",
       "L 56.203125 54.6875 \n",
       "L 35.6875 0 \n",
       "L 23.484375 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-118\"/>\n",
       "      <path d=\"M 44.390625 34.1875 \n",
       "Q 47.5625 33.109375 50.5625 29.59375 \n",
       "Q 53.5625 26.078125 56.59375 19.921875 \n",
       "L 66.609375 0 \n",
       "L 56 0 \n",
       "L 46.6875 18.703125 \n",
       "Q 43.0625 26.03125 39.671875 28.421875 \n",
       "Q 36.28125 30.8125 30.421875 30.8125 \n",
       "L 19.671875 30.8125 \n",
       "L 19.671875 0 \n",
       "L 9.8125 0 \n",
       "L 9.8125 72.90625 \n",
       "L 32.078125 72.90625 \n",
       "Q 44.578125 72.90625 50.734375 67.671875 \n",
       "Q 56.890625 62.453125 56.890625 51.90625 \n",
       "Q 56.890625 45.015625 53.6875 40.46875 \n",
       "Q 50.484375 35.9375 44.390625 34.1875 \n",
       "z\n",
       "M 19.671875 64.796875 \n",
       "L 19.671875 38.921875 \n",
       "L 32.078125 38.921875 \n",
       "Q 39.203125 38.921875 42.84375 42.21875 \n",
       "Q 46.484375 45.515625 46.484375 51.90625 \n",
       "Q 46.484375 58.296875 42.84375 61.546875 \n",
       "Q 39.203125 64.796875 32.078125 64.796875 \n",
       "z\n",
       "\" id=\"DejaVuSans-82\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(163.975781 268.034687)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "      <use x=\"57.378906\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"118.658203\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"146.441406\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"198.541016\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"260.064453\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"291.851562\" xlink:href=\"#DejaVuSans-80\"/>\n",
       "      <use x=\"352.107422\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"413.289062\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"465.388672\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"493.171875\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"532.380859\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"560.164062\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"619.34375\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"680.867188\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"712.654297\" xlink:href=\"#DejaVuSans-82\"/>\n",
       "      <use x=\"782.105469\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"843.384766\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"882.59375\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m243b8ebeeb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m243b8ebeeb\" y=\"229.874489\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(20.878125 233.673707)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m243b8ebeeb\" y=\"190.339943\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(20.878125 194.139162)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m243b8ebeeb\" y=\"150.805398\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(20.878125 154.604616)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m243b8ebeeb\" y=\"111.270852\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(20.878125 115.070071)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m243b8ebeeb\" y=\"71.736307\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(20.878125 75.535526)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m243b8ebeeb\" y=\"32.201761\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(20.878125 36.00098)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- True Positive Rate -->\n",
       "     <defs>\n",
       "      <path d=\"M -0.296875 72.90625 \n",
       "L 61.375 72.90625 \n",
       "L 61.375 64.59375 \n",
       "L 35.5 64.59375 \n",
       "L 35.5 0 \n",
       "L 25.59375 0 \n",
       "L 25.59375 64.59375 \n",
       "L -0.296875 64.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-84\"/>\n",
       "      <path d=\"M 41.109375 46.296875 \n",
       "Q 39.59375 47.171875 37.8125 47.578125 \n",
       "Q 36.03125 48 33.890625 48 \n",
       "Q 26.265625 48 22.1875 43.046875 \n",
       "Q 18.109375 38.09375 18.109375 28.8125 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 20.953125 51.171875 25.484375 53.578125 \n",
       "Q 30.03125 56 36.53125 56 \n",
       "Q 37.453125 56 38.578125 55.875 \n",
       "Q 39.703125 55.765625 41.0625 55.515625 \n",
       "z\n",
       "\" id=\"DejaVuSans-114\"/>\n",
       "      <path d=\"M 8.5 21.578125 \n",
       "L 8.5 54.6875 \n",
       "L 17.484375 54.6875 \n",
       "L 17.484375 21.921875 \n",
       "Q 17.484375 14.15625 20.5 10.265625 \n",
       "Q 23.53125 6.390625 29.59375 6.390625 \n",
       "Q 36.859375 6.390625 41.078125 11.03125 \n",
       "Q 45.3125 15.671875 45.3125 23.6875 \n",
       "L 45.3125 54.6875 \n",
       "L 54.296875 54.6875 \n",
       "L 54.296875 0 \n",
       "L 45.3125 0 \n",
       "L 45.3125 8.40625 \n",
       "Q 42.046875 3.421875 37.71875 1 \n",
       "Q 33.40625 -1.421875 27.6875 -1.421875 \n",
       "Q 18.265625 -1.421875 13.375 4.4375 \n",
       "Q 8.5 10.296875 8.5 21.578125 \n",
       "z\n",
       "M 31.109375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-117\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798438 176.584219)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-84\"/>\n",
       "      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "      <use x=\"165.357422\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"226.880859\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"258.667969\" xlink:href=\"#DejaVuSans-80\"/>\n",
       "      <use x=\"318.923828\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"380.105469\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"432.205078\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"459.988281\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"499.197266\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "      <use x=\"526.980469\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"586.160156\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"647.683594\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"679.470703\" xlink:href=\"#DejaVuSans-82\"/>\n",
       "      <use x=\"748.921875\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"810.201172\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "      <use x=\"849.410156\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path clip-path=\"url(#p19afa1a8ed)\" d=\"M 58.999432 229.874489 \n",
       "L 59.390143 229.874489 \n",
       "L 59.390143 229.665973 \n",
       "L 59.780853 229.665973 \n",
       "L 59.780853 229.457458 \n",
       "L 60.952985 229.457458 \n",
       "L 60.952985 228.831911 \n",
       "L 61.343696 228.831911 \n",
       "L 61.343696 227.789333 \n",
       "L 62.125117 227.789333 \n",
       "L 62.125117 227.580818 \n",
       "L 63.29725 227.580818 \n",
       "L 63.29725 226.329725 \n",
       "L 63.68796 226.329725 \n",
       "L 63.68796 226.121209 \n",
       "L 64.078671 226.121209 \n",
       "L 64.078671 225.704178 \n",
       "L 64.469382 225.704178 \n",
       "L 64.469382 225.287147 \n",
       "L 64.860092 225.287147 \n",
       "L 64.860092 224.244569 \n",
       "L 65.250803 224.244569 \n",
       "L 65.250803 224.036054 \n",
       "L 66.422935 224.036054 \n",
       "L 66.422935 223.201992 \n",
       "L 66.813646 223.201992 \n",
       "L 66.813646 222.993476 \n",
       "L 68.376489 222.993476 \n",
       "L 68.376489 222.576445 \n",
       "L 68.767199 222.576445 \n",
       "L 68.767199 222.159414 \n",
       "L 69.548621 222.159414 \n",
       "L 69.548621 221.742383 \n",
       "L 69.939331 221.742383 \n",
       "L 69.939331 221.533867 \n",
       "L 70.720753 221.533867 \n",
       "L 70.720753 221.325352 \n",
       "L 71.892885 221.325352 \n",
       "L 71.892885 220.699805 \n",
       "L 72.283596 220.699805 \n",
       "L 72.283596 220.282774 \n",
       "L 73.065017 220.282774 \n",
       "L 73.065017 220.074258 \n",
       "L 74.237149 220.074258 \n",
       "L 74.237149 219.865743 \n",
       "L 74.62786 219.865743 \n",
       "L 74.62786 219.448712 \n",
       "L 75.018571 219.448712 \n",
       "L 75.018571 219.031681 \n",
       "L 75.409281 219.031681 \n",
       "L 75.409281 218.61465 \n",
       "L 75.799992 218.61465 \n",
       "L 75.799992 218.406134 \n",
       "L 76.190703 218.406134 \n",
       "L 76.190703 218.197619 \n",
       "L 77.362835 218.197619 \n",
       "L 77.362835 217.989103 \n",
       "L 78.925678 217.989103 \n",
       "L 78.925678 217.572072 \n",
       "L 79.316388 217.572072 \n",
       "L 79.316388 217.155041 \n",
       "L 80.879231 217.155041 \n",
       "L 80.879231 216.320979 \n",
       "L 81.269942 216.320979 \n",
       "L 81.269942 216.112463 \n",
       "L 81.660652 216.112463 \n",
       "L 81.660652 212.98473 \n",
       "L 83.223495 212.98473 \n",
       "L 83.223495 212.776215 \n",
       "L 83.614206 212.776215 \n",
       "L 83.614206 212.150668 \n",
       "L 84.004917 212.150668 \n",
       "L 84.004917 211.942153 \n",
       "L 84.395627 211.942153 \n",
       "L 84.395627 211.525122 \n",
       "L 84.786338 211.525122 \n",
       "L 84.786338 211.316606 \n",
       "L 85.177049 211.316606 \n",
       "L 85.177049 210.899575 \n",
       "L 85.56776 210.899575 \n",
       "L 85.56776 210.065513 \n",
       "L 86.739892 210.065513 \n",
       "L 86.739892 209.439966 \n",
       "L 88.302734 209.439966 \n",
       "L 88.302734 208.605904 \n",
       "L 89.084156 208.605904 \n",
       "L 89.084156 207.771842 \n",
       "L 89.474867 207.771842 \n",
       "L 89.474867 207.563326 \n",
       "L 89.865577 207.563326 \n",
       "L 89.865577 206.103718 \n",
       "L 91.037709 206.103718 \n",
       "L 91.037709 205.686687 \n",
       "L 91.42842 205.686687 \n",
       "L 91.42842 205.478171 \n",
       "L 91.819131 205.478171 \n",
       "L 91.819131 205.269655 \n",
       "L 92.209841 205.269655 \n",
       "L 92.209841 204.852624 \n",
       "L 92.600552 204.852624 \n",
       "L 92.600552 204.435593 \n",
       "L 92.991263 204.435593 \n",
       "L 92.991263 204.227078 \n",
       "L 93.381974 204.227078 \n",
       "L 93.381974 204.018562 \n",
       "L 94.554106 204.018562 \n",
       "L 94.554106 203.810047 \n",
       "L 95.335527 203.810047 \n",
       "L 95.335527 203.601531 \n",
       "L 96.116948 203.601531 \n",
       "L 96.116948 203.1845 \n",
       "L 96.507659 203.1845 \n",
       "L 96.507659 201.516376 \n",
       "L 96.89837 201.516376 \n",
       "L 96.89837 200.890829 \n",
       "L 97.289081 200.890829 \n",
       "L 97.289081 200.682314 \n",
       "L 97.679791 200.682314 \n",
       "L 97.679791 200.473798 \n",
       "L 98.461213 200.473798 \n",
       "L 98.461213 200.265283 \n",
       "L 99.633345 200.265283 \n",
       "L 99.633345 200.056767 \n",
       "L 100.024055 200.056767 \n",
       "L 100.024055 199.431221 \n",
       "L 101.586898 199.431221 \n",
       "L 101.586898 199.014189 \n",
       "L 102.75903 199.014189 \n",
       "L 102.75903 198.597158 \n",
       "L 103.149741 198.597158 \n",
       "L 103.149741 198.388643 \n",
       "L 103.540452 198.388643 \n",
       "L 103.540452 197.971612 \n",
       "L 103.931162 197.971612 \n",
       "L 103.931162 197.554581 \n",
       "L 104.321873 197.554581 \n",
       "L 104.321873 197.346065 \n",
       "L 105.103295 197.346065 \n",
       "L 105.103295 197.13755 \n",
       "L 105.494005 197.13755 \n",
       "L 105.494005 196.512003 \n",
       "L 106.666137 196.512003 \n",
       "L 106.666137 196.094972 \n",
       "L 107.838269 196.094972 \n",
       "L 107.838269 195.886456 \n",
       "L 108.22898 195.886456 \n",
       "L 108.22898 195.26091 \n",
       "L 109.010402 195.26091 \n",
       "L 109.010402 195.052394 \n",
       "L 109.791823 195.052394 \n",
       "L 109.791823 194.426848 \n",
       "L 110.963955 194.426848 \n",
       "L 110.963955 194.009817 \n",
       "L 111.354666 194.009817 \n",
       "L 111.354666 193.592786 \n",
       "L 111.745377 193.592786 \n",
       "L 111.745377 193.38427 \n",
       "L 112.136087 193.38427 \n",
       "L 112.136087 192.967239 \n",
       "L 112.526798 192.967239 \n",
       "L 112.526798 192.550208 \n",
       "L 114.089641 192.550208 \n",
       "L 114.089641 192.341692 \n",
       "L 114.871062 192.341692 \n",
       "L 114.871062 191.924661 \n",
       "L 116.043194 191.924661 \n",
       "L 116.043194 191.716146 \n",
       "L 117.215326 191.716146 \n",
       "L 117.215326 191.50763 \n",
       "L 117.996748 191.50763 \n",
       "L 117.996748 191.299115 \n",
       "L 118.387458 191.299115 \n",
       "L 118.387458 191.090599 \n",
       "L 118.778169 191.090599 \n",
       "L 118.778169 190.882084 \n",
       "L 119.16888 190.882084 \n",
       "L 119.16888 190.465053 \n",
       "L 119.559591 190.465053 \n",
       "L 119.559591 190.256537 \n",
       "L 119.950301 190.256537 \n",
       "L 119.950301 189.63099 \n",
       "L 121.903855 189.63099 \n",
       "L 121.903855 189.422475 \n",
       "L 122.294565 189.422475 \n",
       "L 122.294565 189.213959 \n",
       "L 123.857408 189.213959 \n",
       "L 123.857408 188.796928 \n",
       "L 124.248119 188.796928 \n",
       "L 124.248119 188.379897 \n",
       "L 125.420251 188.379897 \n",
       "L 125.420251 188.171382 \n",
       "L 125.810962 188.171382 \n",
       "L 125.810962 187.962866 \n",
       "L 126.592383 187.962866 \n",
       "L 126.592383 187.754351 \n",
       "L 126.983094 187.754351 \n",
       "L 126.983094 186.711773 \n",
       "L 127.373805 186.711773 \n",
       "L 127.373805 186.086226 \n",
       "L 127.764515 186.086226 \n",
       "L 127.764515 185.669195 \n",
       "L 128.545937 185.669195 \n",
       "L 128.545937 185.252164 \n",
       "L 128.936647 185.252164 \n",
       "L 128.936647 185.043649 \n",
       "L 129.718069 185.043649 \n",
       "L 129.718069 184.835133 \n",
       "L 130.890201 184.835133 \n",
       "L 130.890201 184.418102 \n",
       "L 131.280912 184.418102 \n",
       "L 131.280912 183.375524 \n",
       "L 131.671622 183.375524 \n",
       "L 131.671622 182.124431 \n",
       "L 132.062333 182.124431 \n",
       "L 132.062333 180.247791 \n",
       "L 132.843754 180.247791 \n",
       "L 132.843754 179.83076 \n",
       "L 133.234465 179.83076 \n",
       "L 133.234465 179.413729 \n",
       "L 133.625176 179.413729 \n",
       "L 133.625176 179.205214 \n",
       "L 135.578729 179.205214 \n",
       "L 135.578729 178.996698 \n",
       "L 136.750861 178.996698 \n",
       "L 136.750861 178.162636 \n",
       "L 141.048679 178.162636 \n",
       "L 141.048679 177.120058 \n",
       "L 141.43939 177.120058 \n",
       "L 141.43939 176.703027 \n",
       "L 141.830101 176.703027 \n",
       "L 141.830101 176.285996 \n",
       "L 142.220811 176.285996 \n",
       "L 142.220811 175.868965 \n",
       "L 143.392943 175.868965 \n",
       "L 143.392943 175.66045 \n",
       "L 144.174365 175.66045 \n",
       "L 144.174365 175.243418 \n",
       "L 144.565075 175.243418 \n",
       "L 144.565075 175.034903 \n",
       "L 145.737208 175.034903 \n",
       "L 145.737208 174.826387 \n",
       "L 146.90934 174.826387 \n",
       "L 146.90934 174.617872 \n",
       "L 147.30005 174.617872 \n",
       "L 147.30005 174.200841 \n",
       "L 147.690761 174.200841 \n",
       "L 147.690761 173.992325 \n",
       "L 148.081472 173.992325 \n",
       "L 148.081472 173.575294 \n",
       "L 148.472182 173.575294 \n",
       "L 148.472182 173.366779 \n",
       "L 148.862893 173.366779 \n",
       "L 148.862893 172.949748 \n",
       "L 149.253604 172.949748 \n",
       "L 149.253604 172.741232 \n",
       "L 149.644315 172.741232 \n",
       "L 149.644315 172.324201 \n",
       "L 150.425736 172.324201 \n",
       "L 150.425736 171.90717 \n",
       "L 151.207157 171.90717 \n",
       "L 151.207157 170.864592 \n",
       "L 151.597868 170.864592 \n",
       "L 151.597868 170.239046 \n",
       "L 151.988579 170.239046 \n",
       "L 151.988579 169.822015 \n",
       "L 152.77 169.822015 \n",
       "L 152.77 169.196468 \n",
       "L 153.551422 169.196468 \n",
       "L 153.551422 168.570921 \n",
       "L 153.942132 168.570921 \n",
       "L 153.942132 168.15389 \n",
       "L 154.332843 168.15389 \n",
       "L 154.332843 167.111313 \n",
       "L 154.723554 167.111313 \n",
       "L 154.723554 166.902797 \n",
       "L 155.504975 166.902797 \n",
       "L 155.504975 166.485766 \n",
       "L 155.895686 166.485766 \n",
       "L 155.895686 166.27725 \n",
       "L 156.286396 166.27725 \n",
       "L 156.286396 165.651704 \n",
       "L 156.677107 165.651704 \n",
       "L 156.677107 165.443188 \n",
       "L 157.067818 165.443188 \n",
       "L 157.067818 165.234673 \n",
       "L 157.458529 165.234673 \n",
       "L 157.458529 165.026157 \n",
       "L 157.849239 165.026157 \n",
       "L 157.849239 164.400611 \n",
       "L 158.630661 164.400611 \n",
       "L 158.630661 164.192095 \n",
       "L 160.193503 164.192095 \n",
       "L 160.193503 163.775064 \n",
       "L 161.756346 163.775064 \n",
       "L 161.756346 163.566548 \n",
       "L 162.537768 163.566548 \n",
       "L 162.537768 163.358033 \n",
       "L 163.319189 163.358033 \n",
       "L 163.319189 162.732486 \n",
       "L 164.10061 162.732486 \n",
       "L 164.10061 162.523971 \n",
       "L 164.491321 162.523971 \n",
       "L 164.491321 161.481393 \n",
       "L 164.882032 161.481393 \n",
       "L 164.882032 161.272878 \n",
       "L 165.272743 161.272878 \n",
       "L 165.272743 161.064362 \n",
       "L 166.054164 161.064362 \n",
       "L 166.054164 160.438815 \n",
       "L 166.444875 160.438815 \n",
       "L 166.444875 160.2303 \n",
       "L 167.226296 160.2303 \n",
       "L 167.226296 160.021784 \n",
       "L 168.007717 160.021784 \n",
       "L 168.007717 159.396238 \n",
       "L 169.57056 159.396238 \n",
       "L 169.57056 158.35366 \n",
       "L 169.961271 158.35366 \n",
       "L 169.961271 157.936629 \n",
       "L 170.742692 157.936629 \n",
       "L 170.742692 157.519598 \n",
       "L 171.133403 157.519598 \n",
       "L 171.133403 156.685536 \n",
       "L 171.524114 156.685536 \n",
       "L 171.524114 156.47702 \n",
       "L 172.305535 156.47702 \n",
       "L 172.305535 156.268505 \n",
       "L 172.696246 156.268505 \n",
       "L 172.696246 155.851474 \n",
       "L 173.086957 155.851474 \n",
       "L 173.086957 155.434443 \n",
       "L 173.477667 155.434443 \n",
       "L 173.477667 155.017412 \n",
       "L 174.259089 155.017412 \n",
       "L 174.259089 154.808896 \n",
       "L 174.649799 154.808896 \n",
       "L 174.649799 154.391865 \n",
       "L 175.04051 154.391865 \n",
       "L 175.04051 154.183349 \n",
       "L 175.821932 154.183349 \n",
       "L 175.821932 153.974834 \n",
       "L 177.384774 153.974834 \n",
       "L 177.384774 153.766318 \n",
       "L 177.775485 153.766318 \n",
       "L 177.775485 153.557803 \n",
       "L 178.166196 153.557803 \n",
       "L 178.166196 152.723741 \n",
       "L 180.51046 152.723741 \n",
       "L 180.51046 152.515225 \n",
       "L 180.901171 152.515225 \n",
       "L 180.901171 152.30671 \n",
       "L 181.682592 152.30671 \n",
       "L 181.682592 151.889679 \n",
       "L 182.073303 151.889679 \n",
       "L 182.073303 151.681163 \n",
       "L 182.464013 151.681163 \n",
       "L 182.464013 150.847101 \n",
       "L 183.636146 150.847101 \n",
       "L 183.636146 149.596008 \n",
       "L 184.417567 149.596008 \n",
       "L 184.417567 148.761945 \n",
       "L 184.808278 148.761945 \n",
       "L 184.808278 148.136399 \n",
       "L 185.198988 148.136399 \n",
       "L 185.198988 147.927883 \n",
       "L 185.589699 147.927883 \n",
       "L 185.589699 147.719368 \n",
       "L 187.543253 147.719368 \n",
       "L 187.543253 147.510852 \n",
       "L 187.933963 147.510852 \n",
       "L 187.933963 147.302337 \n",
       "L 188.324674 147.302337 \n",
       "L 188.324674 146.468275 \n",
       "L 189.496806 146.468275 \n",
       "L 189.496806 145.425697 \n",
       "L 189.887517 145.425697 \n",
       "L 189.887517 145.217181 \n",
       "L 190.278227 145.217181 \n",
       "L 190.278227 145.008666 \n",
       "L 191.45036 145.008666 \n",
       "L 191.45036 144.80015 \n",
       "L 192.622492 144.80015 \n",
       "L 192.622492 144.591635 \n",
       "L 193.013202 144.591635 \n",
       "L 193.013202 143.966088 \n",
       "L 193.403913 143.966088 \n",
       "L 193.403913 143.549057 \n",
       "L 194.185334 143.549057 \n",
       "L 194.185334 142.923511 \n",
       "L 194.966756 142.923511 \n",
       "L 194.966756 142.297964 \n",
       "L 195.357467 142.297964 \n",
       "L 195.357467 142.089448 \n",
       "L 195.748177 142.089448 \n",
       "L 195.748177 140.62984 \n",
       "L 196.920309 140.62984 \n",
       "L 198.873863 138.7532 \n",
       "L 199.264574 138.7532 \n",
       "L 199.264574 137.710622 \n",
       "L 199.655284 137.710622 \n",
       "L 199.655284 136.459529 \n",
       "L 200.436706 136.459529 \n",
       "L 200.436706 136.251013 \n",
       "L 200.827416 136.251013 \n",
       "L 200.827416 135.833982 \n",
       "L 201.999549 135.833982 \n",
       "L 201.999549 135.416951 \n",
       "L 202.78097 135.416951 \n",
       "L 202.78097 134.99992 \n",
       "L 203.171681 134.99992 \n",
       "L 203.171681 134.791405 \n",
       "L 203.953102 134.791405 \n",
       "L 203.953102 134.582889 \n",
       "L 205.515945 134.582889 \n",
       "L 205.515945 134.374374 \n",
       "L 206.297366 134.374374 \n",
       "L 206.297366 133.957342 \n",
       "L 206.688077 133.957342 \n",
       "L 206.688077 133.748827 \n",
       "L 207.078788 133.748827 \n",
       "L 207.078788 133.331796 \n",
       "L 207.860209 133.331796 \n",
       "L 207.860209 133.12328 \n",
       "L 209.813763 133.12328 \n",
       "L 209.813763 132.914765 \n",
       "L 210.985895 132.914765 \n",
       "L 210.985895 132.706249 \n",
       "L 211.376605 132.706249 \n",
       "L 211.376605 132.497734 \n",
       "L 211.767316 132.497734 \n",
       "L 211.767316 131.872187 \n",
       "L 212.548737 131.872187 \n",
       "L 212.548737 130.621094 \n",
       "L 212.939448 130.621094 \n",
       "L 212.939448 130.412578 \n",
       "L 213.330159 130.412578 \n",
       "L 213.330159 129.370001 \n",
       "L 214.502291 129.370001 \n",
       "L 214.502291 129.161485 \n",
       "L 214.893002 129.161485 \n",
       "L 214.893002 128.95297 \n",
       "L 221.925794 125.408206 \n",
       "L 223.097926 125.408206 \n",
       "L 223.097926 125.19969 \n",
       "L 224.270058 125.19969 \n",
       "L 224.270058 124.782659 \n",
       "L 224.660769 124.782659 \n",
       "L 224.660769 124.365628 \n",
       "L 225.05148 124.365628 \n",
       "L 225.05148 124.157112 \n",
       "L 225.442191 124.157112 \n",
       "L 225.442191 123.32305 \n",
       "L 228.177166 123.32305 \n",
       "L 228.177166 123.114535 \n",
       "L 229.349298 123.114535 \n",
       "L 229.349298 122.697504 \n",
       "L 229.740008 122.697504 \n",
       "L 229.740008 122.488988 \n",
       "L 230.130719 122.488988 \n",
       "L 230.130719 122.280473 \n",
       "L 231.302851 122.280473 \n",
       "L 231.302851 121.863441 \n",
       "L 231.693562 121.863441 \n",
       "L 231.693562 120.820864 \n",
       "L 232.084273 120.820864 \n",
       "L 232.084273 120.612348 \n",
       "L 232.474983 120.612348 \n",
       "L 232.474983 119.569771 \n",
       "L 232.865694 119.569771 \n",
       "L 232.865694 119.152739 \n",
       "L 234.037826 119.152739 \n",
       "L 234.037826 118.318677 \n",
       "L 234.819247 118.318677 \n",
       "L 234.819247 116.650553 \n",
       "L 235.209958 116.650553 \n",
       "L 235.209958 115.816491 \n",
       "L 235.99138 115.816491 \n",
       "L 235.99138 115.607975 \n",
       "L 236.38209 115.607975 \n",
       "L 236.38209 115.190944 \n",
       "L 236.772801 115.190944 \n",
       "L 236.772801 114.565398 \n",
       "L 237.163512 114.565398 \n",
       "L 237.163512 113.731336 \n",
       "L 237.944933 113.731336 \n",
       "L 237.944933 113.314305 \n",
       "L 239.117065 113.314305 \n",
       "L 239.117065 112.688758 \n",
       "L 239.507776 112.688758 \n",
       "L 239.507776 112.480242 \n",
       "L 240.679908 112.480242 \n",
       "L 240.679908 112.271727 \n",
       "L 241.070619 112.271727 \n",
       "L 241.070619 112.063211 \n",
       "L 241.461329 112.063211 \n",
       "L 241.461329 111.437665 \n",
       "L 241.85204 111.437665 \n",
       "L 241.85204 111.020634 \n",
       "L 243.805594 111.020634 \n",
       "L 243.805594 110.812118 \n",
       "L 244.587015 110.812118 \n",
       "L 244.587015 110.395087 \n",
       "L 248.884833 108.309932 \n",
       "L 248.884833 108.101416 \n",
       "L 249.275543 108.101416 \n",
       "L 249.275543 107.892901 \n",
       "L 250.056965 107.892901 \n",
       "L 250.056965 107.47587 \n",
       "L 250.447675 107.47587 \n",
       "L 250.447675 107.267354 \n",
       "L 251.619808 107.267354 \n",
       "L 251.619808 106.641807 \n",
       "L 252.010518 106.641807 \n",
       "L 252.010518 105.59923 \n",
       "L 252.401229 105.59923 \n",
       "L 252.401229 105.390714 \n",
       "L 253.573361 105.390714 \n",
       "L 253.573361 104.348137 \n",
       "L 253.964072 104.348137 \n",
       "L 253.964072 103.514074 \n",
       "L 254.354783 103.514074 \n",
       "L 254.354783 102.888528 \n",
       "L 254.745493 102.888528 \n",
       "L 254.745493 102.680012 \n",
       "L 255.136204 102.680012 \n",
       "L 255.136204 102.471497 \n",
       "L 256.308336 102.471497 \n",
       "L 256.308336 102.054466 \n",
       "L 257.089757 102.054466 \n",
       "L 257.089757 101.637435 \n",
       "L 257.480468 101.637435 \n",
       "L 257.480468 101.428919 \n",
       "L 257.871179 101.428919 \n",
       "L 257.871179 100.594857 \n",
       "L 258.6526 100.594857 \n",
       "L 258.6526 99.96931 \n",
       "L 259.043311 99.96931 \n",
       "L 259.043311 99.135248 \n",
       "L 259.824732 99.135248 \n",
       "L 259.824732 98.718217 \n",
       "L 260.606154 98.718217 \n",
       "L 260.606154 98.301186 \n",
       "L 261.387575 98.301186 \n",
       "L 261.387575 98.09267 \n",
       "L 262.950418 98.09267 \n",
       "L 262.950418 97.675639 \n",
       "L 264.513261 97.675639 \n",
       "L 264.513261 96.424546 \n",
       "L 264.903971 96.424546 \n",
       "L 264.903971 96.007515 \n",
       "L 266.076104 96.007515 \n",
       "L 266.076104 95.799 \n",
       "L 266.466814 95.799 \n",
       "L 266.466814 95.381968 \n",
       "L 266.857525 95.381968 \n",
       "L 266.857525 93.296813 \n",
       "L 268.420368 93.296813 \n",
       "L 268.420368 92.254235 \n",
       "L 269.5925 92.254235 \n",
       "L 269.5925 91.837204 \n",
       "L 269.983211 91.837204 \n",
       "L 269.983211 91.628689 \n",
       "L 271.155343 91.628689 \n",
       "L 271.155343 91.420173 \n",
       "L 271.546053 91.420173 \n",
       "L 271.546053 90.794627 \n",
       "L 273.499607 90.794627 \n",
       "L 273.499607 90.586111 \n",
       "L 273.890318 90.586111 \n",
       "L 273.890318 90.377596 \n",
       "L 274.281028 90.377596 \n",
       "L 274.281028 89.543534 \n",
       "L 274.671739 89.543534 \n",
       "L 274.671739 88.709471 \n",
       "L 276.625292 88.709471 \n",
       "L 276.625292 88.083925 \n",
       "L 277.797425 88.083925 \n",
       "L 277.797425 87.875409 \n",
       "L 278.578846 87.875409 \n",
       "L 278.578846 87.041347 \n",
       "L 278.969557 87.041347 \n",
       "L 278.969557 86.624316 \n",
       "L 280.141689 86.624316 \n",
       "L 280.141689 85.998769 \n",
       "L 280.532399 85.998769 \n",
       "L 280.532399 85.790254 \n",
       "L 280.92311 85.790254 \n",
       "L 280.92311 85.581738 \n",
       "L 281.313821 85.581738 \n",
       "L 281.313821 85.373223 \n",
       "L 282.095242 85.373223 \n",
       "L 282.095242 85.164707 \n",
       "L 282.485953 85.164707 \n",
       "L 282.485953 84.956192 \n",
       "L 283.267374 84.956192 \n",
       "L 283.267374 84.539161 \n",
       "L 284.048796 84.539161 \n",
       "L 284.048796 83.705099 \n",
       "L 284.439507 83.705099 \n",
       "L 284.439507 83.496583 \n",
       "L 284.830217 83.496583 \n",
       "L 284.830217 82.871036 \n",
       "L 285.220928 82.871036 \n",
       "L 285.220928 82.662521 \n",
       "L 286.002349 82.662521 \n",
       "L 286.002349 82.454005 \n",
       "L 286.39306 82.454005 \n",
       "L 286.39306 82.24549 \n",
       "L 286.783771 82.24549 \n",
       "L 286.783771 80.994397 \n",
       "L 287.565192 80.994397 \n",
       "L 287.565192 80.785881 \n",
       "L 287.955903 80.785881 \n",
       "L 287.955903 80.577366 \n",
       "L 288.737324 80.577366 \n",
       "L 288.737324 80.160334 \n",
       "L 289.128035 80.160334 \n",
       "L 289.128035 79.951819 \n",
       "L 311.398545 65.981278 \n",
       "L 311.789256 65.981278 \n",
       "L 311.789256 65.772763 \n",
       "L 312.961388 65.772763 \n",
       "L 312.961388 64.9387 \n",
       "L 313.742809 64.9387 \n",
       "L 313.742809 64.521669 \n",
       "L 314.524231 64.521669 \n",
       "L 314.524231 63.896123 \n",
       "L 315.305652 63.896123 \n",
       "L 315.305652 63.687607 \n",
       "L 315.696363 63.687607 \n",
       "L 315.696363 63.479092 \n",
       "L 316.477784 63.479092 \n",
       "L 316.477784 63.062061 \n",
       "L 317.259205 63.062061 \n",
       "L 317.259205 62.227998 \n",
       "L 317.649916 62.227998 \n",
       "L 317.649916 61.393936 \n",
       "L 318.431338 61.393936 \n",
       "L 318.431338 61.185421 \n",
       "L 318.822048 61.185421 \n",
       "L 318.822048 60.142843 \n",
       "L 319.60347 60.142843 \n",
       "L 319.60347 59.517296 \n",
       "L 319.99418 59.517296 \n",
       "L 319.99418 59.308781 \n",
       "L 320.384891 59.308781 \n",
       "L 320.384891 58.89175 \n",
       "L 320.775602 58.89175 \n",
       "L 320.775602 58.474719 \n",
       "L 321.166312 58.474719 \n",
       "L 321.166312 58.266203 \n",
       "L 321.557023 58.266203 \n",
       "L 321.557023 58.057688 \n",
       "L 321.947734 58.057688 \n",
       "L 321.947734 57.432141 \n",
       "L 322.729155 57.432141 \n",
       "L 322.729155 57.01511 \n",
       "L 324.291998 57.01511 \n",
       "L 324.291998 56.806595 \n",
       "L 324.682709 56.806595 \n",
       "L 324.682709 56.389563 \n",
       "L 325.46413 56.389563 \n",
       "L 325.46413 55.972532 \n",
       "L 325.854841 55.972532 \n",
       "L 325.854841 54.721439 \n",
       "L 327.026973 54.721439 \n",
       "L 327.026973 54.095893 \n",
       "L 327.417684 54.095893 \n",
       "L 327.417684 53.887377 \n",
       "L 328.199105 53.887377 \n",
       "L 328.199105 53.678861 \n",
       "L 328.589816 53.678861 \n",
       "L 328.589816 53.470346 \n",
       "L 328.980526 53.470346 \n",
       "L 328.980526 53.053315 \n",
       "L 329.371237 53.053315 \n",
       "L 329.371237 52.844799 \n",
       "L 330.543369 52.844799 \n",
       "L 330.543369 52.636284 \n",
       "L 332.106212 52.636284 \n",
       "L 332.106212 52.010737 \n",
       "L 332.887633 52.010737 \n",
       "L 337.576162 49.925582 \n",
       "L 338.748294 49.925582 \n",
       "L 338.748294 49.717066 \n",
       "L 339.529715 49.717066 \n",
       "L 339.529715 49.09152 \n",
       "L 341.092558 49.09152 \n",
       "L 341.092558 48.674489 \n",
       "L 341.483269 48.674489 \n",
       "L 341.483269 48.048942 \n",
       "L 342.655401 48.048942 \n",
       "L 342.655401 47.840426 \n",
       "L 344.608955 47.840426 \n",
       "L 344.608955 47.006364 \n",
       "L 344.999665 47.006364 \n",
       "L 344.999665 46.797849 \n",
       "L 345.390376 46.797849 \n",
       "L 345.390376 46.172302 \n",
       "L 346.953219 46.172302 \n",
       "L 346.953219 45.963787 \n",
       "L 347.73464 45.963787 \n",
       "L 347.73464 45.546756 \n",
       "L 348.516062 45.546756 \n",
       "L 348.516062 44.921209 \n",
       "L 348.906772 44.921209 \n",
       "L 348.906772 44.087147 \n",
       "L 349.688194 44.087147 \n",
       "L 349.688194 43.4616 \n",
       "L 350.860326 43.4616 \n",
       "L 350.860326 43.253085 \n",
       "L 351.251036 43.253085 \n",
       "L 351.251036 42.627538 \n",
       "L 352.032458 42.627538 \n",
       "L 352.032458 42.001992 \n",
       "L 352.423169 42.001992 \n",
       "L 352.423169 41.793476 \n",
       "L 353.20459 41.793476 \n",
       "L 353.20459 41.58496 \n",
       "L 353.595301 41.58496 \n",
       "L 353.595301 41.376445 \n",
       "L 353.986011 41.376445 \n",
       "L 353.986011 40.542383 \n",
       "L 354.376722 40.542383 \n",
       "L 354.376722 39.708321 \n",
       "L 354.767433 39.708321 \n",
       "L 354.767433 38.874258 \n",
       "L 355.548854 38.874258 \n",
       "L 355.548854 38.457227 \n",
       "L 356.330276 38.457227 \n",
       "L 356.330276 38.248712 \n",
       "L 357.111697 38.248712 \n",
       "L 357.111697 36.997619 \n",
       "L 357.502408 36.997619 \n",
       "L 357.502408 36.580588 \n",
       "L 358.283829 36.580588 \n",
       "L 358.283829 36.372072 \n",
       "L 359.06525 36.372072 \n",
       "L 359.06525 35.53801 \n",
       "L 359.846672 35.53801 \n",
       "L 359.846672 35.120979 \n",
       "L 360.237383 35.120979 \n",
       "L 360.237383 34.912463 \n",
       "L 360.628093 34.912463 \n",
       "L 360.628093 34.703948 \n",
       "L 361.800225 34.703948 \n",
       "L 361.800225 33.452855 \n",
       "L 362.190936 33.452855 \n",
       "L 362.190936 33.244339 \n",
       "L 362.581647 33.244339 \n",
       "L 362.581647 32.827308 \n",
       "L 362.972357 32.827308 \n",
       "L 362.972357 32.618792 \n",
       "L 363.363068 32.618792 \n",
       "L 363.363068 32.201761 \n",
       "L 363.363068 32.201761 \n",
       "\" style=\"fill:none;stroke:#ff8c00;stroke-linecap:square;stroke-width:2;\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path clip-path=\"url(#p19afa1a8ed)\" d=\"M 58.999432 229.874489 \n",
       "L 363.363068 32.201761 \n",
       "\" style=\"fill:none;stroke:#000080;stroke-dasharray:7.4,3.2;stroke-dashoffset:0;stroke-width:2;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 239.758125 \n",
       "L 43.78125 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 378.58125 239.758125 \n",
       "L 378.58125 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 239.758125 \n",
       "L 378.58125 239.758125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 22.318125 \n",
       "L 378.58125 22.318125 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- Receiver operating characteristic example -->\n",
       "    <defs>\n",
       "     <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "     <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "     <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-110\"/>\n",
       "     <path d=\"M 45.40625 27.984375 \n",
       "Q 45.40625 37.75 41.375 43.109375 \n",
       "Q 37.359375 48.484375 30.078125 48.484375 \n",
       "Q 22.859375 48.484375 18.828125 43.109375 \n",
       "Q 14.796875 37.75 14.796875 27.984375 \n",
       "Q 14.796875 18.265625 18.828125 12.890625 \n",
       "Q 22.859375 7.515625 30.078125 7.515625 \n",
       "Q 37.359375 7.515625 41.375 12.890625 \n",
       "Q 45.40625 18.265625 45.40625 27.984375 \n",
       "z\n",
       "M 54.390625 6.78125 \n",
       "Q 54.390625 -7.171875 48.1875 -13.984375 \n",
       "Q 42 -20.796875 29.203125 -20.796875 \n",
       "Q 24.46875 -20.796875 20.265625 -20.09375 \n",
       "Q 16.0625 -19.390625 12.109375 -17.921875 \n",
       "L 12.109375 -9.1875 \n",
       "Q 16.0625 -11.328125 19.921875 -12.34375 \n",
       "Q 23.78125 -13.375 27.78125 -13.375 \n",
       "Q 36.625 -13.375 41.015625 -8.765625 \n",
       "Q 45.40625 -4.15625 45.40625 5.171875 \n",
       "L 45.40625 9.625 \n",
       "Q 42.625 4.78125 38.28125 2.390625 \n",
       "Q 33.9375 0 27.875 0 \n",
       "Q 17.828125 0 11.671875 7.65625 \n",
       "Q 5.515625 15.328125 5.515625 27.984375 \n",
       "Q 5.515625 40.671875 11.671875 48.328125 \n",
       "Q 17.828125 56 27.875 56 \n",
       "Q 33.9375 56 38.28125 53.609375 \n",
       "Q 42.625 51.21875 45.40625 46.390625 \n",
       "L 45.40625 54.6875 \n",
       "L 54.390625 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-103\"/>\n",
       "     <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     <path d=\"M 54.890625 54.6875 \n",
       "L 35.109375 28.078125 \n",
       "L 55.90625 0 \n",
       "L 45.3125 0 \n",
       "L 29.390625 21.484375 \n",
       "L 13.484375 0 \n",
       "L 2.875 0 \n",
       "L 24.125 28.609375 \n",
       "L 4.6875 54.6875 \n",
       "L 15.28125 54.6875 \n",
       "L 29.78125 35.203125 \n",
       "L 44.28125 54.6875 \n",
       "z\n",
       "\" id=\"DejaVuSans-120\"/>\n",
       "     <path d=\"M 52 44.1875 \n",
       "Q 55.375 50.25 60.0625 53.125 \n",
       "Q 64.75 56 71.09375 56 \n",
       "Q 79.640625 56 84.28125 50.015625 \n",
       "Q 88.921875 44.046875 88.921875 33.015625 \n",
       "L 88.921875 0 \n",
       "L 79.890625 0 \n",
       "L 79.890625 32.71875 \n",
       "Q 79.890625 40.578125 77.09375 44.375 \n",
       "Q 74.3125 48.1875 68.609375 48.1875 \n",
       "Q 61.625 48.1875 57.5625 43.546875 \n",
       "Q 53.515625 38.921875 53.515625 30.90625 \n",
       "L 53.515625 0 \n",
       "L 44.484375 0 \n",
       "L 44.484375 32.71875 \n",
       "Q 44.484375 40.625 41.703125 44.40625 \n",
       "Q 38.921875 48.1875 33.109375 48.1875 \n",
       "Q 26.21875 48.1875 22.15625 43.53125 \n",
       "Q 18.109375 38.875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.1875 51.21875 25.484375 53.609375 \n",
       "Q 29.78125 56 35.6875 56 \n",
       "Q 41.65625 56 45.828125 52.96875 \n",
       "Q 50 49.953125 52 44.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-109\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(83.51625 16.318125)scale(0.12 -0.12)\">\n",
       "     <use xlink:href=\"#DejaVuSans-82\"/>\n",
       "     <use x=\"69.419922\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"130.943359\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"185.923828\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"247.447266\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"275.230469\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "     <use x=\"334.410156\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"395.933594\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"437.046875\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"468.833984\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "     <use x=\"530.015625\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"593.492188\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"655.015625\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"696.128906\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"757.408203\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"796.617188\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"824.400391\" xlink:href=\"#DejaVuSans-110\"/>\n",
       "     <use x=\"887.779297\" xlink:href=\"#DejaVuSans-103\"/>\n",
       "     <use x=\"951.255859\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"983.042969\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"1038.023438\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     <use x=\"1101.402344\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"1162.681641\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"1203.794922\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"1265.074219\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"1320.054688\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"1359.263672\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"1420.787109\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "     <use x=\"1461.900391\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"1489.683594\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     <use x=\"1541.783203\" xlink:href=\"#DejaVuSans-116\"/>\n",
       "     <use x=\"1580.992188\" xlink:href=\"#DejaVuSans-105\"/>\n",
       "     <use x=\"1608.775391\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "     <use x=\"1663.755859\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"1695.542969\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "     <use x=\"1757.050781\" xlink:href=\"#DejaVuSans-120\"/>\n",
       "     <use x=\"1816.230469\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "     <use x=\"1877.509766\" xlink:href=\"#DejaVuSans-109\"/>\n",
       "     <use x=\"1974.921875\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "     <use x=\"2038.398438\" xlink:href=\"#DejaVuSans-108\"/>\n",
       "     <use x=\"2066.181641\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 216.08125 234.758125 \n",
       "L 371.58125 234.758125 \n",
       "Q 373.58125 234.758125 373.58125 232.758125 \n",
       "L 373.58125 219.08 \n",
       "Q 373.58125 217.08 371.58125 217.08 \n",
       "L 216.08125 217.08 \n",
       "Q 214.08125 217.08 214.08125 219.08 \n",
       "L 214.08125 232.758125 \n",
       "Q 214.08125 234.758125 216.08125 234.758125 \n",
       "z\n",
       "\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_15\">\n",
       "     <path d=\"M 218.08125 225.178437 \n",
       "L 238.08125 225.178437 \n",
       "\" style=\"fill:none;stroke:#ff8c00;stroke-linecap:square;stroke-width:2;\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_16\"/>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- ROC curve (area = 0.50) -->\n",
       "     <defs>\n",
       "      <path d=\"M 39.40625 66.21875 \n",
       "Q 28.65625 66.21875 22.328125 58.203125 \n",
       "Q 16.015625 50.203125 16.015625 36.375 \n",
       "Q 16.015625 22.609375 22.328125 14.59375 \n",
       "Q 28.65625 6.59375 39.40625 6.59375 \n",
       "Q 50.140625 6.59375 56.421875 14.59375 \n",
       "Q 62.703125 22.609375 62.703125 36.375 \n",
       "Q 62.703125 50.203125 56.421875 58.203125 \n",
       "Q 50.140625 66.21875 39.40625 66.21875 \n",
       "z\n",
       "M 39.40625 74.21875 \n",
       "Q 54.734375 74.21875 63.90625 63.9375 \n",
       "Q 73.09375 53.65625 73.09375 36.375 \n",
       "Q 73.09375 19.140625 63.90625 8.859375 \n",
       "Q 54.734375 -1.421875 39.40625 -1.421875 \n",
       "Q 24.03125 -1.421875 14.8125 8.828125 \n",
       "Q 5.609375 19.09375 5.609375 36.375 \n",
       "Q 5.609375 53.65625 14.8125 63.9375 \n",
       "Q 24.03125 74.21875 39.40625 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-79\"/>\n",
       "      <path d=\"M 64.40625 67.28125 \n",
       "L 64.40625 56.890625 \n",
       "Q 59.421875 61.53125 53.78125 63.8125 \n",
       "Q 48.140625 66.109375 41.796875 66.109375 \n",
       "Q 29.296875 66.109375 22.65625 58.46875 \n",
       "Q 16.015625 50.828125 16.015625 36.375 \n",
       "Q 16.015625 21.96875 22.65625 14.328125 \n",
       "Q 29.296875 6.6875 41.796875 6.6875 \n",
       "Q 48.140625 6.6875 53.78125 8.984375 \n",
       "Q 59.421875 11.28125 64.40625 15.921875 \n",
       "L 64.40625 5.609375 \n",
       "Q 59.234375 2.09375 53.4375 0.328125 \n",
       "Q 47.65625 -1.421875 41.21875 -1.421875 \n",
       "Q 24.65625 -1.421875 15.125 8.703125 \n",
       "Q 5.609375 18.84375 5.609375 36.375 \n",
       "Q 5.609375 53.953125 15.125 64.078125 \n",
       "Q 24.65625 74.21875 41.21875 74.21875 \n",
       "Q 47.75 74.21875 53.53125 72.484375 \n",
       "Q 59.328125 70.75 64.40625 67.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-67\"/>\n",
       "      <path d=\"M 31 75.875 \n",
       "Q 24.46875 64.65625 21.28125 53.65625 \n",
       "Q 18.109375 42.671875 18.109375 31.390625 \n",
       "Q 18.109375 20.125 21.3125 9.0625 \n",
       "Q 24.515625 -2 31 -13.1875 \n",
       "L 23.1875 -13.1875 \n",
       "Q 15.875 -1.703125 12.234375 9.375 \n",
       "Q 8.59375 20.453125 8.59375 31.390625 \n",
       "Q 8.59375 42.28125 12.203125 53.3125 \n",
       "Q 15.828125 64.359375 23.1875 75.875 \n",
       "z\n",
       "\" id=\"DejaVuSans-40\"/>\n",
       "      <path d=\"M 10.59375 45.40625 \n",
       "L 73.1875 45.40625 \n",
       "L 73.1875 37.203125 \n",
       "L 10.59375 37.203125 \n",
       "z\n",
       "M 10.59375 25.484375 \n",
       "L 73.1875 25.484375 \n",
       "L 73.1875 17.1875 \n",
       "L 10.59375 17.1875 \n",
       "z\n",
       "\" id=\"DejaVuSans-61\"/>\n",
       "      <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      <path d=\"M 8.015625 75.875 \n",
       "L 15.828125 75.875 \n",
       "Q 23.140625 64.359375 26.78125 53.3125 \n",
       "Q 30.421875 42.28125 30.421875 31.390625 \n",
       "Q 30.421875 20.453125 26.78125 9.375 \n",
       "Q 23.140625 -1.703125 15.828125 -13.1875 \n",
       "L 8.015625 -13.1875 \n",
       "Q 14.5 -2 17.703125 9.0625 \n",
       "Q 20.90625 20.125 20.90625 31.390625 \n",
       "Q 20.90625 42.671875 17.703125 53.65625 \n",
       "Q 14.5 64.65625 8.015625 75.875 \n",
       "z\n",
       "\" id=\"DejaVuSans-41\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(246.08125 228.678437)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-82\"/>\n",
       "      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-79\"/>\n",
       "      <use x=\"148.193359\" xlink:href=\"#DejaVuSans-67\"/>\n",
       "      <use x=\"218.017578\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"249.804688\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"304.785156\" xlink:href=\"#DejaVuSans-117\"/>\n",
       "      <use x=\"368.164062\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"409.277344\" xlink:href=\"#DejaVuSans-118\"/>\n",
       "      <use x=\"468.457031\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"529.980469\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"561.767578\" xlink:href=\"#DejaVuSans-40\"/>\n",
       "      <use x=\"600.78125\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"662.060547\" xlink:href=\"#DejaVuSans-114\"/>\n",
       "      <use x=\"703.142578\" xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"764.666016\" xlink:href=\"#DejaVuSans-97\"/>\n",
       "      <use x=\"825.945312\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"857.732422\" xlink:href=\"#DejaVuSans-61\"/>\n",
       "      <use x=\"941.521484\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "      <use x=\"973.308594\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      <use x=\"1036.931641\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "      <use x=\"1068.71875\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      <use x=\"1132.341797\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      <use x=\"1195.964844\" xlink:href=\"#DejaVuSans-41\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p19afa1a8ed\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "print(accuracy_score(y, (oof_pred > 0.5).astype(int)))\n",
    "\n",
    "def history_graph(historys):\n",
    "    history_acc = np.zeros_like(historys[0].history['accuracy'], dtype=np.float)\n",
    "    history_val_acc = np.zeros_like(historys[0].history['val_accuracy'], dtype=np.float)\n",
    "    history_loss = np.zeros_like(historys[0].history['loss'], dtype=np.float)\n",
    "    history_val_loss = np.zeros_like(historys[0].history['val_loss'], dtype=np.float)\n",
    "    for history in historys:\n",
    "        history_acc += history.history['accuracy']\n",
    "        history_val_acc += history.history['val_accuracy']\n",
    "        history_loss += history.history['loss'] \n",
    "        history_val_loss += history.history['val_loss']\n",
    "    history_acc /= len(historys)\n",
    "    history_val_acc /= len(historys)\n",
    "    history_loss /= len(historys)\n",
    "    history_val_loss /= len(historys)\n",
    "    plt.plot(history_acc)\n",
    "    plt.plot(history_val_acc)\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "history_graph(historys)\n",
    "\n",
    "\n",
    "fpr, tpr,thred = metrics.roc_curve(y, oof_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              4.0                  5.583333                   0.40   \n",
      "31                   14.0                  5.428571                   3.20   \n",
      "32                    0.0                  0.000000                   4.00   \n",
      "33                   27.0                  6.222222                   3.11   \n",
      "34                   21.0                  6.396825                   3.27   \n",
      "35                    0.0                  0.000000                   4.00   \n",
      "36                   22.0                  7.287879                   2.24   \n",
      "37                   28.0                  6.940476                   2.91   \n",
      "38                   16.0                  5.812500                   3.19   \n",
      "39                   25.0                  5.853333                   3.31   \n",
      "40                   21.0                  5.809524                   4.12   \n",
      "41                   26.0                  6.512821                   2.55   \n",
      "\n",
      "    last_3_week_home_st_APP  last_3_week_home_st_ERA  \\\n",
      "0                       0.0                 4.000000   \n",
      "1                       0.0                 4.000000   \n",
      "2                       0.0                 4.000000   \n",
      "3                       0.0                 4.000000   \n",
      "4                       0.0                 4.000000   \n",
      "5                       0.0                 4.000000   \n",
      "6                       0.0                 4.000000   \n",
      "7                       0.0                 4.000000   \n",
      "8                       0.0                 4.000000   \n",
      "10                      0.0                 4.000000   \n",
      "11                      0.0                 4.000000   \n",
      "12                      0.0                 4.000000   \n",
      "13                      0.0                 4.000000   \n",
      "14                      0.0                 4.000000   \n",
      "15                      0.0                 4.000000   \n",
      "16                      0.0                 4.000000   \n",
      "17                      0.0                 4.000000   \n",
      "18                      0.0                 4.000000   \n",
      "19                      0.0                 4.000000   \n",
      "20                      0.0                 4.000000   \n",
      "21                      0.0                 4.000000   \n",
      "22                      0.0                 4.000000   \n",
      "23                      0.0                 4.000000   \n",
      "24                      0.0                 4.000000   \n",
      "25                      0.0                 4.000000   \n",
      "26                      0.0                 4.000000   \n",
      "27                      0.0                 4.000000   \n",
      "28                      0.0                 4.000000   \n",
      "29                      0.0                 4.000000   \n",
      "31                      0.0                 4.000000   \n",
      "32                      0.0                 4.000000   \n",
      "33                      1.0                 1.500000   \n",
      "34                      1.0                 0.000000   \n",
      "35                      0.0                 4.000000   \n",
      "36                      1.0                 2.571429   \n",
      "37                      1.0                 3.857143   \n",
      "38                      1.0                 4.050000   \n",
      "39                      1.0                11.250000   \n",
      "40                      1.0                 3.600000   \n",
      "41                      1.0                 1.285714   \n",
      "\n",
      "    last_3_week_home_st_IP_ave  last_week_home_re_times  \\\n",
      "0                     0.000000                 0.000000   \n",
      "1                     0.000000                 0.000000   \n",
      "2                     0.000000                 0.000000   \n",
      "3                     0.000000                 0.000000   \n",
      "4                     0.000000                 0.000000   \n",
      "5                     0.000000                 0.000000   \n",
      "6                     0.000000                 2.000000   \n",
      "7                     0.000000                 2.333333   \n",
      "8                     0.000000                 1.000000   \n",
      "10                    0.000000                 3.000000   \n",
      "11                    0.000000                 2.000000   \n",
      "12                    0.000000                 4.000000   \n",
      "13                    0.000000                 5.333333   \n",
      "14                    0.000000                 3.000000   \n",
      "15                    0.000000                 7.000000   \n",
      "16                    0.000000                 9.000000   \n",
      "17                    0.000000                 7.000000   \n",
      "18                    0.000000                 8.000000   \n",
      "19                    0.000000                 7.333333   \n",
      "20                    0.000000                 7.333333   \n",
      "21                    0.000000                 5.000000   \n",
      "22                    0.000000                 9.000000   \n",
      "23                    0.000000                10.000000   \n",
      "24                    0.000000                12.000000   \n",
      "25                    0.000000                 9.333333   \n",
      "26                    0.000000                10.333333   \n",
      "27                    0.000000                 8.333333   \n",
      "28                    0.000000                11.000000   \n",
      "29                    0.000000                13.000000   \n",
      "31                    0.000000                11.333333   \n",
      "32                    0.000000                15.333333   \n",
      "33                    6.000000                21.666667   \n",
      "34                    7.000000                17.333333   \n",
      "35                    0.000000                12.000000   \n",
      "36                    7.000000                14.000000   \n",
      "37                    7.000000                11.333333   \n",
      "38                    6.666667                16.000000   \n",
      "39                    4.000000                19.666667   \n",
      "40                    5.000000                16.333333   \n",
      "41                    7.000000                14.333333   \n",
      "\n",
      "    last_week_home_re_ERA  last_week_home_runs  last_week_home_runs_allowed  \\\n",
      "0                3.000000                    0                            0   \n",
      "1                3.000000                    0                            0   \n",
      "2                3.000000                    0                            0   \n",
      "3                3.000000                    0                            0   \n",
      "4                3.000000                    0                            0   \n",
      "5                3.000000                    0                            0   \n",
      "6                4.500000                    6                            1   \n",
      "7                3.857143                    4                            5   \n",
      "8                0.000000                    2                            2   \n",
      "10               6.000000                   10                            4   \n",
      "11               0.000000                    6                            2   \n",
      "12               9.000000                   26                            6   \n",
      "13               3.375000                   18                            8   \n",
      "14               0.000000                    8                            3   \n",
      "15               0.000000                   20                            6   \n",
      "16               2.000000                   28                            9   \n",
      "17               0.000000                   18                            6   \n",
      "18              10.125000                   16                           16   \n",
      "19               7.363636                   12                           10   \n",
      "20               1.227273                   24                           14   \n",
      "21               5.400000                   14                            9   \n",
      "22               0.000000                   18                            7   \n",
      "23               0.900000                   26                           10   \n",
      "24               7.500000                   20                           22   \n",
      "25               7.714286                   16                           16   \n",
      "26               0.870968                   32                           16   \n",
      "27               3.240000                   40                           12   \n",
      "28               0.000000                   42                            9   \n",
      "29               2.076923                   30                           16   \n",
      "31               6.352941                   28                           19   \n",
      "32               2.934783                   38                           27   \n",
      "33               7.061538                   48                           32   \n",
      "34               6.230769                   36                           25   \n",
      "35               7.500000                   56                           22   \n",
      "36               1.928571                   48                           17   \n",
      "37               9.529412                   34                           25   \n",
      "38               7.312500                   44                           35   \n",
      "39               6.864407                   54                           30   \n",
      "40               6.061224                   36                           25   \n",
      "41               8.162791                   62                           26   \n",
      "\n",
      "    last_year_away_st_APP  last_year_away_st_IP_ave  last_year_away_st_ERA  \\\n",
      "0                    27.0                  6.222222                   3.11   \n",
      "1                    28.0                  7.392857                   2.52   \n",
      "2                    21.0                  6.396825                   3.27   \n",
      "3                    20.0                  6.700000                   3.22   \n",
      "4                    16.0                  5.812500                   3.19   \n",
      "5                    22.0                  7.287879                   2.24   \n",
      "6                    25.0                  5.853333                   3.31   \n",
      "7                    33.0                  4.242424                   3.92   \n",
      "8                    21.0                  5.809524                   4.12   \n",
      "10                    0.0                  0.000000                   4.00   \n",
      "11                   26.0                  5.705128                   3.51   \n",
      "12                    0.0                  0.000000                   4.00   \n",
      "13                   10.0                  5.200000                   3.81   \n",
      "14                   12.0                  5.972222                   2.89   \n",
      "15                   25.0                  6.573333                   3.17   \n",
      "16                   20.0                  6.516667                   2.48   \n",
      "17                   18.0                  5.722222                   4.79   \n",
      "18                   28.0                  7.107143                   2.40   \n",
      "19                   24.0                  6.125000                   2.94   \n",
      "20                   15.0                  5.800000                   4.64   \n",
      "21                   24.0                  6.763889                   2.38   \n",
      "22                   14.0                  5.476190                   4.58   \n",
      "23                   16.0                  6.875000                   3.02   \n",
      "24                   27.0                  6.296296                   3.22   \n",
      "25                    0.0                  0.000000                   4.00   \n",
      "26                    0.0                  0.000000                   4.00   \n",
      "27                   25.0                  6.440000                   3.35   \n",
      "28                   16.0                  5.375000                   3.44   \n",
      "29                   27.0                  4.962963                   4.22   \n",
      "31                   25.0                  7.160000                   1.91   \n",
      "32                   28.0                  6.928571                   1.85   \n",
      "33                   28.0                  7.392857                   2.52   \n",
      "34                   27.0                  5.901235                   3.72   \n",
      "35                   26.0                  6.282051                   3.19   \n",
      "36                   20.0                  6.700000                   3.22   \n",
      "37                   23.0                  5.782609                   2.84   \n",
      "38                   28.0                  6.726190                   3.39   \n",
      "39                   33.0                  4.242424                   3.92   \n",
      "40                    5.0                  4.066667                   4.35   \n",
      "41                   13.0                  5.076923                   2.71   \n",
      "\n",
      "    last_3_week_away_st_APP  last_3_week_away_st_ERA  \\\n",
      "0                       0.0                 4.000000   \n",
      "1                       0.0                 4.000000   \n",
      "2                       0.0                 4.000000   \n",
      "3                       0.0                 4.000000   \n",
      "4                       0.0                 4.000000   \n",
      "5                       0.0                 4.000000   \n",
      "6                       0.0                 4.000000   \n",
      "7                       0.0                 4.000000   \n",
      "8                       0.0                 4.000000   \n",
      "10                      0.0                 4.000000   \n",
      "11                      0.0                 4.000000   \n",
      "12                      0.0                 4.000000   \n",
      "13                      0.0                 4.000000   \n",
      "14                      0.0                 4.000000   \n",
      "15                      0.0                 4.000000   \n",
      "16                      0.0                 4.000000   \n",
      "17                      0.0                 4.000000   \n",
      "18                      0.0                 4.000000   \n",
      "19                      0.0                 4.000000   \n",
      "20                      0.0                 4.000000   \n",
      "21                      0.0                 4.000000   \n",
      "22                      0.0                 4.000000   \n",
      "23                      0.0                 4.000000   \n",
      "24                      0.0                 4.000000   \n",
      "25                      0.0                 4.000000   \n",
      "26                      0.0                 4.000000   \n",
      "27                      0.0                 4.000000   \n",
      "28                      0.0                 4.000000   \n",
      "29                      0.0                 4.000000   \n",
      "31                      1.0                 0.000000   \n",
      "32                      1.0                 2.250000   \n",
      "33                      1.0                 2.347826   \n",
      "34                      1.0                 1.500000   \n",
      "35                      1.0                 2.571429   \n",
      "36                      1.0                15.000000   \n",
      "37                      1.0                 1.500000   \n",
      "38                      1.0                 0.000000   \n",
      "39                      1.0                10.800000   \n",
      "40                      0.0                 4.000000   \n",
      "41                      1.0                 1.500000   \n",
      "\n",
      "    last_3_week_away_st_IP_ave  last_week_away_re_times  \\\n",
      "0                     0.000000                 0.000000   \n",
      "1                     0.000000                 0.000000   \n",
      "2                     0.000000                 0.000000   \n",
      "3                     0.000000                 0.000000   \n",
      "4                     0.000000                 0.000000   \n",
      "5                     0.000000                 0.000000   \n",
      "6                     0.000000                 2.000000   \n",
      "7                     0.000000                 1.333333   \n",
      "8                     0.000000                 2.000000   \n",
      "10                    0.000000                 1.666667   \n",
      "11                    0.000000                 1.000000   \n",
      "12                    0.000000                 6.000000   \n",
      "13                    0.000000                 4.333333   \n",
      "14                    0.000000                 4.666667   \n",
      "15                    0.000000                 9.000000   \n",
      "16                    0.000000                 6.666667   \n",
      "17                    0.000000                 4.000000   \n",
      "18                    0.000000                 9.333333   \n",
      "19                    0.000000                 7.000000   \n",
      "20                    0.000000                 6.000000   \n",
      "21                    0.000000                 9.666667   \n",
      "22                    0.000000                 9.000000   \n",
      "23                    0.000000                13.000000   \n",
      "24                    0.000000                 9.666667   \n",
      "25                    0.000000                 9.000000   \n",
      "26                    0.000000                 8.666667   \n",
      "27                    0.000000                12.666667   \n",
      "28                    0.000000                11.333333   \n",
      "29                    0.000000                15.000000   \n",
      "31                    7.000000                12.666667   \n",
      "32                    8.000000                10.000000   \n",
      "33                    7.666667                20.333333   \n",
      "34                    6.000000                18.000000   \n",
      "35                    7.000000                15.666667   \n",
      "36                    3.000000                15.333333   \n",
      "37                    6.000000                18.666667   \n",
      "38                    7.000000                15.000000   \n",
      "39                    5.000000                20.000000   \n",
      "40                    0.000000                15.666667   \n",
      "41                    6.000000                17.666667   \n",
      "\n",
      "    last_week_away_re_ERA  last_week_away_runs  last_week_away_runs_allowed  \n",
      "0                3.000000                    0                            0  \n",
      "1                3.000000                    0                            0  \n",
      "2                3.000000                    0                            0  \n",
      "3                3.000000                    0                            0  \n",
      "4                3.000000                    0                            0  \n",
      "5                3.000000                    0                            0  \n",
      "6                9.000000                    2                            3  \n",
      "7                0.000000                   10                            2  \n",
      "8                4.500000                    4                            1  \n",
      "10              10.800000                    8                            5  \n",
      "11               0.000000                    4                            3  \n",
      "12              10.500000                   12                           13  \n",
      "13               0.000000                   16                            9  \n",
      "14               1.928571                    6                            4  \n",
      "15               1.000000                   12                           10  \n",
      "16              12.150000                   18                           14  \n",
      "17               6.750000                   12                            9  \n",
      "18               1.928571                   28                           12  \n",
      "19               6.428571                   32                            8  \n",
      "20               0.000000                   20                            6  \n",
      "21               9.310345                   28                           18  \n",
      "22               7.000000                   20                           13  \n",
      "23               2.076923                   36                           14  \n",
      "24               1.862069                   40                           14  \n",
      "25               7.000000                   44                           10  \n",
      "26               1.038462                   24                           10  \n",
      "27               9.236842                   34                           31  \n",
      "28               9.529412                   24                           25  \n",
      "29               3.000000                   48                           16  \n",
      "31               6.394737                   50                           16  \n",
      "32               0.900000                   46                           13  \n",
      "33               2.213115                   56                           32  \n",
      "34               3.000000                   60                           28  \n",
      "35               5.744681                   62                           20  \n",
      "36               1.760870                   40                           20  \n",
      "37               2.892857                   56                           21  \n",
      "38               0.600000                   42                           14  \n",
      "39               2.250000                   48                           34  \n",
      "40               2.872340                   58                           25  \n",
      "41               4.584906                   68                           23  \n"
     ]
    }
   ],
   "source": [
    "game_result_train = game_result[game_result['home_runs'] != game_result['away_runs']][['home_victory','last_week_home_win_rate_ratio', 'last_time_home_win_rate_ratio', 'last_3days_home_win_rate_ratio', 'last_year_home_st_APP', 'last_year_home_st_IP_ave', 'last_year_home_st_ERA', 'last_3_week_home_st_APP', 'last_3_week_home_st_ERA', 'last_3_week_home_st_IP_ave', 'last_week_home_re_times', 'last_week_home_re_ERA','last_week_home_runs', 'last_week_home_runs_allowed', 'last_year_away_st_APP', 'last_year_away_st_IP_ave', 'last_year_away_st_ERA', 'last_3_week_away_st_APP', 'last_3_week_away_st_ERA', 'last_3_week_away_st_IP_ave', 'last_week_away_re_times', 'last_week_away_re_ERA', 'last_week_away_runs', 'last_week_away_runs_allowed']].copy()\n",
    "print(game_result_train.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_result_train.to_csv(\"game_result_train_20200910.csv\", mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52                 3.258621                   42                           31   \n",
      "53                 9.450000                   56                           26   \n",
      "...                     ...                  ...                          ...   \n",
      "1705               2.547170                   44                           15   \n",
      "1706               4.108696                   28                           26   \n",
      "1707               5.711538                   56                           20   \n",
      "1711               3.500000                   20                           25   \n",
      "1713               2.612903                   40                           17   \n",
      "1715               6.750000                   26                           22   \n",
      "1718               5.062500                   44                           22   \n",
      "1724               4.695652                   52                           29   \n",
      "1725               3.056604                   34                           18   \n",
      "1726               4.764706                   28                           14   \n",
      "1727               4.333333                   58                           30   \n",
      "1728               3.600000                   42                           21   \n",
      "1730               3.750000                   46                           13   \n",
      "1732               1.500000                   20                           10   \n",
      "1733               6.750000                   18                            9   \n",
      "1735               5.062500                   34                           20   \n",
      "1737               1.500000                    8                            3   \n",
      "1738               0.000000                    4                            1   \n",
      "1739               2.872340                   32                           17   \n",
      "1740               1.928571                   12                            1   \n",
      "1741               3.000000                    0                            0   \n",
      "1746               3.000000                    0                            0   \n",
      "1751               0.000000                    6                            5   \n",
      "1752               0.000000                   20                           10   \n",
      "1753               0.000000                   28                           13   \n",
      "1756               0.000000                   22                            3   \n",
      "1757               0.000000                   34                            1   \n",
      "1759               3.857143                   12                           17   \n",
      "1760               3.375000                   24                           17   \n",
      "1761               3.937500                   42                           17   \n",
      "\n",
      "      last_year_away_st_APP  last_year_away_st_IP_ave  last_year_away_st_ERA  \\\n",
      "0                      27.0                  6.222222                   3.11   \n",
      "3                      20.0                  6.700000                   3.22   \n",
      "4                      16.0                  5.812500                   3.19   \n",
      "5                      22.0                  7.287879                   2.24   \n",
      "6                      25.0                  5.853333                   3.31   \n",
      "7                      33.0                  4.242424                   3.92   \n",
      "8                      21.0                  5.809524                   4.12   \n",
      "10                      0.0                  0.000000                   4.00   \n",
      "11                     26.0                  5.705128                   3.51   \n",
      "12                      0.0                  0.000000                   4.00   \n",
      "13                     10.0                  5.200000                   3.81   \n",
      "14                     12.0                  5.972222                   2.89   \n",
      "20                     15.0                  5.800000                   4.64   \n",
      "21                     24.0                  6.763889                   2.38   \n",
      "22                     14.0                  5.476190                   4.58   \n",
      "24                     27.0                  6.296296                   3.22   \n",
      "25                      0.0                  0.000000                   4.00   \n",
      "29                     27.0                  4.962963                   4.22   \n",
      "32                     28.0                  6.928571                   1.85   \n",
      "33                     28.0                  7.392857                   2.52   \n",
      "34                     27.0                  5.901235                   3.72   \n",
      "36                     20.0                  6.700000                   3.22   \n",
      "37                     23.0                  5.782609                   2.84   \n",
      "39                     33.0                  4.242424                   3.92   \n",
      "41                     13.0                  5.076923                   2.71   \n",
      "43                     26.0                  5.846154                   3.55   \n",
      "48                     25.0                  6.573333                   3.17   \n",
      "50                     23.0                  6.260870                   3.74   \n",
      "52                      0.0                  0.000000                   4.00   \n",
      "53                     14.0                  5.642857                   4.76   \n",
      "...                     ...                       ...                    ...   \n",
      "1705                   19.0                  6.842105                   2.49   \n",
      "1706                    4.0                  1.166667                  19.29   \n",
      "1707                   23.0                  5.739130                   3.88   \n",
      "1711                   19.0                  6.807018                   3.54   \n",
      "1713                   18.0                  5.481481                   4.38   \n",
      "1715                   13.0                  6.307692                   3.84   \n",
      "1718                   18.0                  4.944444                   3.02   \n",
      "1724                    0.0                  0.000000                   4.00   \n",
      "1725                    0.0                  0.000000                   4.00   \n",
      "1726                   26.0                  5.961538                   4.30   \n",
      "1727                   13.0                  1.076923                   2.51   \n",
      "1728                   24.0                  6.750000                   3.83   \n",
      "1730                   15.0                  4.800000                   3.36   \n",
      "1732                    6.0                  1.166667                   7.71   \n",
      "1733                   54.0                  0.938272                   2.66   \n",
      "1735                   27.0                  6.777778                   2.95   \n",
      "1737                    0.0                  0.000000                   4.00   \n",
      "1738                   22.0                  5.500000                   4.24   \n",
      "1739                    0.0                  0.000000                   4.00   \n",
      "1740                   23.0                  6.579710                   3.50   \n",
      "1741                   28.0                  6.964286                   2.91   \n",
      "1746                   25.0                  6.120000                   3.12   \n",
      "1751                   28.0                  6.964286                   2.91   \n",
      "1752                   19.0                  6.842105                   2.49   \n",
      "1753                   26.0                  5.961538                   4.30   \n",
      "1756                   23.0                  6.579710                   3.50   \n",
      "1757                   22.0                  6.136364                   2.93   \n",
      "1759                   24.0                  6.791667                   3.04   \n",
      "1760                   13.0                  6.307692                   3.84   \n",
      "1761                   22.0                  6.136364                   2.93   \n",
      "\n",
      "      last_3_week_away_st_APP  last_3_week_away_st_ERA  \\\n",
      "0                         0.0                 4.000000   \n",
      "3                         0.0                 4.000000   \n",
      "4                         0.0                 4.000000   \n",
      "5                         0.0                 4.000000   \n",
      "6                         0.0                 4.000000   \n",
      "7                         0.0                 4.000000   \n",
      "8                         0.0                 4.000000   \n",
      "10                        0.0                 4.000000   \n",
      "11                        0.0                 4.000000   \n",
      "12                        0.0                 4.000000   \n",
      "13                        0.0                 4.000000   \n",
      "14                        0.0                 4.000000   \n",
      "20                        0.0                 4.000000   \n",
      "21                        0.0                 4.000000   \n",
      "22                        0.0                 4.000000   \n",
      "24                        0.0                 4.000000   \n",
      "25                        0.0                 4.000000   \n",
      "29                        0.0                 4.000000   \n",
      "32                        1.0                 2.250000   \n",
      "33                        1.0                 2.347826   \n",
      "34                        1.0                 1.500000   \n",
      "36                        1.0                15.000000   \n",
      "37                        1.0                 1.500000   \n",
      "39                        1.0                10.800000   \n",
      "41                        1.0                 1.500000   \n",
      "43                        1.0                15.000000   \n",
      "48                        1.0                 0.000000   \n",
      "50                        1.0                 1.285714   \n",
      "52                        1.0                 3.857143   \n",
      "53                        1.0                 1.500000   \n",
      "...                       ...                      ...   \n",
      "1705                      3.0                 3.970588   \n",
      "1706                      1.0                 7.714286   \n",
      "1707                      2.0                 4.500000   \n",
      "1711                      2.0                 1.800000   \n",
      "1713                      2.0                 0.642857   \n",
      "1715                      2.0                 4.500000   \n",
      "1718                      0.0                 4.000000   \n",
      "1724                      2.0                 1.200000   \n",
      "1725                      0.0                 4.000000   \n",
      "1726                      3.0                 2.142857   \n",
      "1727                      0.0                 4.000000   \n",
      "1728                      3.0                 1.285714   \n",
      "1730                      2.0                 2.250000   \n",
      "1732                      0.0                 4.000000   \n",
      "1733                      1.0                 0.000000   \n",
      "1735                      2.0                 8.437500   \n",
      "1737                      1.0                 3.600000   \n",
      "1738                      0.0                 4.000000   \n",
      "1739                      1.0                 6.000000   \n",
      "1740                      1.0                15.000000   \n",
      "1741                      3.0                 0.900000   \n",
      "1746                      1.0                 0.000000   \n",
      "1751                      3.0                 4.500000   \n",
      "1752                      3.0                 0.600000   \n",
      "1753                      3.0                 3.780000   \n",
      "1756                      2.0                 1.636364   \n",
      "1757                      1.0                 9.000000   \n",
      "1759                      1.0                 9.000000   \n",
      "1760                      1.0                 3.857143   \n",
      "1761                      2.0                 4.000000   \n",
      "\n",
      "      last_3_week_away_st_IP_ave  last_week_away_re_times  \\\n",
      "0                       0.000000                 0.000000   \n",
      "3                       0.000000                 0.000000   \n",
      "4                       0.000000                 0.000000   \n",
      "5                       0.000000                 0.000000   \n",
      "6                       0.000000                 2.000000   \n",
      "7                       0.000000                 1.333333   \n",
      "8                       0.000000                 2.000000   \n",
      "10                      0.000000                 1.666667   \n",
      "11                      0.000000                 1.000000   \n",
      "12                      0.000000                 6.000000   \n",
      "13                      0.000000                 4.333333   \n",
      "14                      0.000000                 4.666667   \n",
      "20                      0.000000                 6.000000   \n",
      "21                      0.000000                 9.666667   \n",
      "22                      0.000000                 9.000000   \n",
      "24                      0.000000                 9.666667   \n",
      "25                      0.000000                 9.000000   \n",
      "29                      0.000000                15.000000   \n",
      "32                      8.000000                10.000000   \n",
      "33                      7.666667                20.333333   \n",
      "34                      6.000000                18.000000   \n",
      "36                      3.000000                15.333333   \n",
      "37                      6.000000                18.666667   \n",
      "39                      5.000000                20.000000   \n",
      "41                      6.000000                17.666667   \n",
      "43                      3.000000                15.666667   \n",
      "48                      9.000000                10.000000   \n",
      "50                      7.000000                12.666667   \n",
      "52                      7.000000                14.000000   \n",
      "53                      6.000000                17.666667   \n",
      "...                          ...                      ...   \n",
      "1705                    7.555556                 7.000000   \n",
      "1706                    4.666667                 6.000000   \n",
      "1707                    5.000000                18.000000   \n",
      "1711                    7.500000                11.000000   \n",
      "1713                    7.000000                 8.000000   \n",
      "1715                    6.000000                15.000000   \n",
      "1718                    0.000000                 9.666667   \n",
      "1724                    7.500000                 5.000000   \n",
      "1725                    0.000000                14.666667   \n",
      "1726                    7.000000                17.333333   \n",
      "1727                    0.000000                 5.000000   \n",
      "1728                    7.000000                15.666667   \n",
      "1730                    8.000000                 5.000000   \n",
      "1732                    0.000000                24.333333   \n",
      "1733                    9.000000                11.666667   \n",
      "1735                    5.333333                11.666667   \n",
      "1737                    5.000000                13.000000   \n",
      "1738                    0.000000                 2.000000   \n",
      "1739                    6.000000                 8.000000   \n",
      "1740                    3.000000                 0.000000   \n",
      "1741                    6.666667                15.666667   \n",
      "1746                    6.000000                10.000000   \n",
      "1751                    5.333333                17.333333   \n",
      "1752                    5.000000                18.333333   \n",
      "1753                    5.555556                17.333333   \n",
      "1756                    5.500000                12.000000   \n",
      "1757                    3.000000                15.666667   \n",
      "1759                    5.000000                10.333333   \n",
      "1760                    4.666667                13.333333   \n",
      "1761                    4.500000                14.666667   \n",
      "\n",
      "      last_week_away_re_ERA  last_week_away_runs  last_week_away_runs_allowed  \n",
      "0                  3.000000                    0                            0  \n",
      "3                  3.000000                    0                            0  \n",
      "4                  3.000000                    0                            0  \n",
      "5                  3.000000                    0                            0  \n",
      "6                  9.000000                    2                            3  \n",
      "7                  0.000000                   10                            2  \n",
      "8                  4.500000                    4                            1  \n",
      "10                10.800000                    8                            5  \n",
      "11                 0.000000                    4                            3  \n",
      "12                10.500000                   12                           13  \n",
      "13                 0.000000                   16                            9  \n",
      "14                 1.928571                    6                            4  \n",
      "20                 0.000000                   20                            6  \n",
      "21                 9.310345                   28                           18  \n",
      "22                 7.000000                   20                           13  \n",
      "24                 1.862069                   40                           14  \n",
      "25                 7.000000                   44                           10  \n",
      "29                 3.000000                   48                           16  \n",
      "32                 0.900000                   46                           13  \n",
      "33                 2.213115                   56                           32  \n",
      "34                 3.000000                   60                           28  \n",
      "36                 1.760870                   40                           20  \n",
      "37                 2.892857                   56                           21  \n",
      "39                 2.250000                   48                           34  \n",
      "41                 4.584906                   68                           23  \n",
      "43                 2.297872                   50                           21  \n",
      "48                 4.500000                   42                           17  \n",
      "50                 0.710526                   44                           13  \n",
      "52                 7.714286                   34                           29  \n",
      "53                 4.075472                   58                           21  \n",
      "...                     ...                  ...                          ...  \n",
      "1705               3.857143                   40                           14  \n",
      "1706              10.500000                   32                            9  \n",
      "1707               2.000000                   46                           16  \n",
      "1711               4.090909                   18                           17  \n",
      "1713               9.000000                   36                           13  \n",
      "1715               4.800000                   36                           25  \n",
      "1718               2.793103                   20                            5  \n",
      "1724               5.400000                   24                           13  \n",
      "1725               4.295455                   34                           18  \n",
      "1726               3.634615                   26                           14  \n",
      "1727               5.400000                   26                           16  \n",
      "1728               4.021277                   40                           22  \n",
      "1730               5.400000                   26                           16  \n",
      "1732               4.438356                   34                           25  \n",
      "1733               3.857143                   36                           22  \n",
      "1735               0.000000                   12                            3  \n",
      "1737               6.230769                   16                           24  \n",
      "1738               0.000000                    6                            7  \n",
      "1739               1.125000                   20                            4  \n",
      "1740               3.000000                    0                            0  \n",
      "1741               0.000000                   30                            3  \n",
      "1746               2.700000                   38                            9  \n",
      "1751               3.115385                   28                           16  \n",
      "1752               3.927273                   38                           23  \n",
      "1753               3.634615                   44                           17  \n",
      "1756               0.750000                   26                            6  \n",
      "1757               2.297872                   28                           16  \n",
      "1759               1.741935                   34                            6  \n",
      "1760               4.050000                   34                           12  \n",
      "1761               2.454545                   34                           21  \n",
      "\n",
      "[948 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(game_result_train[(game_result_train['home_victory'] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b758aa3e2fd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 構築データでモデル構築\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# 構築データの予測値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "K = 5\n",
    "kf =  StratifiedKFold(n_splits=K, shuffle=True, random_state=114)\n",
    "\n",
    "score_train_tmp = 0\n",
    "score_test_tmp = 0\n",
    "score_test_tmp2 = 0\n",
    "fpr_tmp = 0\n",
    "tpr_tmp = 0\n",
    "X = game_result_train.drop(columns='home_victory')\n",
    "y = game_result_train['home_victory']\n",
    "clf = tree.DecisionTreeClassifier(random_state=114 ,max_depth=3, criterion=\"gini\",min_samples_split= 1e-3,max_features=1,min_impurity_decrease=1e-4,)\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # 構築データでモデル構築\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # 構築データの予測値\n",
    "    pred_train = clf.predict(X_train)\n",
    "\n",
    "    # 構築データのaccuracy\n",
    "    auccuracy = accuracy_score(pred_train, y_train)\n",
    "\n",
    "    #構築データのaccuracyを足していく\n",
    "    score_train_tmp+=auccuracy\n",
    "\n",
    "    #検証データの予測値\n",
    "    pred_test = clf.predict(X_test)\n",
    "\n",
    "    #検証データのaccuracy\n",
    "    auccuracy = accuracy_score(pred_test, y_test)\n",
    "\n",
    "    #検証データのaccuracyを足していく\n",
    "    score_test_tmp+=auccuracy\n",
    "\n",
    "    prob = clf.predict_proba(X_test)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, prob[:,1])\n",
    "    fpr_tmp += fpr\n",
    "    tpr_tmp += tpr\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show() \n",
    "    score_test_tmp2 += metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "score_train_tmp /= K\n",
    "score_test_tmp /= K\n",
    "score_test_tmp2 /= K\n",
    "fpr_tmp /= K\n",
    "tpr_tmp /= K\n",
    "plt.plot(fpr_tmp, tpr_tmp)\n",
    "plt.title(\"ROC curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show() \n",
    "print(score_train_tmp, score_test_tmp,score_test_tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tubotu/.local/lib/python3.6/site-packages/optuna/_experimental.py:90: ExperimentalWarning: train is experimental (supported from v0.18.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687139\n",
      "[400]\tvalid_0's binary_logloss: 0.684397\n",
      "[600]\tvalid_0's binary_logloss: 0.682421\n",
      "[800]\tvalid_0's binary_logloss: 0.680972\n",
      "[1000]\tvalid_0's binary_logloss: 0.679866\n",
      "[1200]\tvalid_0's binary_logloss: 0.679304\n",
      "[1400]\tvalid_0's binary_logloss: 0.678703\n",
      "[1600]\tvalid_0's binary_logloss: 0.678333\n",
      "Early stopping, best iteration is:\n",
      "[1574]\tvalid_0's binary_logloss: 0.678263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678263:   0%|          | 0/7 [00:17<?, ?it/s]\n",
      "feature_fraction, val_score: 0.678263:  14%|#4        | 1/7 [00:17<01:46, 17.78s/it][I 2020-09-17 04:10:50,694] Finished trial#0 with value: 0.6782629753688442 with parameters: {'feature_fraction': 0.4}. Best is trial#0 with value: 0.6782629753688442.\n",
      "\n",
      "feature_fraction, val_score: 0.678263:  14%|#4        | 1/7 [00:17<01:46, 17.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687016\n",
      "[400]\tvalid_0's binary_logloss: 0.684402\n",
      "[600]\tvalid_0's binary_logloss: 0.682425\n",
      "[800]\tvalid_0's binary_logloss: 0.681146\n",
      "[1000]\tvalid_0's binary_logloss: 0.680498\n",
      "[1200]\tvalid_0's binary_logloss: 0.679923\n",
      "[1400]\tvalid_0's binary_logloss: 0.679818\n",
      "Early stopping, best iteration is:\n",
      "[1389]\tvalid_0's binary_logloss: 0.679736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678263:  14%|#4        | 1/7 [00:33<01:46, 17.78s/it]\n",
      "feature_fraction, val_score: 0.678263:  29%|##8       | 2/7 [00:33<01:25, 17.02s/it][I 2020-09-17 04:11:05,935] Finished trial#1 with value: 0.6797358950203829 with parameters: {'feature_fraction': 0.6}. Best is trial#0 with value: 0.6782629753688442.\n",
      "\n",
      "feature_fraction, val_score: 0.678263:  29%|##8       | 2/7 [00:33<01:25, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686672\n",
      "[400]\tvalid_0's binary_logloss: 0.6839\n",
      "[600]\tvalid_0's binary_logloss: 0.682149\n",
      "[800]\tvalid_0's binary_logloss: 0.681122\n",
      "Early stopping, best iteration is:\n",
      "[830]\tvalid_0's binary_logloss: 0.681008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678263:  29%|##8       | 2/7 [00:41<01:25, 17.02s/it]\n",
      "feature_fraction, val_score: 0.678263:  43%|####2     | 3/7 [00:41<00:58, 14.55s/it][I 2020-09-17 04:11:14,726] Finished trial#2 with value: 0.6810083506362147 with parameters: {'feature_fraction': 0.7}. Best is trial#0 with value: 0.6782629753688442.\n",
      "\n",
      "feature_fraction, val_score: 0.678263:  43%|####2     | 3/7 [00:41<00:58, 14.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68675\n",
      "[400]\tvalid_0's binary_logloss: 0.68424\n",
      "[600]\tvalid_0's binary_logloss: 0.682252\n",
      "Early stopping, best iteration is:\n",
      "[692]\tvalid_0's binary_logloss: 0.681836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678263:  43%|####2     | 3/7 [00:47<00:58, 14.55s/it]\n",
      "feature_fraction, val_score: 0.678263:  57%|#####7    | 4/7 [00:47<00:35, 11.75s/it][I 2020-09-17 04:11:19,956] Finished trial#3 with value: 0.6818358663569292 with parameters: {'feature_fraction': 0.8}. Best is trial#0 with value: 0.6782629753688442.\n",
      "\n",
      "feature_fraction, val_score: 0.678263:  57%|#####7    | 4/7 [00:47<00:35, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686967\n",
      "[400]\tvalid_0's binary_logloss: 0.684039\n",
      "[600]\tvalid_0's binary_logloss: 0.682181\n",
      "[800]\tvalid_0's binary_logloss: 0.680949\n",
      "[1000]\tvalid_0's binary_logloss: 0.679842\n",
      "[1200]\tvalid_0's binary_logloss: 0.679142\n",
      "[1400]\tvalid_0's binary_logloss: 0.678667\n",
      "[1600]\tvalid_0's binary_logloss: 0.678469\n",
      "Early stopping, best iteration is:\n",
      "[1618]\tvalid_0's binary_logloss: 0.678449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678263:  57%|#####7    | 4/7 [01:00<00:35, 11.75s/it]\n",
      "feature_fraction, val_score: 0.678263:  71%|#######1  | 5/7 [01:00<00:24, 12.36s/it][I 2020-09-17 04:11:33,750] Finished trial#4 with value: 0.6784486134052083 with parameters: {'feature_fraction': 0.5}. Best is trial#0 with value: 0.6782629753688442.\n",
      "\n",
      "feature_fraction, val_score: 0.678263:  71%|#######1  | 5/7 [01:00<00:24, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68692\n",
      "[400]\tvalid_0's binary_logloss: 0.684074\n",
      "[600]\tvalid_0's binary_logloss: 0.68189\n",
      "[800]\tvalid_0's binary_logloss: 0.681533\n",
      "Early stopping, best iteration is:\n",
      "[724]\tvalid_0's binary_logloss: 0.681269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678263:  71%|#######1  | 5/7 [01:11<00:24, 12.36s/it]\n",
      "feature_fraction, val_score: 0.678263:  86%|########5 | 6/7 [01:11<00:11, 11.97s/it][I 2020-09-17 04:11:44,773] Finished trial#5 with value: 0.6812687443581543 with parameters: {'feature_fraction': 0.8999999999999999}. Best is trial#0 with value: 0.6782629753688442.\n",
      "\n",
      "feature_fraction, val_score: 0.678263:  86%|########5 | 6/7 [01:11<00:11, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687441\n",
      "[400]\tvalid_0's binary_logloss: 0.684445\n",
      "[600]\tvalid_0's binary_logloss: 0.681077\n",
      "[800]\tvalid_0's binary_logloss: 0.679879\n",
      "Early stopping, best iteration is:\n",
      "[755]\tvalid_0's binary_logloss: 0.679691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678263:  86%|########5 | 6/7 [01:23<00:11, 11.97s/it]\n",
      "feature_fraction, val_score: 0.678263: 100%|##########| 7/7 [01:23<00:00, 11.83s/it][I 2020-09-17 04:11:56,298] Finished trial#6 with value: 0.6796905023145039 with parameters: {'feature_fraction': 1.0}. Best is trial#0 with value: 0.6782629753688442.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "num_leaves, val_score: 0.678263:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689201\n",
      "[400]\tvalid_0's binary_logloss: 0.687898\n",
      "[600]\tvalid_0's binary_logloss: 0.686841\n",
      "[800]\tvalid_0's binary_logloss: 0.686037\n",
      "[1000]\tvalid_0's binary_logloss: 0.685374\n",
      "[1200]\tvalid_0's binary_logloss: 0.684829\n",
      "[1400]\tvalid_0's binary_logloss: 0.684239\n",
      "[1600]\tvalid_0's binary_logloss: 0.683731\n",
      "[1800]\tvalid_0's binary_logloss: 0.683308\n",
      "[2000]\tvalid_0's binary_logloss: 0.682917\n",
      "[2200]\tvalid_0's binary_logloss: 0.68254\n",
      "[2400]\tvalid_0's binary_logloss: 0.682184\n",
      "[2600]\tvalid_0's binary_logloss: 0.681866\n",
      "[2800]\tvalid_0's binary_logloss: 0.681627\n",
      "[3000]\tvalid_0's binary_logloss: 0.681467\n",
      "[3200]\tvalid_0's binary_logloss: 0.681323\n",
      "[3400]\tvalid_0's binary_logloss: 0.681194\n",
      "[3600]\tvalid_0's binary_logloss: 0.681076\n",
      "[3800]\tvalid_0's binary_logloss: 0.680987\n",
      "[4000]\tvalid_0's binary_logloss: 0.680919\n",
      "[4200]\tvalid_0's binary_logloss: 0.680854\n",
      "[4400]\tvalid_0's binary_logloss: 0.680788\n",
      "[4600]\tvalid_0's binary_logloss: 0.680752\n",
      "[4800]\tvalid_0's binary_logloss: 0.680725\n",
      "[5000]\tvalid_0's binary_logloss: 0.680704\n",
      "[5200]\tvalid_0's binary_logloss: 0.680683\n",
      "[5400]\tvalid_0's binary_logloss: 0.680657\n",
      "Early stopping, best iteration is:\n",
      "[5460]\tvalid_0's binary_logloss: 0.68065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678263:   0%|          | 0/20 [00:23<?, ?it/s]\n",
      "num_leaves, val_score: 0.678263:   5%|5         | 1/20 [00:23<07:18, 23.05s/it][I 2020-09-17 04:12:19,413] Finished trial#7 with value: 0.6806499679975394 with parameters: {'num_leaves': 2}. Best is trial#7 with value: 0.6806499679975394.\n",
      "\n",
      "num_leaves, val_score: 0.678263:   5%|5         | 1/20 [00:23<07:18, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678263:   5%|5         | 1/20 [00:38<07:18, 23.05s/it]\n",
      "num_leaves, val_score: 0.678263:  10%|#         | 2/20 [00:38<06:11, 20.63s/it][I 2020-09-17 04:12:34,393] Finished trial#8 with value: 0.680564674122208 with parameters: {'num_leaves': 249}. Best is trial#8 with value: 0.680564674122208.\n",
      "\n",
      "num_leaves, val_score: 0.678263:  10%|#         | 2/20 [00:38<06:11, 20.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678263:  10%|#         | 2/20 [01:12<06:11, 20.63s/it]\n",
      "num_leaves, val_score: 0.678263:  15%|#5        | 3/20 [01:12<07:02, 24.85s/it][I 2020-09-17 04:13:09,076] Finished trial#9 with value: 0.680564674122208 with parameters: {'num_leaves': 220}. Best is trial#8 with value: 0.680564674122208.\n",
      "\n",
      "num_leaves, val_score: 0.678263:  15%|#5        | 3/20 [01:12<07:02, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687411\n",
      "[400]\tvalid_0's binary_logloss: 0.684695\n",
      "[600]\tvalid_0's binary_logloss: 0.682769\n",
      "[800]\tvalid_0's binary_logloss: 0.681307\n",
      "[1000]\tvalid_0's binary_logloss: 0.680098\n",
      "[1200]\tvalid_0's binary_logloss: 0.679144\n",
      "[1400]\tvalid_0's binary_logloss: 0.678137\n",
      "[1600]\tvalid_0's binary_logloss: 0.677656\n",
      "[1800]\tvalid_0's binary_logloss: 0.677427\n",
      "Early stopping, best iteration is:\n",
      "[1859]\tvalid_0's binary_logloss: 0.67734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.677340:  15%|#5        | 3/20 [01:25<07:02, 24.85s/it]\n",
      "num_leaves, val_score: 0.677340:  20%|##        | 4/20 [01:25<05:39, 21.19s/it][I 2020-09-17 04:13:21,741] Finished trial#10 with value: 0.6773403283399357 with parameters: {'num_leaves': 23}. Best is trial#10 with value: 0.6773403283399357.\n",
      "\n",
      "num_leaves, val_score: 0.677340:  20%|##        | 4/20 [01:25<05:39, 21.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.677340:  20%|##        | 4/20 [01:40<05:39, 21.19s/it]\n",
      "num_leaves, val_score: 0.677340:  25%|##5       | 5/20 [01:40<04:50, 19.39s/it][I 2020-09-17 04:13:36,916] Finished trial#11 with value: 0.680564674122208 with parameters: {'num_leaves': 172}. Best is trial#10 with value: 0.6773403283399357.\n",
      "\n",
      "num_leaves, val_score: 0.677340:  25%|##5       | 5/20 [01:40<04:50, 19.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687184\n",
      "[400]\tvalid_0's binary_logloss: 0.684308\n",
      "[600]\tvalid_0's binary_logloss: 0.682158\n",
      "[800]\tvalid_0's binary_logloss: 0.680954\n",
      "[1000]\tvalid_0's binary_logloss: 0.680044\n",
      "[1200]\tvalid_0's binary_logloss: 0.679586\n",
      "[1400]\tvalid_0's binary_logloss: 0.679227\n",
      "[1600]\tvalid_0's binary_logloss: 0.679052\n",
      "[1800]\tvalid_0's binary_logloss: 0.67916\n",
      "Early stopping, best iteration is:\n",
      "[1717]\tvalid_0's binary_logloss: 0.678878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.677340:  25%|##5       | 5/20 [02:02<04:50, 19.39s/it]\n",
      "num_leaves, val_score: 0.677340:  30%|###       | 6/20 [02:02<04:43, 20.23s/it][I 2020-09-17 04:13:59,123] Finished trial#12 with value: 0.6788782589533416 with parameters: {'num_leaves': 49}. Best is trial#10 with value: 0.6773403283399357.\n",
      "\n",
      "num_leaves, val_score: 0.677340:  30%|###       | 6/20 [02:02<04:43, 20.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.677340:  30%|###       | 6/20 [02:17<04:43, 20.23s/it]\n",
      "num_leaves, val_score: 0.677340:  35%|###5      | 7/20 [02:17<03:59, 18.46s/it][I 2020-09-17 04:14:13,445] Finished trial#13 with value: 0.680564674122208 with parameters: {'num_leaves': 185}. Best is trial#10 with value: 0.6773403283399357.\n",
      "\n",
      "num_leaves, val_score: 0.677340:  35%|###5      | 7/20 [02:17<03:59, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688109\n",
      "[400]\tvalid_0's binary_logloss: 0.685887\n",
      "[600]\tvalid_0's binary_logloss: 0.68414\n",
      "[800]\tvalid_0's binary_logloss: 0.682482\n",
      "[1000]\tvalid_0's binary_logloss: 0.681249\n",
      "[1200]\tvalid_0's binary_logloss: 0.680407\n",
      "[1400]\tvalid_0's binary_logloss: 0.679519\n",
      "[1600]\tvalid_0's binary_logloss: 0.678724\n",
      "[1800]\tvalid_0's binary_logloss: 0.677958\n",
      "[2000]\tvalid_0's binary_logloss: 0.677341\n",
      "[2200]\tvalid_0's binary_logloss: 0.67694\n",
      "[2400]\tvalid_0's binary_logloss: 0.676609\n",
      "[2600]\tvalid_0's binary_logloss: 0.676226\n",
      "[2800]\tvalid_0's binary_logloss: 0.675895\n",
      "[3000]\tvalid_0's binary_logloss: 0.675707\n",
      "[3200]\tvalid_0's binary_logloss: 0.675612\n",
      "[3400]\tvalid_0's binary_logloss: 0.675434\n",
      "[3600]\tvalid_0's binary_logloss: 0.6753\n",
      "[3800]\tvalid_0's binary_logloss: 0.675124\n",
      "Early stopping, best iteration is:\n",
      "[3860]\tvalid_0's binary_logloss: 0.675076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  35%|###5      | 7/20 [02:38<03:59, 18.46s/it]\n",
      "num_leaves, val_score: 0.675076:  40%|####      | 8/20 [02:38<03:52, 19.34s/it][I 2020-09-17 04:14:34,843] Finished trial#14 with value: 0.675075550336106 with parameters: {'num_leaves': 4}. Best is trial#14 with value: 0.675075550336106.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  40%|####      | 8/20 [02:38<03:52, 19.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  40%|####      | 8/20 [02:53<03:52, 19.34s/it]\n",
      "num_leaves, val_score: 0.675076:  45%|####5     | 9/20 [02:53<03:18, 18.01s/it][I 2020-09-17 04:14:49,752] Finished trial#15 with value: 0.680564674122208 with parameters: {'num_leaves': 144}. Best is trial#14 with value: 0.675075550336106.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  45%|####5     | 9/20 [02:53<03:18, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687305\n",
      "[400]\tvalid_0's binary_logloss: 0.68448\n",
      "[600]\tvalid_0's binary_logloss: 0.682496\n",
      "[800]\tvalid_0's binary_logloss: 0.681005\n",
      "[1000]\tvalid_0's binary_logloss: 0.679835\n",
      "[1200]\tvalid_0's binary_logloss: 0.679137\n",
      "[1400]\tvalid_0's binary_logloss: 0.678332\n",
      "[1600]\tvalid_0's binary_logloss: 0.677979\n",
      "[1800]\tvalid_0's binary_logloss: 0.67756\n",
      "Early stopping, best iteration is:\n",
      "[1888]\tvalid_0's binary_logloss: 0.677522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  45%|####5     | 9/20 [03:14<03:18, 18.01s/it]\n",
      "num_leaves, val_score: 0.675076:  50%|#####     | 10/20 [03:14<03:08, 18.88s/it][I 2020-09-17 04:15:10,680] Finished trial#16 with value: 0.6775224035947274 with parameters: {'num_leaves': 26}. Best is trial#14 with value: 0.675075550336106.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  50%|#####     | 10/20 [03:14<03:08, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687183\n",
      "[400]\tvalid_0's binary_logloss: 0.684534\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.681545\n",
      "[1000]\tvalid_0's binary_logloss: 0.680578\n",
      "[1200]\tvalid_0's binary_logloss: 0.680219\n",
      "[1400]\tvalid_0's binary_logloss: 0.680063\n",
      "Early stopping, best iteration is:\n",
      "[1389]\tvalid_0's binary_logloss: 0.680021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  50%|#####     | 10/20 [03:31<03:08, 18.88s/it]\n",
      "num_leaves, val_score: 0.675076:  55%|#####5    | 11/20 [03:31<02:44, 18.30s/it][I 2020-09-17 04:15:27,607] Finished trial#17 with value: 0.6800206648327526 with parameters: {'num_leaves': 81}. Best is trial#14 with value: 0.675075550336106.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  55%|#####5    | 11/20 [03:31<02:44, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687226\n",
      "[400]\tvalid_0's binary_logloss: 0.684778\n",
      "[600]\tvalid_0's binary_logloss: 0.683008\n",
      "[800]\tvalid_0's binary_logloss: 0.682088\n",
      "[1000]\tvalid_0's binary_logloss: 0.681166\n",
      "[1200]\tvalid_0's binary_logloss: 0.680769\n",
      "Early stopping, best iteration is:\n",
      "[1230]\tvalid_0's binary_logloss: 0.680598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  55%|#####5    | 11/20 [03:52<02:44, 18.30s/it]\n",
      "num_leaves, val_score: 0.675076:  60%|######    | 12/20 [03:52<02:34, 19.26s/it][I 2020-09-17 04:15:49,107] Finished trial#18 with value: 0.6805976268552058 with parameters: {'num_leaves': 78}. Best is trial#14 with value: 0.675075550336106.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  60%|######    | 12/20 [03:52<02:34, 19.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688109\n",
      "[400]\tvalid_0's binary_logloss: 0.685887\n",
      "[600]\tvalid_0's binary_logloss: 0.68414\n",
      "[800]\tvalid_0's binary_logloss: 0.682482\n",
      "[1000]\tvalid_0's binary_logloss: 0.681249\n",
      "[1200]\tvalid_0's binary_logloss: 0.680407\n",
      "[1400]\tvalid_0's binary_logloss: 0.679519\n",
      "[1600]\tvalid_0's binary_logloss: 0.678724\n",
      "[1800]\tvalid_0's binary_logloss: 0.677958\n",
      "[2000]\tvalid_0's binary_logloss: 0.677341\n",
      "[2200]\tvalid_0's binary_logloss: 0.67694\n",
      "[2400]\tvalid_0's binary_logloss: 0.676609\n",
      "[2600]\tvalid_0's binary_logloss: 0.676226\n",
      "[2800]\tvalid_0's binary_logloss: 0.675895\n",
      "[3000]\tvalid_0's binary_logloss: 0.675707\n",
      "[3200]\tvalid_0's binary_logloss: 0.675612\n",
      "[3400]\tvalid_0's binary_logloss: 0.675434\n",
      "[3600]\tvalid_0's binary_logloss: 0.6753\n",
      "[3800]\tvalid_0's binary_logloss: 0.675124\n",
      "Early stopping, best iteration is:\n",
      "[3860]\tvalid_0's binary_logloss: 0.675076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  60%|######    | 12/20 [04:08<02:34, 19.26s/it]\n",
      "num_leaves, val_score: 0.675076:  65%|######5   | 13/20 [04:08<02:07, 18.23s/it][I 2020-09-17 04:16:04,944] Finished trial#19 with value: 0.6750755503361059 with parameters: {'num_leaves': 4}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  65%|######5   | 13/20 [04:08<02:07, 18.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  65%|######5   | 13/20 [04:32<02:07, 18.23s/it]\n",
      "num_leaves, val_score: 0.675076:  70%|#######   | 14/20 [04:32<01:59, 19.86s/it][I 2020-09-17 04:16:28,608] Finished trial#20 with value: 0.680564674122208 with parameters: {'num_leaves': 88}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  70%|#######   | 14/20 [04:32<01:59, 19.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688109\n",
      "[400]\tvalid_0's binary_logloss: 0.685887\n",
      "[600]\tvalid_0's binary_logloss: 0.68414\n",
      "[800]\tvalid_0's binary_logloss: 0.682482\n",
      "[1000]\tvalid_0's binary_logloss: 0.681249\n",
      "[1200]\tvalid_0's binary_logloss: 0.680407\n",
      "[1400]\tvalid_0's binary_logloss: 0.679519\n",
      "[1600]\tvalid_0's binary_logloss: 0.678724\n",
      "[1800]\tvalid_0's binary_logloss: 0.677958\n",
      "[2000]\tvalid_0's binary_logloss: 0.677341\n",
      "[2200]\tvalid_0's binary_logloss: 0.67694\n",
      "[2400]\tvalid_0's binary_logloss: 0.676609\n",
      "[2600]\tvalid_0's binary_logloss: 0.676226\n",
      "[2800]\tvalid_0's binary_logloss: 0.675895\n",
      "[3000]\tvalid_0's binary_logloss: 0.675707\n",
      "[3200]\tvalid_0's binary_logloss: 0.675612\n",
      "[3400]\tvalid_0's binary_logloss: 0.675434\n",
      "[3600]\tvalid_0's binary_logloss: 0.6753\n",
      "[3800]\tvalid_0's binary_logloss: 0.675124\n",
      "Early stopping, best iteration is:\n",
      "[3860]\tvalid_0's binary_logloss: 0.675076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  70%|#######   | 14/20 [04:49<01:59, 19.86s/it]\n",
      "num_leaves, val_score: 0.675076:  75%|#######5  | 15/20 [04:49<01:35, 19.01s/it][I 2020-09-17 04:16:45,628] Finished trial#21 with value: 0.675075550336106 with parameters: {'num_leaves': 4}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  75%|#######5  | 15/20 [04:49<01:35, 19.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687195\n",
      "[400]\tvalid_0's binary_logloss: 0.68442\n",
      "[600]\tvalid_0's binary_logloss: 0.682311\n",
      "[800]\tvalid_0's binary_logloss: 0.681131\n",
      "[1000]\tvalid_0's binary_logloss: 0.680297\n",
      "[1200]\tvalid_0's binary_logloss: 0.679838\n",
      "Early stopping, best iteration is:\n",
      "[1107]\tvalid_0's binary_logloss: 0.679759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  75%|#######5  | 15/20 [05:09<01:35, 19.01s/it]\n",
      "num_leaves, val_score: 0.675076:  80%|########  | 16/20 [05:09<01:17, 19.32s/it][I 2020-09-17 04:17:05,686] Finished trial#22 with value: 0.6797585055282707 with parameters: {'num_leaves': 50}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  80%|########  | 16/20 [05:09<01:17, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  80%|########  | 16/20 [05:36<01:17, 19.32s/it]\n",
      "num_leaves, val_score: 0.675076:  85%|########5 | 17/20 [05:36<01:05, 21.75s/it][I 2020-09-17 04:17:33,102] Finished trial#23 with value: 0.680564674122208 with parameters: {'num_leaves': 116}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  85%|########5 | 17/20 [05:36<01:05, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688533\n",
      "[400]\tvalid_0's binary_logloss: 0.686627\n",
      "[600]\tvalid_0's binary_logloss: 0.68505\n",
      "[800]\tvalid_0's binary_logloss: 0.683668\n",
      "[1000]\tvalid_0's binary_logloss: 0.682587\n",
      "[1200]\tvalid_0's binary_logloss: 0.681704\n",
      "[1400]\tvalid_0's binary_logloss: 0.680926\n",
      "[1600]\tvalid_0's binary_logloss: 0.680196\n",
      "[1800]\tvalid_0's binary_logloss: 0.679543\n",
      "[2000]\tvalid_0's binary_logloss: 0.678993\n",
      "[2200]\tvalid_0's binary_logloss: 0.678574\n",
      "[2400]\tvalid_0's binary_logloss: 0.678192\n",
      "[2600]\tvalid_0's binary_logloss: 0.67786\n",
      "[2800]\tvalid_0's binary_logloss: 0.677533\n",
      "[3000]\tvalid_0's binary_logloss: 0.677259\n",
      "[3200]\tvalid_0's binary_logloss: 0.677074\n",
      "[3400]\tvalid_0's binary_logloss: 0.676907\n",
      "[3600]\tvalid_0's binary_logloss: 0.676795\n",
      "[3800]\tvalid_0's binary_logloss: 0.676671\n",
      "[4000]\tvalid_0's binary_logloss: 0.676607\n",
      "[4200]\tvalid_0's binary_logloss: 0.67651\n",
      "[4400]\tvalid_0's binary_logloss: 0.676417\n",
      "[4600]\tvalid_0's binary_logloss: 0.676333\n",
      "[4800]\tvalid_0's binary_logloss: 0.676256\n",
      "[5000]\tvalid_0's binary_logloss: 0.676173\n",
      "[5200]\tvalid_0's binary_logloss: 0.676115\n",
      "Early stopping, best iteration is:\n",
      "[5279]\tvalid_0's binary_logloss: 0.676076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  85%|########5 | 17/20 [06:01<01:05, 21.75s/it]\n",
      "num_leaves, val_score: 0.675076:  90%|######### | 18/20 [06:01<00:45, 22.66s/it][I 2020-09-17 04:17:57,887] Finished trial#24 with value: 0.6760755978829173 with parameters: {'num_leaves': 3}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  90%|######### | 18/20 [06:01<00:45, 22.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687184\n",
      "[400]\tvalid_0's binary_logloss: 0.684308\n",
      "[600]\tvalid_0's binary_logloss: 0.682158\n",
      "[800]\tvalid_0's binary_logloss: 0.680954\n",
      "[1000]\tvalid_0's binary_logloss: 0.680044\n",
      "[1200]\tvalid_0's binary_logloss: 0.679586\n",
      "[1400]\tvalid_0's binary_logloss: 0.679227\n",
      "[1600]\tvalid_0's binary_logloss: 0.679052\n",
      "[1800]\tvalid_0's binary_logloss: 0.67916\n",
      "Early stopping, best iteration is:\n",
      "[1717]\tvalid_0's binary_logloss: 0.678878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  90%|######### | 18/20 [06:25<00:45, 22.66s/it]\n",
      "num_leaves, val_score: 0.675076:  95%|#########5| 19/20 [06:25<00:23, 23.17s/it][I 2020-09-17 04:18:22,251] Finished trial#25 with value: 0.6788782589533415 with parameters: {'num_leaves': 49}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "num_leaves, val_score: 0.675076:  95%|#########5| 19/20 [06:25<00:23, 23.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687095\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.682651\n",
      "[800]\tvalid_0's binary_logloss: 0.68158\n",
      "[1000]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[1074]\tvalid_0's binary_logloss: 0.680565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675076:  95%|#########5| 19/20 [06:40<00:23, 23.17s/it]\n",
      "num_leaves, val_score: 0.675076: 100%|##########| 20/20 [06:40<00:00, 20.64s/it][I 2020-09-17 04:18:36,992] Finished trial#26 with value: 0.6805646741222081 with parameters: {'num_leaves': 103}. Best is trial#19 with value: 0.6750755503361059.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "bagging, val_score: 0.675076:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688398\n",
      "[400]\tvalid_0's binary_logloss: 0.686516\n",
      "[600]\tvalid_0's binary_logloss: 0.684958\n",
      "[800]\tvalid_0's binary_logloss: 0.683751\n",
      "[1000]\tvalid_0's binary_logloss: 0.682887\n",
      "[1200]\tvalid_0's binary_logloss: 0.68203\n",
      "[1400]\tvalid_0's binary_logloss: 0.681424\n",
      "[1600]\tvalid_0's binary_logloss: 0.680915\n",
      "[1800]\tvalid_0's binary_logloss: 0.680466\n",
      "[2000]\tvalid_0's binary_logloss: 0.680136\n",
      "Early stopping, best iteration is:\n",
      "[2081]\tvalid_0's binary_logloss: 0.680048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:   0%|          | 0/10 [00:11<?, ?it/s]\n",
      "bagging, val_score: 0.675076:  10%|#         | 1/10 [00:11<01:45, 11.68s/it][I 2020-09-17 04:18:48,739] Finished trial#27 with value: 0.680047887346921 with parameters: {'bagging_fraction': 0.7786293132304354, 'bagging_freq': 6}. Best is trial#27 with value: 0.680047887346921.\n",
      "\n",
      "bagging, val_score: 0.675076:  10%|#         | 1/10 [00:11<01:45, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688308\n",
      "[400]\tvalid_0's binary_logloss: 0.686415\n",
      "[600]\tvalid_0's binary_logloss: 0.684876\n",
      "[800]\tvalid_0's binary_logloss: 0.683658\n",
      "[1000]\tvalid_0's binary_logloss: 0.682734\n",
      "[1200]\tvalid_0's binary_logloss: 0.681861\n",
      "[1400]\tvalid_0's binary_logloss: 0.681182\n",
      "[1600]\tvalid_0's binary_logloss: 0.680713\n",
      "[1800]\tvalid_0's binary_logloss: 0.680227\n",
      "[2000]\tvalid_0's binary_logloss: 0.679961\n",
      "[2200]\tvalid_0's binary_logloss: 0.679669\n",
      "[2400]\tvalid_0's binary_logloss: 0.679438\n",
      "[2600]\tvalid_0's binary_logloss: 0.679277\n",
      "[2800]\tvalid_0's binary_logloss: 0.679104\n",
      "Early stopping, best iteration is:\n",
      "[2785]\tvalid_0's binary_logloss: 0.679075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  10%|#         | 1/10 [00:23<01:45, 11.68s/it]\n",
      "bagging, val_score: 0.675076:  20%|##        | 2/10 [00:23<01:33, 11.69s/it][I 2020-09-17 04:19:00,427] Finished trial#28 with value: 0.6790747881459186 with parameters: {'bagging_fraction': 0.7893562587122659, 'bagging_freq': 6}. Best is trial#28 with value: 0.6790747881459186.\n",
      "\n",
      "bagging, val_score: 0.675076:  20%|##        | 2/10 [00:23<01:33, 11.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688648\n",
      "[400]\tvalid_0's binary_logloss: 0.686969\n",
      "[600]\tvalid_0's binary_logloss: 0.685288\n",
      "[800]\tvalid_0's binary_logloss: 0.683939\n",
      "[1000]\tvalid_0's binary_logloss: 0.683208\n",
      "[1200]\tvalid_0's binary_logloss: 0.682495\n",
      "[1400]\tvalid_0's binary_logloss: 0.681719\n",
      "[1600]\tvalid_0's binary_logloss: 0.681076\n",
      "[1800]\tvalid_0's binary_logloss: 0.680661\n",
      "[2000]\tvalid_0's binary_logloss: 0.680436\n",
      "[2200]\tvalid_0's binary_logloss: 0.680243\n",
      "[2400]\tvalid_0's binary_logloss: 0.680054\n",
      "[2600]\tvalid_0's binary_logloss: 0.679883\n",
      "[2800]\tvalid_0's binary_logloss: 0.67962\n",
      "[3000]\tvalid_0's binary_logloss: 0.679443\n",
      "Early stopping, best iteration is:\n",
      "[3048]\tvalid_0's binary_logloss: 0.679394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  20%|##        | 2/10 [00:37<01:33, 11.69s/it]\n",
      "bagging, val_score: 0.675076:  30%|###       | 3/10 [00:37<01:27, 12.46s/it][I 2020-09-17 04:19:14,695] Finished trial#29 with value: 0.6793937974211077 with parameters: {'bagging_fraction': 0.6707952081982997, 'bagging_freq': 4}. Best is trial#28 with value: 0.6790747881459186.\n",
      "\n",
      "bagging, val_score: 0.675076:  30%|###       | 3/10 [00:37<01:27, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688471\n",
      "[400]\tvalid_0's binary_logloss: 0.686517\n",
      "[600]\tvalid_0's binary_logloss: 0.685033\n",
      "[800]\tvalid_0's binary_logloss: 0.683764\n",
      "[1000]\tvalid_0's binary_logloss: 0.682795\n",
      "[1200]\tvalid_0's binary_logloss: 0.682\n",
      "[1400]\tvalid_0's binary_logloss: 0.681265\n",
      "[1600]\tvalid_0's binary_logloss: 0.68083\n",
      "[1800]\tvalid_0's binary_logloss: 0.68023\n",
      "[2000]\tvalid_0's binary_logloss: 0.679866\n",
      "[2200]\tvalid_0's binary_logloss: 0.679563\n",
      "[2400]\tvalid_0's binary_logloss: 0.679382\n",
      "[2600]\tvalid_0's binary_logloss: 0.679186\n",
      "Early stopping, best iteration is:\n",
      "[2620]\tvalid_0's binary_logloss: 0.679149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  30%|###       | 3/10 [00:50<01:27, 12.46s/it]\n",
      "bagging, val_score: 0.675076:  40%|####      | 4/10 [00:50<01:15, 12.55s/it][I 2020-09-17 04:19:27,467] Finished trial#30 with value: 0.6791488037020315 with parameters: {'bagging_fraction': 0.7916775209494495, 'bagging_freq': 1}. Best is trial#28 with value: 0.6790747881459186.\n",
      "\n",
      "bagging, val_score: 0.675076:  40%|####      | 4/10 [00:50<01:15, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68856\n",
      "[400]\tvalid_0's binary_logloss: 0.686539\n",
      "[600]\tvalid_0's binary_logloss: 0.685019\n",
      "[800]\tvalid_0's binary_logloss: 0.683957\n",
      "[1000]\tvalid_0's binary_logloss: 0.683002\n",
      "[1200]\tvalid_0's binary_logloss: 0.682247\n",
      "[1400]\tvalid_0's binary_logloss: 0.681274\n",
      "[1600]\tvalid_0's binary_logloss: 0.680784\n",
      "[1800]\tvalid_0's binary_logloss: 0.680416\n",
      "[2000]\tvalid_0's binary_logloss: 0.680163\n",
      "[2200]\tvalid_0's binary_logloss: 0.679847\n",
      "[2400]\tvalid_0's binary_logloss: 0.679636\n",
      "Early stopping, best iteration is:\n",
      "[2394]\tvalid_0's binary_logloss: 0.679619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  40%|####      | 4/10 [01:03<01:15, 12.55s/it]\n",
      "bagging, val_score: 0.675076:  50%|#####     | 5/10 [01:03<01:03, 12.62s/it][I 2020-09-17 04:19:40,237] Finished trial#31 with value: 0.6796185076953385 with parameters: {'bagging_fraction': 0.6131997991208378, 'bagging_freq': 7}. Best is trial#28 with value: 0.6790747881459186.\n",
      "\n",
      "bagging, val_score: 0.675076:  50%|#####     | 5/10 [01:03<01:03, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688642\n",
      "[400]\tvalid_0's binary_logloss: 0.686779\n",
      "[600]\tvalid_0's binary_logloss: 0.685199\n",
      "[800]\tvalid_0's binary_logloss: 0.684006\n",
      "[1000]\tvalid_0's binary_logloss: 0.683217\n",
      "[1200]\tvalid_0's binary_logloss: 0.682376\n",
      "[1400]\tvalid_0's binary_logloss: 0.681857\n",
      "[1600]\tvalid_0's binary_logloss: 0.681341\n",
      "[1800]\tvalid_0's binary_logloss: 0.680898\n",
      "[2000]\tvalid_0's binary_logloss: 0.680568\n",
      "[2200]\tvalid_0's binary_logloss: 0.680385\n",
      "Early stopping, best iteration is:\n",
      "[2292]\tvalid_0's binary_logloss: 0.6802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  50%|#####     | 5/10 [01:16<01:03, 12.62s/it]\n",
      "bagging, val_score: 0.675076:  60%|######    | 6/10 [01:16<00:51, 12.84s/it][I 2020-09-17 04:19:53,609] Finished trial#32 with value: 0.6801995505658106 with parameters: {'bagging_fraction': 0.6540286469183902, 'bagging_freq': 6}. Best is trial#28 with value: 0.6790747881459186.\n",
      "\n",
      "bagging, val_score: 0.675076:  60%|######    | 6/10 [01:16<00:51, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688497\n",
      "[400]\tvalid_0's binary_logloss: 0.686592\n",
      "[600]\tvalid_0's binary_logloss: 0.685029\n",
      "[800]\tvalid_0's binary_logloss: 0.683745\n",
      "[1000]\tvalid_0's binary_logloss: 0.682777\n",
      "[1200]\tvalid_0's binary_logloss: 0.68206\n",
      "[1400]\tvalid_0's binary_logloss: 0.681373\n",
      "[1600]\tvalid_0's binary_logloss: 0.680849\n",
      "[1800]\tvalid_0's binary_logloss: 0.680313\n",
      "[2000]\tvalid_0's binary_logloss: 0.680033\n",
      "[2200]\tvalid_0's binary_logloss: 0.67971\n",
      "[2400]\tvalid_0's binary_logloss: 0.67945\n",
      "[2600]\tvalid_0's binary_logloss: 0.679227\n",
      "[2800]\tvalid_0's binary_logloss: 0.679062\n",
      "[3000]\tvalid_0's binary_logloss: 0.678952\n",
      "[3200]\tvalid_0's binary_logloss: 0.678828\n",
      "[3400]\tvalid_0's binary_logloss: 0.678734\n",
      "Early stopping, best iteration is:\n",
      "[3368]\tvalid_0's binary_logloss: 0.678722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  60%|######    | 6/10 [01:39<00:51, 12.84s/it]\n",
      "bagging, val_score: 0.675076:  70%|#######   | 7/10 [01:39<00:47, 15.87s/it][I 2020-09-17 04:20:16,528] Finished trial#33 with value: 0.6787222223017355 with parameters: {'bagging_fraction': 0.7342334900302518, 'bagging_freq': 1}. Best is trial#33 with value: 0.6787222223017355.\n",
      "\n",
      "bagging, val_score: 0.675076:  70%|#######   | 7/10 [01:39<00:47, 15.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688279\n",
      "[400]\tvalid_0's binary_logloss: 0.686154\n",
      "[600]\tvalid_0's binary_logloss: 0.684463\n",
      "[800]\tvalid_0's binary_logloss: 0.683151\n",
      "[1000]\tvalid_0's binary_logloss: 0.682105\n",
      "[1200]\tvalid_0's binary_logloss: 0.68132\n",
      "[1400]\tvalid_0's binary_logloss: 0.680534\n",
      "[1600]\tvalid_0's binary_logloss: 0.679895\n",
      "[1800]\tvalid_0's binary_logloss: 0.679346\n",
      "[2000]\tvalid_0's binary_logloss: 0.678974\n",
      "[2200]\tvalid_0's binary_logloss: 0.678666\n",
      "[2400]\tvalid_0's binary_logloss: 0.678424\n",
      "[2600]\tvalid_0's binary_logloss: 0.678181\n",
      "[2800]\tvalid_0's binary_logloss: 0.677973\n",
      "[3000]\tvalid_0's binary_logloss: 0.677907\n",
      "[3200]\tvalid_0's binary_logloss: 0.677889\n",
      "[3400]\tvalid_0's binary_logloss: 0.677746\n",
      "[3600]\tvalid_0's binary_logloss: 0.677755\n",
      "Early stopping, best iteration is:\n",
      "[3568]\tvalid_0's binary_logloss: 0.677679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  70%|#######   | 7/10 [01:53<00:47, 15.87s/it]\n",
      "bagging, val_score: 0.675076:  80%|########  | 8/10 [01:53<00:30, 15.44s/it][I 2020-09-17 04:20:30,979] Finished trial#34 with value: 0.6776787871072995 with parameters: {'bagging_fraction': 0.8887982399565855, 'bagging_freq': 4}. Best is trial#34 with value: 0.6776787871072995.\n",
      "\n",
      "bagging, val_score: 0.675076:  80%|########  | 8/10 [01:53<00:30, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688499\n",
      "[400]\tvalid_0's binary_logloss: 0.686662\n",
      "[600]\tvalid_0's binary_logloss: 0.684937\n",
      "[800]\tvalid_0's binary_logloss: 0.683682\n",
      "[1000]\tvalid_0's binary_logloss: 0.682826\n",
      "[1200]\tvalid_0's binary_logloss: 0.681991\n",
      "[1400]\tvalid_0's binary_logloss: 0.681424\n",
      "[1600]\tvalid_0's binary_logloss: 0.680905\n",
      "[1800]\tvalid_0's binary_logloss: 0.680461\n",
      "[2000]\tvalid_0's binary_logloss: 0.680248\n",
      "Early stopping, best iteration is:\n",
      "[2004]\tvalid_0's binary_logloss: 0.680234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  80%|########  | 8/10 [02:05<00:30, 15.44s/it]\n",
      "bagging, val_score: 0.675076:  90%|######### | 9/10 [02:05<00:14, 14.18s/it][I 2020-09-17 04:20:42,219] Finished trial#35 with value: 0.68023401207613 with parameters: {'bagging_fraction': 0.725199107703144, 'bagging_freq': 2}. Best is trial#34 with value: 0.6776787871072995.\n",
      "\n",
      "bagging, val_score: 0.675076:  90%|######### | 9/10 [02:05<00:14, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688727\n",
      "[400]\tvalid_0's binary_logloss: 0.687017\n",
      "[600]\tvalid_0's binary_logloss: 0.685421\n",
      "[800]\tvalid_0's binary_logloss: 0.684143\n",
      "[1000]\tvalid_0's binary_logloss: 0.683402\n",
      "[1200]\tvalid_0's binary_logloss: 0.682751\n",
      "[1400]\tvalid_0's binary_logloss: 0.681939\n",
      "[1600]\tvalid_0's binary_logloss: 0.681273\n",
      "[1800]\tvalid_0's binary_logloss: 0.680819\n",
      "[2000]\tvalid_0's binary_logloss: 0.680537\n",
      "[2200]\tvalid_0's binary_logloss: 0.680399\n",
      "[2400]\tvalid_0's binary_logloss: 0.680217\n",
      "[2600]\tvalid_0's binary_logloss: 0.680027\n",
      "[2800]\tvalid_0's binary_logloss: 0.679826\n",
      "[3000]\tvalid_0's binary_logloss: 0.679751\n",
      "Early stopping, best iteration is:\n",
      "[3032]\tvalid_0's binary_logloss: 0.679717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.675076:  90%|######### | 9/10 [02:17<00:14, 14.18s/it]\n",
      "bagging, val_score: 0.675076: 100%|##########| 10/10 [02:17<00:00, 13.76s/it][I 2020-09-17 04:20:54,990] Finished trial#36 with value: 0.6797167601045898 with parameters: {'bagging_fraction': 0.6474528516760513, 'bagging_freq': 4}. Best is trial#34 with value: 0.6776787871072995.\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.675076:   0%|          | 0/3 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687796\n",
      "[400]\tvalid_0's binary_logloss: 0.685357\n",
      "[600]\tvalid_0's binary_logloss: 0.683376\n",
      "[800]\tvalid_0's binary_logloss: 0.681938\n",
      "[1000]\tvalid_0's binary_logloss: 0.680873\n",
      "[1200]\tvalid_0's binary_logloss: 0.67991\n",
      "[1400]\tvalid_0's binary_logloss: 0.679178\n",
      "[1600]\tvalid_0's binary_logloss: 0.678359\n",
      "[1800]\tvalid_0's binary_logloss: 0.677617\n",
      "[2000]\tvalid_0's binary_logloss: 0.677095\n",
      "[2200]\tvalid_0's binary_logloss: 0.676769\n",
      "[2400]\tvalid_0's binary_logloss: 0.676493\n",
      "[2600]\tvalid_0's binary_logloss: 0.676192\n",
      "[2800]\tvalid_0's binary_logloss: 0.675832\n",
      "[3000]\tvalid_0's binary_logloss: 0.675621\n",
      "[3200]\tvalid_0's binary_logloss: 0.675446\n",
      "[3400]\tvalid_0's binary_logloss: 0.675294\n",
      "[3600]\tvalid_0's binary_logloss: 0.675151\n",
      "[3800]\tvalid_0's binary_logloss: 0.675051\n",
      "[4000]\tvalid_0's binary_logloss: 0.674932\n",
      "[4200]\tvalid_0's binary_logloss: 0.674861\n",
      "[4400]\tvalid_0's binary_logloss: 0.67478\n",
      "Early stopping, best iteration is:\n",
      "[4435]\tvalid_0's binary_logloss: 0.674756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.674756:   0%|          | 0/3 [00:14<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.674756:  33%|###3      | 1/3 [00:14<00:29, 14.65s/it][I 2020-09-17 04:21:09,698] Finished trial#37 with value: 0.6747555152574477 with parameters: {'feature_fraction': 0.48000000000000004}. Best is trial#37 with value: 0.6747555152574477.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.674756:  33%|###3      | 1/3 [00:14<00:29, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688109\n",
      "[400]\tvalid_0's binary_logloss: 0.685887\n",
      "[600]\tvalid_0's binary_logloss: 0.68414\n",
      "[800]\tvalid_0's binary_logloss: 0.682482\n",
      "[1000]\tvalid_0's binary_logloss: 0.681249\n",
      "[1200]\tvalid_0's binary_logloss: 0.680407\n",
      "[1400]\tvalid_0's binary_logloss: 0.679519\n",
      "[1600]\tvalid_0's binary_logloss: 0.678724\n",
      "[1800]\tvalid_0's binary_logloss: 0.677958\n",
      "[2000]\tvalid_0's binary_logloss: 0.677341\n",
      "[2200]\tvalid_0's binary_logloss: 0.67694\n",
      "[2400]\tvalid_0's binary_logloss: 0.676609\n",
      "[2600]\tvalid_0's binary_logloss: 0.676226\n",
      "[2800]\tvalid_0's binary_logloss: 0.675895\n",
      "[3000]\tvalid_0's binary_logloss: 0.675707\n",
      "[3200]\tvalid_0's binary_logloss: 0.675612\n",
      "[3400]\tvalid_0's binary_logloss: 0.675434\n",
      "[3600]\tvalid_0's binary_logloss: 0.6753\n",
      "[3800]\tvalid_0's binary_logloss: 0.675124\n",
      "Early stopping, best iteration is:\n",
      "[3860]\tvalid_0's binary_logloss: 0.675076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.674756:  33%|###3      | 1/3 [00:36<00:29, 14.65s/it]\n",
      "feature_fraction_stage2, val_score: 0.674756:  67%|######6   | 2/3 [00:36<00:16, 16.73s/it][I 2020-09-17 04:21:31,283] Finished trial#38 with value: 0.6750755503361059 with parameters: {'feature_fraction': 0.41600000000000004}. Best is trial#37 with value: 0.6747555152574477.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.674756:  67%|######6   | 2/3 [00:36<00:16, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687951\n",
      "[400]\tvalid_0's binary_logloss: 0.685563\n",
      "[600]\tvalid_0's binary_logloss: 0.683598\n",
      "[800]\tvalid_0's binary_logloss: 0.68209\n",
      "[1000]\tvalid_0's binary_logloss: 0.681017\n",
      "[1200]\tvalid_0's binary_logloss: 0.680035\n",
      "[1400]\tvalid_0's binary_logloss: 0.679273\n",
      "[1600]\tvalid_0's binary_logloss: 0.678507\n",
      "[1800]\tvalid_0's binary_logloss: 0.677709\n",
      "[2000]\tvalid_0's binary_logloss: 0.677148\n",
      "[2200]\tvalid_0's binary_logloss: 0.676739\n",
      "[2400]\tvalid_0's binary_logloss: 0.676382\n",
      "[2600]\tvalid_0's binary_logloss: 0.676102\n",
      "[2800]\tvalid_0's binary_logloss: 0.675748\n",
      "[3000]\tvalid_0's binary_logloss: 0.675557\n",
      "[3200]\tvalid_0's binary_logloss: 0.675437\n",
      "[3400]\tvalid_0's binary_logloss: 0.675216\n",
      "[3600]\tvalid_0's binary_logloss: 0.675135\n",
      "[3800]\tvalid_0's binary_logloss: 0.674992\n",
      "[4000]\tvalid_0's binary_logloss: 0.674926\n",
      "Early stopping, best iteration is:\n",
      "[4074]\tvalid_0's binary_logloss: 0.674909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.674756:  67%|######6   | 2/3 [00:51<00:16, 16.73s/it]\n",
      "feature_fraction_stage2, val_score: 0.674756: 100%|##########| 3/3 [00:51<00:00, 16.18s/it][I 2020-09-17 04:21:46,172] Finished trial#39 with value: 0.6749086608544386 with parameters: {'feature_fraction': 0.44800000000000006}. Best is trial#37 with value: 0.6747555152574477.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "regularization_factors, val_score: 0.674756:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687843\n",
      "[400]\tvalid_0's binary_logloss: 0.685396\n",
      "[600]\tvalid_0's binary_logloss: 0.683365\n",
      "[800]\tvalid_0's binary_logloss: 0.681982\n",
      "[1000]\tvalid_0's binary_logloss: 0.68094\n",
      "[1200]\tvalid_0's binary_logloss: 0.679938\n",
      "[1400]\tvalid_0's binary_logloss: 0.679112\n",
      "[1600]\tvalid_0's binary_logloss: 0.678293\n",
      "[1800]\tvalid_0's binary_logloss: 0.677598\n",
      "[2000]\tvalid_0's binary_logloss: 0.67701\n",
      "[2200]\tvalid_0's binary_logloss: 0.676696\n",
      "[2400]\tvalid_0's binary_logloss: 0.676421\n",
      "[2600]\tvalid_0's binary_logloss: 0.676022\n",
      "[2800]\tvalid_0's binary_logloss: 0.675731\n",
      "[3000]\tvalid_0's binary_logloss: 0.675558\n",
      "[3200]\tvalid_0's binary_logloss: 0.675346\n",
      "[3400]\tvalid_0's binary_logloss: 0.675174\n",
      "[3600]\tvalid_0's binary_logloss: 0.674995\n",
      "[3800]\tvalid_0's binary_logloss: 0.67487\n",
      "[4000]\tvalid_0's binary_logloss: 0.674746\n",
      "[4200]\tvalid_0's binary_logloss: 0.674728\n",
      "[4400]\tvalid_0's binary_logloss: 0.674608\n",
      "[4600]\tvalid_0's binary_logloss: 0.674608\n",
      "Early stopping, best iteration is:\n",
      "[4561]\tvalid_0's binary_logloss: 0.674589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:   0%|          | 0/20 [00:23<?, ?it/s]\n",
      "regularization_factors, val_score: 0.674589:   5%|5         | 1/20 [00:23<07:34, 23.90s/it][I 2020-09-17 04:22:10,131] Finished trial#40 with value: 0.6745886371265658 with parameters: {'lambda_l1': 0.02534114064680401, 'lambda_l2': 0.0003534163596152154}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:   5%|5         | 1/20 [00:23<07:34, 23.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687796\n",
      "[400]\tvalid_0's binary_logloss: 0.685357\n",
      "[600]\tvalid_0's binary_logloss: 0.683376\n",
      "[800]\tvalid_0's binary_logloss: 0.681938\n",
      "[1000]\tvalid_0's binary_logloss: 0.680873\n",
      "[1200]\tvalid_0's binary_logloss: 0.67991\n",
      "[1400]\tvalid_0's binary_logloss: 0.679177\n",
      "[1600]\tvalid_0's binary_logloss: 0.67834\n",
      "[1800]\tvalid_0's binary_logloss: 0.677632\n",
      "[2000]\tvalid_0's binary_logloss: 0.677165\n",
      "[2200]\tvalid_0's binary_logloss: 0.676819\n",
      "[2400]\tvalid_0's binary_logloss: 0.676605\n",
      "[2600]\tvalid_0's binary_logloss: 0.676318\n",
      "[2800]\tvalid_0's binary_logloss: 0.675997\n",
      "[3000]\tvalid_0's binary_logloss: 0.675793\n",
      "[3200]\tvalid_0's binary_logloss: 0.675594\n",
      "[3400]\tvalid_0's binary_logloss: 0.675377\n",
      "[3600]\tvalid_0's binary_logloss: 0.67523\n",
      "[3800]\tvalid_0's binary_logloss: 0.675117\n",
      "[4000]\tvalid_0's binary_logloss: 0.675022\n",
      "Early stopping, best iteration is:\n",
      "[4075]\tvalid_0's binary_logloss: 0.674988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:   5%|5         | 1/20 [00:46<07:34, 23.90s/it]\n",
      "regularization_factors, val_score: 0.674589:  10%|#         | 2/20 [00:46<07:03, 23.52s/it][I 2020-09-17 04:22:32,778] Finished trial#41 with value: 0.6749884917018073 with parameters: {'lambda_l1': 7.877266312640276e-06, 'lambda_l2': 4.6735465419883316e-05}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  10%|#         | 2/20 [00:46<07:03, 23.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687802\n",
      "[400]\tvalid_0's binary_logloss: 0.685347\n",
      "[600]\tvalid_0's binary_logloss: 0.683364\n",
      "[800]\tvalid_0's binary_logloss: 0.681925\n",
      "[1000]\tvalid_0's binary_logloss: 0.680862\n",
      "[1200]\tvalid_0's binary_logloss: 0.679857\n",
      "[1400]\tvalid_0's binary_logloss: 0.679052\n",
      "[1600]\tvalid_0's binary_logloss: 0.678264\n",
      "[1800]\tvalid_0's binary_logloss: 0.677552\n",
      "[2000]\tvalid_0's binary_logloss: 0.677029\n",
      "[2200]\tvalid_0's binary_logloss: 0.67671\n",
      "[2400]\tvalid_0's binary_logloss: 0.676386\n",
      "[2600]\tvalid_0's binary_logloss: 0.676087\n",
      "[2800]\tvalid_0's binary_logloss: 0.675754\n",
      "[3000]\tvalid_0's binary_logloss: 0.675553\n",
      "[3200]\tvalid_0's binary_logloss: 0.675356\n",
      "[3400]\tvalid_0's binary_logloss: 0.67513\n",
      "[3600]\tvalid_0's binary_logloss: 0.675015\n",
      "[3800]\tvalid_0's binary_logloss: 0.674904\n",
      "[4000]\tvalid_0's binary_logloss: 0.674815\n",
      "[4200]\tvalid_0's binary_logloss: 0.67476\n",
      "[4400]\tvalid_0's binary_logloss: 0.674672\n",
      "Early stopping, best iteration is:\n",
      "[4482]\tvalid_0's binary_logloss: 0.674665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  10%|#         | 2/20 [01:02<07:03, 23.52s/it]\n",
      "regularization_factors, val_score: 0.674589:  15%|#5        | 3/20 [01:02<05:59, 21.14s/it][I 2020-09-17 04:22:48,353] Finished trial#42 with value: 0.6746653852389557 with parameters: {'lambda_l1': 0.001588105965506813, 'lambda_l2': 0.013943115556317996}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  15%|#5        | 3/20 [01:02<05:59, 21.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687802\n",
      "[400]\tvalid_0's binary_logloss: 0.685347\n",
      "[600]\tvalid_0's binary_logloss: 0.683397\n",
      "[800]\tvalid_0's binary_logloss: 0.681941\n",
      "[1000]\tvalid_0's binary_logloss: 0.680878\n",
      "[1200]\tvalid_0's binary_logloss: 0.679912\n",
      "[1400]\tvalid_0's binary_logloss: 0.679122\n",
      "[1600]\tvalid_0's binary_logloss: 0.678316\n",
      "[1800]\tvalid_0's binary_logloss: 0.67759\n",
      "[2000]\tvalid_0's binary_logloss: 0.67705\n",
      "[2200]\tvalid_0's binary_logloss: 0.676638\n",
      "[2400]\tvalid_0's binary_logloss: 0.676339\n",
      "[2600]\tvalid_0's binary_logloss: 0.676002\n",
      "[2800]\tvalid_0's binary_logloss: 0.675709\n",
      "[3000]\tvalid_0's binary_logloss: 0.67552\n",
      "[3200]\tvalid_0's binary_logloss: 0.675421\n",
      "[3400]\tvalid_0's binary_logloss: 0.675187\n",
      "[3600]\tvalid_0's binary_logloss: 0.675054\n",
      "[3800]\tvalid_0's binary_logloss: 0.67494\n",
      "[4000]\tvalid_0's binary_logloss: 0.674841\n",
      "[4200]\tvalid_0's binary_logloss: 0.674816\n",
      "[4400]\tvalid_0's binary_logloss: 0.674742\n",
      "Early stopping, best iteration is:\n",
      "[4491]\tvalid_0's binary_logloss: 0.674704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  15%|#5        | 3/20 [01:24<05:59, 21.14s/it]\n",
      "regularization_factors, val_score: 0.674589:  20%|##        | 4/20 [01:24<05:45, 21.57s/it][I 2020-09-17 04:23:10,939] Finished trial#43 with value: 0.67470411587476 with parameters: {'lambda_l1': 0.0048223882930622155, 'lambda_l2': 4.2477651935020164e-07}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  20%|##        | 4/20 [01:24<05:45, 21.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688196\n",
      "[400]\tvalid_0's binary_logloss: 0.685946\n",
      "[600]\tvalid_0's binary_logloss: 0.684093\n",
      "[800]\tvalid_0's binary_logloss: 0.682692\n",
      "[1000]\tvalid_0's binary_logloss: 0.68153\n",
      "[1200]\tvalid_0's binary_logloss: 0.680463\n",
      "[1400]\tvalid_0's binary_logloss: 0.679547\n",
      "[1600]\tvalid_0's binary_logloss: 0.678699\n",
      "[1800]\tvalid_0's binary_logloss: 0.678033\n",
      "[2000]\tvalid_0's binary_logloss: 0.67744\n",
      "[2200]\tvalid_0's binary_logloss: 0.676977\n",
      "[2400]\tvalid_0's binary_logloss: 0.676501\n",
      "[2600]\tvalid_0's binary_logloss: 0.676148\n",
      "[2800]\tvalid_0's binary_logloss: 0.675848\n",
      "[3000]\tvalid_0's binary_logloss: 0.675586\n",
      "[3200]\tvalid_0's binary_logloss: 0.675376\n",
      "[3400]\tvalid_0's binary_logloss: 0.675224\n",
      "[3600]\tvalid_0's binary_logloss: 0.675144\n",
      "[3800]\tvalid_0's binary_logloss: 0.675078\n",
      "[4000]\tvalid_0's binary_logloss: 0.674987\n",
      "[4200]\tvalid_0's binary_logloss: 0.674895\n",
      "[4400]\tvalid_0's binary_logloss: 0.674768\n",
      "[4600]\tvalid_0's binary_logloss: 0.674696\n",
      "[4800]\tvalid_0's binary_logloss: 0.674694\n",
      "Early stopping, best iteration is:\n",
      "[4862]\tvalid_0's binary_logloss: 0.674635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  20%|##        | 4/20 [01:48<05:45, 21.57s/it]\n",
      "regularization_factors, val_score: 0.674589:  25%|##5       | 5/20 [01:48<05:34, 22.32s/it][I 2020-09-17 04:23:35,009] Finished trial#44 with value: 0.6746349033791571 with parameters: {'lambda_l1': 2.7548410791110656, 'lambda_l2': 0.00011651464384302558}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  25%|##5       | 5/20 [01:48<05:34, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687796\n",
      "[400]\tvalid_0's binary_logloss: 0.685317\n",
      "[600]\tvalid_0's binary_logloss: 0.683332\n",
      "[800]\tvalid_0's binary_logloss: 0.681847\n",
      "[1000]\tvalid_0's binary_logloss: 0.680744\n",
      "[1200]\tvalid_0's binary_logloss: 0.679826\n",
      "[1400]\tvalid_0's binary_logloss: 0.679099\n",
      "[1600]\tvalid_0's binary_logloss: 0.678295\n",
      "[1800]\tvalid_0's binary_logloss: 0.677557\n",
      "[2000]\tvalid_0's binary_logloss: 0.676944\n",
      "[2200]\tvalid_0's binary_logloss: 0.676596\n",
      "[2400]\tvalid_0's binary_logloss: 0.676315\n",
      "[2600]\tvalid_0's binary_logloss: 0.675998\n",
      "[2800]\tvalid_0's binary_logloss: 0.675698\n",
      "[3000]\tvalid_0's binary_logloss: 0.675524\n",
      "[3200]\tvalid_0's binary_logloss: 0.675315\n",
      "[3400]\tvalid_0's binary_logloss: 0.67512\n",
      "[3600]\tvalid_0's binary_logloss: 0.674979\n",
      "[3800]\tvalid_0's binary_logloss: 0.674891\n",
      "[4000]\tvalid_0's binary_logloss: 0.674839\n",
      "Early stopping, best iteration is:\n",
      "[3966]\tvalid_0's binary_logloss: 0.674806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  25%|##5       | 5/20 [02:03<05:34, 22.32s/it]\n",
      "regularization_factors, val_score: 0.674589:  30%|###       | 6/20 [02:03<04:41, 20.11s/it][I 2020-09-17 04:23:49,957] Finished trial#45 with value: 0.674806444258168 with parameters: {'lambda_l1': 0.0023226621303606538, 'lambda_l2': 0.00011296454213802948}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  30%|###       | 6/20 [02:03<04:41, 20.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687796\n",
      "[400]\tvalid_0's binary_logloss: 0.685357\n",
      "[600]\tvalid_0's binary_logloss: 0.683376\n",
      "[800]\tvalid_0's binary_logloss: 0.681938\n",
      "[1000]\tvalid_0's binary_logloss: 0.680879\n",
      "[1200]\tvalid_0's binary_logloss: 0.679944\n",
      "[1400]\tvalid_0's binary_logloss: 0.679142\n",
      "[1600]\tvalid_0's binary_logloss: 0.678385\n",
      "[1800]\tvalid_0's binary_logloss: 0.677638\n",
      "[2000]\tvalid_0's binary_logloss: 0.677191\n",
      "[2200]\tvalid_0's binary_logloss: 0.676844\n",
      "[2400]\tvalid_0's binary_logloss: 0.676582\n",
      "[2600]\tvalid_0's binary_logloss: 0.676267\n",
      "[2800]\tvalid_0's binary_logloss: 0.675984\n",
      "[3000]\tvalid_0's binary_logloss: 0.675741\n",
      "[3200]\tvalid_0's binary_logloss: 0.675464\n",
      "[3400]\tvalid_0's binary_logloss: 0.675289\n",
      "[3600]\tvalid_0's binary_logloss: 0.675173\n",
      "Early stopping, best iteration is:\n",
      "[3623]\tvalid_0's binary_logloss: 0.675153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  30%|###       | 6/20 [02:25<04:41, 20.11s/it]\n",
      "regularization_factors, val_score: 0.674589:  35%|###5      | 7/20 [02:25<04:26, 20.53s/it][I 2020-09-17 04:24:11,476] Finished trial#46 with value: 0.6751526462327708 with parameters: {'lambda_l1': 0.00017804709958861537, 'lambda_l2': 0.0007528930952442684}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  35%|###5      | 7/20 [02:25<04:26, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687796\n",
      "[400]\tvalid_0's binary_logloss: 0.685357\n",
      "[600]\tvalid_0's binary_logloss: 0.683376\n",
      "[800]\tvalid_0's binary_logloss: 0.681938\n",
      "[1000]\tvalid_0's binary_logloss: 0.680873\n",
      "[1200]\tvalid_0's binary_logloss: 0.67991\n",
      "[1400]\tvalid_0's binary_logloss: 0.679177\n",
      "[1600]\tvalid_0's binary_logloss: 0.67834\n",
      "[1800]\tvalid_0's binary_logloss: 0.677632\n",
      "[2000]\tvalid_0's binary_logloss: 0.677166\n",
      "[2200]\tvalid_0's binary_logloss: 0.676819\n",
      "[2400]\tvalid_0's binary_logloss: 0.676605\n",
      "[2600]\tvalid_0's binary_logloss: 0.676258\n",
      "[2800]\tvalid_0's binary_logloss: 0.675945\n",
      "[3000]\tvalid_0's binary_logloss: 0.675679\n",
      "[3200]\tvalid_0's binary_logloss: 0.675437\n",
      "[3400]\tvalid_0's binary_logloss: 0.675271\n",
      "[3600]\tvalid_0's binary_logloss: 0.675197\n",
      "[3800]\tvalid_0's binary_logloss: 0.675099\n",
      "[4000]\tvalid_0's binary_logloss: 0.674974\n",
      "[4200]\tvalid_0's binary_logloss: 0.67489\n",
      "[4400]\tvalid_0's binary_logloss: 0.674817\n",
      "[4600]\tvalid_0's binary_logloss: 0.674761\n",
      "Early stopping, best iteration is:\n",
      "[4563]\tvalid_0's binary_logloss: 0.674746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  35%|###5      | 7/20 [02:41<04:26, 20.53s/it]\n",
      "regularization_factors, val_score: 0.674589:  40%|####      | 8/20 [02:41<03:51, 19.28s/it][I 2020-09-17 04:24:27,819] Finished trial#47 with value: 0.6747462392286756 with parameters: {'lambda_l1': 3.980958272531244e-08, 'lambda_l2': 0.0001081559554154683}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  40%|####      | 8/20 [02:41<03:51, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687924\n",
      "[400]\tvalid_0's binary_logloss: 0.685486\n",
      "[600]\tvalid_0's binary_logloss: 0.683483\n",
      "[800]\tvalid_0's binary_logloss: 0.682008\n",
      "[1000]\tvalid_0's binary_logloss: 0.680852\n",
      "[1200]\tvalid_0's binary_logloss: 0.679885\n",
      "[1400]\tvalid_0's binary_logloss: 0.67915\n",
      "[1600]\tvalid_0's binary_logloss: 0.678363\n",
      "[1800]\tvalid_0's binary_logloss: 0.677657\n",
      "[2000]\tvalid_0's binary_logloss: 0.67718\n",
      "[2200]\tvalid_0's binary_logloss: 0.67677\n",
      "[2400]\tvalid_0's binary_logloss: 0.676472\n",
      "[2600]\tvalid_0's binary_logloss: 0.676117\n",
      "[2800]\tvalid_0's binary_logloss: 0.675869\n",
      "[3000]\tvalid_0's binary_logloss: 0.675724\n",
      "[3200]\tvalid_0's binary_logloss: 0.675411\n",
      "[3400]\tvalid_0's binary_logloss: 0.675241\n",
      "[3600]\tvalid_0's binary_logloss: 0.675111\n",
      "[3800]\tvalid_0's binary_logloss: 0.675052\n",
      "[4000]\tvalid_0's binary_logloss: 0.674928\n",
      "[4200]\tvalid_0's binary_logloss: 0.674866\n",
      "[4400]\tvalid_0's binary_logloss: 0.674779\n",
      "[4600]\tvalid_0's binary_logloss: 0.674735\n",
      "Early stopping, best iteration is:\n",
      "[4589]\tvalid_0's binary_logloss: 0.674714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  40%|####      | 8/20 [03:04<03:51, 19.28s/it]\n",
      "regularization_factors, val_score: 0.674589:  45%|####5     | 9/20 [03:04<03:43, 20.33s/it][I 2020-09-17 04:24:50,617] Finished trial#48 with value: 0.6747138960597047 with parameters: {'lambda_l1': 0.3661278372326799, 'lambda_l2': 0.15205652743580084}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  45%|####5     | 9/20 [03:04<03:43, 20.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687919\n",
      "[400]\tvalid_0's binary_logloss: 0.685487\n",
      "[600]\tvalid_0's binary_logloss: 0.683499\n",
      "[800]\tvalid_0's binary_logloss: 0.682007\n",
      "[1000]\tvalid_0's binary_logloss: 0.680825\n",
      "[1200]\tvalid_0's binary_logloss: 0.679801\n",
      "[1400]\tvalid_0's binary_logloss: 0.679016\n",
      "[1600]\tvalid_0's binary_logloss: 0.678179\n",
      "[1800]\tvalid_0's binary_logloss: 0.677502\n",
      "[2000]\tvalid_0's binary_logloss: 0.67695\n",
      "[2200]\tvalid_0's binary_logloss: 0.676648\n",
      "[2400]\tvalid_0's binary_logloss: 0.676351\n",
      "[2600]\tvalid_0's binary_logloss: 0.676037\n",
      "[2800]\tvalid_0's binary_logloss: 0.67581\n",
      "[3000]\tvalid_0's binary_logloss: 0.675657\n",
      "[3200]\tvalid_0's binary_logloss: 0.675471\n",
      "[3400]\tvalid_0's binary_logloss: 0.67517\n",
      "[3600]\tvalid_0's binary_logloss: 0.675099\n",
      "[3800]\tvalid_0's binary_logloss: 0.674997\n",
      "[4000]\tvalid_0's binary_logloss: 0.674951\n",
      "[4200]\tvalid_0's binary_logloss: 0.674886\n",
      "[4400]\tvalid_0's binary_logloss: 0.674816\n",
      "Early stopping, best iteration is:\n",
      "[4419]\tvalid_0's binary_logloss: 0.674804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  45%|####5     | 9/20 [03:28<03:43, 20.33s/it]\n",
      "regularization_factors, val_score: 0.674589:  50%|#####     | 10/20 [03:28<03:33, 21.36s/it][I 2020-09-17 04:25:14,376] Finished trial#49 with value: 0.674804199180319 with parameters: {'lambda_l1': 0.4762439386283276, 'lambda_l2': 3.4485865898832724e-07}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  50%|#####     | 10/20 [03:28<03:33, 21.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688089\n",
      "[400]\tvalid_0's binary_logloss: 0.685811\n",
      "[600]\tvalid_0's binary_logloss: 0.683992\n",
      "[800]\tvalid_0's binary_logloss: 0.68262\n",
      "[1000]\tvalid_0's binary_logloss: 0.681473\n",
      "[1200]\tvalid_0's binary_logloss: 0.680493\n",
      "[1400]\tvalid_0's binary_logloss: 0.679688\n",
      "[1600]\tvalid_0's binary_logloss: 0.678887\n",
      "[1800]\tvalid_0's binary_logloss: 0.678158\n",
      "[2000]\tvalid_0's binary_logloss: 0.677724\n",
      "[2200]\tvalid_0's binary_logloss: 0.677324\n",
      "[2400]\tvalid_0's binary_logloss: 0.676921\n",
      "[2600]\tvalid_0's binary_logloss: 0.676638\n",
      "[2800]\tvalid_0's binary_logloss: 0.676444\n",
      "[3000]\tvalid_0's binary_logloss: 0.676391\n",
      "[3200]\tvalid_0's binary_logloss: 0.676259\n",
      "[3400]\tvalid_0's binary_logloss: 0.676151\n",
      "[3600]\tvalid_0's binary_logloss: 0.675991\n",
      "[3800]\tvalid_0's binary_logloss: 0.67589\n",
      "[4000]\tvalid_0's binary_logloss: 0.675821\n",
      "[4200]\tvalid_0's binary_logloss: 0.675665\n",
      "[4400]\tvalid_0's binary_logloss: 0.675562\n",
      "[4600]\tvalid_0's binary_logloss: 0.675442\n",
      "[4800]\tvalid_0's binary_logloss: 0.675334\n",
      "[5000]\tvalid_0's binary_logloss: 0.675242\n",
      "[5200]\tvalid_0's binary_logloss: 0.675159\n",
      "[5400]\tvalid_0's binary_logloss: 0.675094\n",
      "[5600]\tvalid_0's binary_logloss: 0.675087\n",
      "[5800]\tvalid_0's binary_logloss: 0.675014\n",
      "Early stopping, best iteration is:\n",
      "[5787]\tvalid_0's binary_logloss: 0.675008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  50%|#####     | 10/20 [03:54<03:33, 21.36s/it]\n",
      "regularization_factors, val_score: 0.674589:  55%|#####5    | 11/20 [03:54<03:25, 22.81s/it][I 2020-09-17 04:25:40,568] Finished trial#50 with value: 0.6750082072163728 with parameters: {'lambda_l1': 2.659057856420227e-06, 'lambda_l2': 6.691264609024429}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  55%|#####5    | 11/20 [03:54<03:25, 22.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688591\n",
      "[400]\tvalid_0's binary_logloss: 0.686698\n",
      "[600]\tvalid_0's binary_logloss: 0.685095\n",
      "[800]\tvalid_0's binary_logloss: 0.683755\n",
      "[1000]\tvalid_0's binary_logloss: 0.682667\n",
      "[1200]\tvalid_0's binary_logloss: 0.681683\n",
      "[1400]\tvalid_0's binary_logloss: 0.680885\n",
      "[1600]\tvalid_0's binary_logloss: 0.680098\n",
      "[1800]\tvalid_0's binary_logloss: 0.679353\n",
      "[2000]\tvalid_0's binary_logloss: 0.678717\n",
      "[2200]\tvalid_0's binary_logloss: 0.678191\n",
      "[2400]\tvalid_0's binary_logloss: 0.677762\n",
      "[2600]\tvalid_0's binary_logloss: 0.677378\n",
      "[2800]\tvalid_0's binary_logloss: 0.67701\n",
      "[3000]\tvalid_0's binary_logloss: 0.676639\n",
      "[3200]\tvalid_0's binary_logloss: 0.67635\n",
      "[3400]\tvalid_0's binary_logloss: 0.676113\n",
      "[3600]\tvalid_0's binary_logloss: 0.675873\n",
      "[3800]\tvalid_0's binary_logloss: 0.675678\n",
      "[4000]\tvalid_0's binary_logloss: 0.675515\n",
      "[4200]\tvalid_0's binary_logloss: 0.675412\n",
      "[4400]\tvalid_0's binary_logloss: 0.67527\n",
      "[4600]\tvalid_0's binary_logloss: 0.675138\n",
      "[4800]\tvalid_0's binary_logloss: 0.675029\n",
      "[5000]\tvalid_0's binary_logloss: 0.674895\n",
      "[5200]\tvalid_0's binary_logloss: 0.674809\n",
      "[5400]\tvalid_0's binary_logloss: 0.674742\n",
      "[5600]\tvalid_0's binary_logloss: 0.6747\n",
      "Early stopping, best iteration is:\n",
      "[5616]\tvalid_0's binary_logloss: 0.674686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  55%|#####5    | 11/20 [04:21<03:25, 22.81s/it]\n",
      "regularization_factors, val_score: 0.674589:  60%|######    | 12/20 [04:21<03:12, 24.03s/it][I 2020-09-17 04:26:07,435] Finished trial#51 with value: 0.6746856375958534 with parameters: {'lambda_l1': 8.491757896751016, 'lambda_l2': 4.252399479060845e-06}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  60%|######    | 12/20 [04:21<03:12, 24.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687866\n",
      "[400]\tvalid_0's binary_logloss: 0.68543\n",
      "[600]\tvalid_0's binary_logloss: 0.68345\n",
      "[800]\tvalid_0's binary_logloss: 0.682041\n",
      "[1000]\tvalid_0's binary_logloss: 0.680896\n",
      "[1200]\tvalid_0's binary_logloss: 0.679914\n",
      "[1400]\tvalid_0's binary_logloss: 0.679132\n",
      "[1600]\tvalid_0's binary_logloss: 0.67833\n",
      "[1800]\tvalid_0's binary_logloss: 0.677637\n",
      "[2000]\tvalid_0's binary_logloss: 0.677143\n",
      "[2200]\tvalid_0's binary_logloss: 0.676796\n",
      "[2400]\tvalid_0's binary_logloss: 0.676473\n",
      "[2600]\tvalid_0's binary_logloss: 0.676125\n",
      "[2800]\tvalid_0's binary_logloss: 0.675867\n",
      "[3000]\tvalid_0's binary_logloss: 0.675661\n",
      "[3200]\tvalid_0's binary_logloss: 0.675438\n",
      "[3400]\tvalid_0's binary_logloss: 0.67532\n",
      "[3600]\tvalid_0's binary_logloss: 0.675186\n",
      "[3800]\tvalid_0's binary_logloss: 0.675103\n",
      "[4000]\tvalid_0's binary_logloss: 0.675003\n",
      "[4200]\tvalid_0's binary_logloss: 0.674933\n",
      "[4400]\tvalid_0's binary_logloss: 0.674832\n",
      "Early stopping, best iteration is:\n",
      "[4463]\tvalid_0's binary_logloss: 0.674794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  60%|######    | 12/20 [04:45<03:12, 24.03s/it]\n",
      "regularization_factors, val_score: 0.674589:  65%|######5   | 13/20 [04:45<02:48, 24.03s/it][I 2020-09-17 04:26:31,471] Finished trial#52 with value: 0.6747937855089606 with parameters: {'lambda_l1': 0.1708372906432063, 'lambda_l2': 0.0055472052122203}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  65%|######5   | 13/20 [04:45<02:48, 24.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688687\n",
      "[400]\tvalid_0's binary_logloss: 0.686875\n",
      "[600]\tvalid_0's binary_logloss: 0.685336\n",
      "[800]\tvalid_0's binary_logloss: 0.684009\n",
      "[1000]\tvalid_0's binary_logloss: 0.682952\n",
      "[1200]\tvalid_0's binary_logloss: 0.681999\n",
      "[1400]\tvalid_0's binary_logloss: 0.6812\n",
      "[1600]\tvalid_0's binary_logloss: 0.680412\n",
      "[1800]\tvalid_0's binary_logloss: 0.679671\n",
      "[2000]\tvalid_0's binary_logloss: 0.678996\n",
      "[2200]\tvalid_0's binary_logloss: 0.678452\n",
      "[2400]\tvalid_0's binary_logloss: 0.678004\n",
      "[2600]\tvalid_0's binary_logloss: 0.67761\n",
      "[2800]\tvalid_0's binary_logloss: 0.677223\n",
      "[3000]\tvalid_0's binary_logloss: 0.676876\n",
      "[3200]\tvalid_0's binary_logloss: 0.676563\n",
      "[3400]\tvalid_0's binary_logloss: 0.676326\n",
      "[3600]\tvalid_0's binary_logloss: 0.676123\n",
      "[3800]\tvalid_0's binary_logloss: 0.675931\n",
      "[4000]\tvalid_0's binary_logloss: 0.675764\n",
      "[4200]\tvalid_0's binary_logloss: 0.675638\n",
      "[4400]\tvalid_0's binary_logloss: 0.675558\n",
      "[4600]\tvalid_0's binary_logloss: 0.675446\n",
      "[4800]\tvalid_0's binary_logloss: 0.675342\n",
      "[5000]\tvalid_0's binary_logloss: 0.675261\n",
      "[5200]\tvalid_0's binary_logloss: 0.675135\n",
      "[5400]\tvalid_0's binary_logloss: 0.675048\n",
      "[5600]\tvalid_0's binary_logloss: 0.674987\n",
      "[5800]\tvalid_0's binary_logloss: 0.674945\n",
      "[6000]\tvalid_0's binary_logloss: 0.674913\n",
      "Early stopping, best iteration is:\n",
      "[5973]\tvalid_0's binary_logloss: 0.674906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  65%|######5   | 13/20 [05:13<02:48, 24.03s/it]\n",
      "regularization_factors, val_score: 0.674589:  70%|#######   | 14/20 [05:13<02:31, 25.27s/it][I 2020-09-17 04:26:59,619] Finished trial#53 with value: 0.674906088823108 with parameters: {'lambda_l1': 9.49720958249382, 'lambda_l2': 1.2720527281955158e-08}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  70%|#######   | 14/20 [05:13<02:31, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687831\n",
      "[400]\tvalid_0's binary_logloss: 0.685385\n",
      "[600]\tvalid_0's binary_logloss: 0.683411\n",
      "[800]\tvalid_0's binary_logloss: 0.681906\n",
      "[1000]\tvalid_0's binary_logloss: 0.680794\n",
      "[1200]\tvalid_0's binary_logloss: 0.679865\n",
      "[1400]\tvalid_0's binary_logloss: 0.67911\n",
      "[1600]\tvalid_0's binary_logloss: 0.678315\n",
      "[1800]\tvalid_0's binary_logloss: 0.677611\n",
      "[2000]\tvalid_0's binary_logloss: 0.677086\n",
      "[2200]\tvalid_0's binary_logloss: 0.67672\n",
      "[2400]\tvalid_0's binary_logloss: 0.676422\n",
      "[2600]\tvalid_0's binary_logloss: 0.676097\n",
      "[2800]\tvalid_0's binary_logloss: 0.675786\n",
      "[3000]\tvalid_0's binary_logloss: 0.675594\n",
      "[3200]\tvalid_0's binary_logloss: 0.675431\n",
      "[3400]\tvalid_0's binary_logloss: 0.675249\n",
      "[3600]\tvalid_0's binary_logloss: 0.675094\n",
      "[3800]\tvalid_0's binary_logloss: 0.674987\n",
      "[4000]\tvalid_0's binary_logloss: 0.674931\n",
      "[4200]\tvalid_0's binary_logloss: 0.674897\n",
      "[4400]\tvalid_0's binary_logloss: 0.674797\n",
      "Early stopping, best iteration is:\n",
      "[4404]\tvalid_0's binary_logloss: 0.674788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  70%|#######   | 14/20 [05:37<02:31, 25.27s/it]\n",
      "regularization_factors, val_score: 0.674589:  75%|#######5  | 15/20 [05:37<02:05, 25.04s/it][I 2020-09-17 04:27:24,153] Finished trial#54 with value: 0.6747880160930478 with parameters: {'lambda_l1': 0.04548338781633459, 'lambda_l2': 0.13962968698202796}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  75%|#######5  | 15/20 [05:37<02:05, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688689\n",
      "[400]\tvalid_0's binary_logloss: 0.686879\n",
      "[600]\tvalid_0's binary_logloss: 0.685348\n",
      "[800]\tvalid_0's binary_logloss: 0.684025\n",
      "[1000]\tvalid_0's binary_logloss: 0.682959\n",
      "[1200]\tvalid_0's binary_logloss: 0.682022\n",
      "[1400]\tvalid_0's binary_logloss: 0.681228\n",
      "[1600]\tvalid_0's binary_logloss: 0.680456\n",
      "[1800]\tvalid_0's binary_logloss: 0.679676\n",
      "[2000]\tvalid_0's binary_logloss: 0.679014\n",
      "[2200]\tvalid_0's binary_logloss: 0.678502\n",
      "[2400]\tvalid_0's binary_logloss: 0.678057\n",
      "[2600]\tvalid_0's binary_logloss: 0.677675\n",
      "[2800]\tvalid_0's binary_logloss: 0.677296\n",
      "[3000]\tvalid_0's binary_logloss: 0.676924\n",
      "[3200]\tvalid_0's binary_logloss: 0.676656\n",
      "[3400]\tvalid_0's binary_logloss: 0.676425\n",
      "[3600]\tvalid_0's binary_logloss: 0.676242\n",
      "[3800]\tvalid_0's binary_logloss: 0.676098\n",
      "[4000]\tvalid_0's binary_logloss: 0.675941\n",
      "[4200]\tvalid_0's binary_logloss: 0.675795\n",
      "[4400]\tvalid_0's binary_logloss: 0.675667\n",
      "[4600]\tvalid_0's binary_logloss: 0.675555\n",
      "[4800]\tvalid_0's binary_logloss: 0.675447\n",
      "[5000]\tvalid_0's binary_logloss: 0.67537\n",
      "[5200]\tvalid_0's binary_logloss: 0.675273\n",
      "[5400]\tvalid_0's binary_logloss: 0.67519\n",
      "[5600]\tvalid_0's binary_logloss: 0.675121\n",
      "[5800]\tvalid_0's binary_logloss: 0.675072\n",
      "[6000]\tvalid_0's binary_logloss: 0.675047\n",
      "[6200]\tvalid_0's binary_logloss: 0.67502\n",
      "Early stopping, best iteration is:\n",
      "[6298]\tvalid_0's binary_logloss: 0.675003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  75%|#######5  | 15/20 [06:15<02:05, 25.04s/it]\n",
      "regularization_factors, val_score: 0.674589:  80%|########  | 16/20 [06:15<01:55, 28.92s/it][I 2020-09-17 04:28:02,113] Finished trial#55 with value: 0.675002500933786 with parameters: {'lambda_l1': 9.530742786256475, 'lambda_l2': 1.6112231885356486e-06}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  80%|########  | 16/20 [06:15<01:55, 28.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687843\n",
      "[400]\tvalid_0's binary_logloss: 0.685394\n",
      "[600]\tvalid_0's binary_logloss: 0.683422\n",
      "[800]\tvalid_0's binary_logloss: 0.68199\n",
      "[1000]\tvalid_0's binary_logloss: 0.680927\n",
      "[1200]\tvalid_0's binary_logloss: 0.679981\n",
      "[1400]\tvalid_0's binary_logloss: 0.679169\n",
      "[1600]\tvalid_0's binary_logloss: 0.678381\n",
      "[1800]\tvalid_0's binary_logloss: 0.677671\n",
      "[2000]\tvalid_0's binary_logloss: 0.677102\n",
      "[2200]\tvalid_0's binary_logloss: 0.67679\n",
      "[2400]\tvalid_0's binary_logloss: 0.676514\n",
      "[2600]\tvalid_0's binary_logloss: 0.676205\n",
      "[2800]\tvalid_0's binary_logloss: 0.675947\n",
      "[3000]\tvalid_0's binary_logloss: 0.675718\n",
      "[3200]\tvalid_0's binary_logloss: 0.675464\n",
      "[3400]\tvalid_0's binary_logloss: 0.675281\n",
      "[3600]\tvalid_0's binary_logloss: 0.675162\n",
      "[3800]\tvalid_0's binary_logloss: 0.675064\n",
      "[4000]\tvalid_0's binary_logloss: 0.674966\n",
      "[4200]\tvalid_0's binary_logloss: 0.674912\n",
      "[4400]\tvalid_0's binary_logloss: 0.674839\n",
      "Early stopping, best iteration is:\n",
      "[4471]\tvalid_0's binary_logloss: 0.674803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674589:  80%|########  | 16/20 [06:32<01:55, 28.92s/it]\n",
      "regularization_factors, val_score: 0.674589:  85%|########5 | 17/20 [06:32<01:15, 25.12s/it][I 2020-09-17 04:28:18,374] Finished trial#56 with value: 0.674802923125697 with parameters: {'lambda_l1': 0.033618784201754084, 'lambda_l2': 0.002395630963592597}. Best is trial#40 with value: 0.6745886371265658.\n",
      "\n",
      "regularization_factors, val_score: 0.674589:  85%|########5 | 17/20 [06:32<01:15, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688001\n",
      "[400]\tvalid_0's binary_logloss: 0.685689\n",
      "[600]\tvalid_0's binary_logloss: 0.683782\n",
      "[800]\tvalid_0's binary_logloss: 0.682341\n",
      "[1000]\tvalid_0's binary_logloss: 0.68115\n",
      "[1200]\tvalid_0's binary_logloss: 0.680067\n",
      "[1400]\tvalid_0's binary_logloss: 0.679258\n",
      "[1600]\tvalid_0's binary_logloss: 0.67844\n",
      "[1800]\tvalid_0's binary_logloss: 0.677754\n",
      "[2000]\tvalid_0's binary_logloss: 0.677245\n",
      "[2200]\tvalid_0's binary_logloss: 0.676804\n",
      "[2400]\tvalid_0's binary_logloss: 0.676459\n",
      "[2600]\tvalid_0's binary_logloss: 0.676126\n",
      "[2800]\tvalid_0's binary_logloss: 0.675852\n",
      "[3000]\tvalid_0's binary_logloss: 0.675568\n",
      "[3200]\tvalid_0's binary_logloss: 0.675303\n",
      "[3400]\tvalid_0's binary_logloss: 0.675091\n",
      "[3600]\tvalid_0's binary_logloss: 0.674942\n",
      "[3800]\tvalid_0's binary_logloss: 0.674861\n",
      "[4000]\tvalid_0's binary_logloss: 0.674752\n",
      "[4200]\tvalid_0's binary_logloss: 0.674683\n",
      "[4400]\tvalid_0's binary_logloss: 0.674618\n",
      "[4600]\tvalid_0's binary_logloss: 0.674545\n",
      "[4800]\tvalid_0's binary_logloss: 0.674517\n",
      "[5000]\tvalid_0's binary_logloss: 0.674382\n",
      "[5200]\tvalid_0's binary_logloss: 0.674291\n",
      "[5400]\tvalid_0's binary_logloss: 0.674202\n",
      "[5600]\tvalid_0's binary_logloss: 0.674101\n",
      "[5800]\tvalid_0's binary_logloss: 0.67411\n",
      "Early stopping, best iteration is:\n",
      "[5757]\tvalid_0's binary_logloss: 0.674081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674081:  85%|########5 | 17/20 [07:04<01:15, 25.12s/it]\n",
      "regularization_factors, val_score: 0.674081:  90%|######### | 18/20 [07:04<00:54, 27.42s/it][I 2020-09-17 04:28:51,139] Finished trial#57 with value: 0.6740812889094047 with parameters: {'lambda_l1': 1.3238244812760307, 'lambda_l2': 1.5708768474099787e-05}. Best is trial#57 with value: 0.6740812889094047.\n",
      "\n",
      "regularization_factors, val_score: 0.674081:  90%|######### | 18/20 [07:04<00:54, 27.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687796\n",
      "[400]\tvalid_0's binary_logloss: 0.685357\n",
      "[600]\tvalid_0's binary_logloss: 0.683376\n",
      "[800]\tvalid_0's binary_logloss: 0.681938\n",
      "[1000]\tvalid_0's binary_logloss: 0.680873\n",
      "[1200]\tvalid_0's binary_logloss: 0.679909\n",
      "[1400]\tvalid_0's binary_logloss: 0.679113\n",
      "[1600]\tvalid_0's binary_logloss: 0.678348\n",
      "[1800]\tvalid_0's binary_logloss: 0.677668\n",
      "[2000]\tvalid_0's binary_logloss: 0.677125\n",
      "[2200]\tvalid_0's binary_logloss: 0.676813\n",
      "[2400]\tvalid_0's binary_logloss: 0.67651\n",
      "[2600]\tvalid_0's binary_logloss: 0.676156\n",
      "[2800]\tvalid_0's binary_logloss: 0.675872\n",
      "[3000]\tvalid_0's binary_logloss: 0.675689\n",
      "[3200]\tvalid_0's binary_logloss: 0.675431\n",
      "[3400]\tvalid_0's binary_logloss: 0.675263\n",
      "[3600]\tvalid_0's binary_logloss: 0.675134\n",
      "[3800]\tvalid_0's binary_logloss: 0.67504\n",
      "[4000]\tvalid_0's binary_logloss: 0.674936\n",
      "[4200]\tvalid_0's binary_logloss: 0.674862\n",
      "[4400]\tvalid_0's binary_logloss: 0.67478\n",
      "Early stopping, best iteration is:\n",
      "[4471]\tvalid_0's binary_logloss: 0.674742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674081:  90%|######### | 18/20 [07:23<00:54, 27.42s/it]\n",
      "regularization_factors, val_score: 0.674081:  95%|#########5| 19/20 [07:23<00:24, 24.71s/it][I 2020-09-17 04:29:09,527] Finished trial#58 with value: 0.6747422380996966 with parameters: {'lambda_l1': 0.00013959338474693345, 'lambda_l2': 1.9129190672754588e-08}. Best is trial#57 with value: 0.6740812889094047.\n",
      "\n",
      "regularization_factors, val_score: 0.674081:  95%|#########5| 19/20 [07:23<00:24, 24.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687842\n",
      "[400]\tvalid_0's binary_logloss: 0.685395\n",
      "[600]\tvalid_0's binary_logloss: 0.683402\n",
      "[800]\tvalid_0's binary_logloss: 0.681943\n",
      "[1000]\tvalid_0's binary_logloss: 0.680835\n",
      "[1200]\tvalid_0's binary_logloss: 0.679764\n",
      "[1400]\tvalid_0's binary_logloss: 0.678998\n",
      "[1600]\tvalid_0's binary_logloss: 0.678192\n",
      "[1800]\tvalid_0's binary_logloss: 0.677464\n",
      "[2000]\tvalid_0's binary_logloss: 0.676979\n",
      "[2200]\tvalid_0's binary_logloss: 0.676663\n",
      "[2400]\tvalid_0's binary_logloss: 0.676401\n",
      "[2600]\tvalid_0's binary_logloss: 0.676113\n",
      "[2800]\tvalid_0's binary_logloss: 0.675822\n",
      "[3000]\tvalid_0's binary_logloss: 0.675577\n",
      "[3200]\tvalid_0's binary_logloss: 0.675375\n",
      "[3400]\tvalid_0's binary_logloss: 0.675255\n",
      "[3600]\tvalid_0's binary_logloss: 0.675076\n",
      "[3800]\tvalid_0's binary_logloss: 0.674987\n",
      "[4000]\tvalid_0's binary_logloss: 0.674892\n",
      "[4200]\tvalid_0's binary_logloss: 0.674803\n",
      "[4400]\tvalid_0's binary_logloss: 0.674739\n",
      "[4600]\tvalid_0's binary_logloss: 0.674691\n",
      "[4800]\tvalid_0's binary_logloss: 0.674723\n",
      "Early stopping, best iteration is:\n",
      "[4727]\tvalid_0's binary_logloss: 0.674679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674081:  95%|#########5| 19/20 [07:48<00:24, 24.71s/it]\n",
      "regularization_factors, val_score: 0.674081: 100%|##########| 20/20 [07:48<00:00, 24.88s/it][I 2020-09-17 04:29:34,816] Finished trial#59 with value: 0.6746789063675058 with parameters: {'lambda_l1': 0.020210293123320133, 'lambda_l2': 9.870380587171968e-06}. Best is trial#57 with value: 0.6740812889094047.\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.674081:   0%|          | 0/5 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688001\n",
      "[400]\tvalid_0's binary_logloss: 0.685682\n",
      "[600]\tvalid_0's binary_logloss: 0.683764\n",
      "[800]\tvalid_0's binary_logloss: 0.682313\n",
      "[1000]\tvalid_0's binary_logloss: 0.681127\n",
      "[1200]\tvalid_0's binary_logloss: 0.680065\n",
      "[1400]\tvalid_0's binary_logloss: 0.679204\n",
      "[1600]\tvalid_0's binary_logloss: 0.67839\n",
      "[1800]\tvalid_0's binary_logloss: 0.677682\n",
      "[2000]\tvalid_0's binary_logloss: 0.677176\n",
      "[2200]\tvalid_0's binary_logloss: 0.676676\n",
      "[2400]\tvalid_0's binary_logloss: 0.676244\n",
      "[2600]\tvalid_0's binary_logloss: 0.675852\n",
      "[2800]\tvalid_0's binary_logloss: 0.675535\n",
      "[3000]\tvalid_0's binary_logloss: 0.675245\n",
      "[3200]\tvalid_0's binary_logloss: 0.674959\n",
      "[3400]\tvalid_0's binary_logloss: 0.674683\n",
      "[3600]\tvalid_0's binary_logloss: 0.674499\n",
      "[3800]\tvalid_0's binary_logloss: 0.67435\n",
      "[4000]\tvalid_0's binary_logloss: 0.67425\n",
      "[4200]\tvalid_0's binary_logloss: 0.674146\n",
      "[4400]\tvalid_0's binary_logloss: 0.674089\n",
      "[4600]\tvalid_0's binary_logloss: 0.67405\n",
      "Early stopping, best iteration is:\n",
      "[4534]\tvalid_0's binary_logloss: 0.674045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674045:   0%|          | 0/5 [00:24<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.674045:  20%|##        | 1/5 [00:24<01:37, 24.29s/it][I 2020-09-17 04:29:59,162] Finished trial#60 with value: 0.6740446048056735 with parameters: {'min_child_samples': 5}. Best is trial#60 with value: 0.6740446048056735.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.674045:  20%|##        | 1/5 [00:24<01:37, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687976\n",
      "[400]\tvalid_0's binary_logloss: 0.685618\n",
      "[600]\tvalid_0's binary_logloss: 0.683726\n",
      "[800]\tvalid_0's binary_logloss: 0.682159\n",
      "[1000]\tvalid_0's binary_logloss: 0.68087\n",
      "[1200]\tvalid_0's binary_logloss: 0.67975\n",
      "[1400]\tvalid_0's binary_logloss: 0.678905\n",
      "[1600]\tvalid_0's binary_logloss: 0.678134\n",
      "[1800]\tvalid_0's binary_logloss: 0.677434\n",
      "[2000]\tvalid_0's binary_logloss: 0.676954\n",
      "[2200]\tvalid_0's binary_logloss: 0.676523\n",
      "[2400]\tvalid_0's binary_logloss: 0.676154\n",
      "[2600]\tvalid_0's binary_logloss: 0.675867\n",
      "[2800]\tvalid_0's binary_logloss: 0.675635\n",
      "[3000]\tvalid_0's binary_logloss: 0.675424\n",
      "[3200]\tvalid_0's binary_logloss: 0.675217\n",
      "[3400]\tvalid_0's binary_logloss: 0.675039\n",
      "[3600]\tvalid_0's binary_logloss: 0.674906\n",
      "[3800]\tvalid_0's binary_logloss: 0.674814\n",
      "[4000]\tvalid_0's binary_logloss: 0.674717\n",
      "[4200]\tvalid_0's binary_logloss: 0.674649\n",
      "[4400]\tvalid_0's binary_logloss: 0.674517\n",
      "[4600]\tvalid_0's binary_logloss: 0.674447\n",
      "Early stopping, best iteration is:\n",
      "[4643]\tvalid_0's binary_logloss: 0.674412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674045:  20%|##        | 1/5 [00:48<01:37, 24.29s/it]\n",
      "min_data_in_leaf, val_score: 0.674045:  40%|####      | 2/5 [00:48<01:12, 24.26s/it][I 2020-09-17 04:30:23,350] Finished trial#61 with value: 0.6744120297147896 with parameters: {'min_child_samples': 100}. Best is trial#60 with value: 0.6740446048056735.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.674045:  40%|####      | 2/5 [00:48<01:12, 24.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687989\n",
      "[400]\tvalid_0's binary_logloss: 0.685646\n",
      "[600]\tvalid_0's binary_logloss: 0.683733\n",
      "[800]\tvalid_0's binary_logloss: 0.682168\n",
      "[1000]\tvalid_0's binary_logloss: 0.680875\n",
      "[1200]\tvalid_0's binary_logloss: 0.679812\n",
      "[1400]\tvalid_0's binary_logloss: 0.678931\n",
      "[1600]\tvalid_0's binary_logloss: 0.678146\n",
      "[1800]\tvalid_0's binary_logloss: 0.677412\n",
      "[2000]\tvalid_0's binary_logloss: 0.676923\n",
      "[2200]\tvalid_0's binary_logloss: 0.676444\n",
      "[2400]\tvalid_0's binary_logloss: 0.676056\n",
      "[2600]\tvalid_0's binary_logloss: 0.675842\n",
      "[2800]\tvalid_0's binary_logloss: 0.675636\n",
      "[3000]\tvalid_0's binary_logloss: 0.675549\n",
      "[3200]\tvalid_0's binary_logloss: 0.675487\n",
      "[3400]\tvalid_0's binary_logloss: 0.675349\n",
      "Early stopping, best iteration is:\n",
      "[3400]\tvalid_0's binary_logloss: 0.675349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674045:  40%|####      | 2/5 [01:02<01:12, 24.26s/it]\n",
      "min_data_in_leaf, val_score: 0.674045:  60%|######    | 3/5 [01:02<00:42, 21.15s/it][I 2020-09-17 04:30:37,253] Finished trial#62 with value: 0.6753494852048881 with parameters: {'min_child_samples': 50}. Best is trial#60 with value: 0.6740446048056735.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.674045:  60%|######    | 3/5 [01:02<00:42, 21.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688001\n",
      "[400]\tvalid_0's binary_logloss: 0.685682\n",
      "[600]\tvalid_0's binary_logloss: 0.683764\n",
      "[800]\tvalid_0's binary_logloss: 0.682313\n",
      "[1000]\tvalid_0's binary_logloss: 0.681127\n",
      "[1200]\tvalid_0's binary_logloss: 0.680081\n",
      "[1400]\tvalid_0's binary_logloss: 0.679234\n",
      "[1600]\tvalid_0's binary_logloss: 0.678459\n",
      "[1800]\tvalid_0's binary_logloss: 0.677792\n",
      "[2000]\tvalid_0's binary_logloss: 0.677281\n",
      "[2200]\tvalid_0's binary_logloss: 0.676872\n",
      "[2400]\tvalid_0's binary_logloss: 0.67653\n",
      "[2600]\tvalid_0's binary_logloss: 0.676219\n",
      "[2800]\tvalid_0's binary_logloss: 0.675893\n",
      "[3000]\tvalid_0's binary_logloss: 0.675673\n",
      "[3200]\tvalid_0's binary_logloss: 0.675346\n",
      "[3400]\tvalid_0's binary_logloss: 0.675106\n",
      "[3600]\tvalid_0's binary_logloss: 0.674935\n",
      "[3800]\tvalid_0's binary_logloss: 0.674804\n",
      "[4000]\tvalid_0's binary_logloss: 0.674655\n",
      "[4200]\tvalid_0's binary_logloss: 0.674551\n",
      "[4400]\tvalid_0's binary_logloss: 0.674429\n",
      "[4600]\tvalid_0's binary_logloss: 0.674319\n",
      "[4800]\tvalid_0's binary_logloss: 0.674263\n",
      "[5000]\tvalid_0's binary_logloss: 0.674156\n",
      "[5200]\tvalid_0's binary_logloss: 0.67413\n",
      "[5400]\tvalid_0's binary_logloss: 0.673999\n",
      "Early stopping, best iteration is:\n",
      "[5480]\tvalid_0's binary_logloss: 0.673977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.673977:  60%|######    | 3/5 [01:27<00:42, 21.15s/it]\n",
      "min_data_in_leaf, val_score: 0.673977:  80%|########  | 4/5 [01:27<00:22, 22.30s/it][I 2020-09-17 04:31:02,234] Finished trial#63 with value: 0.6739768446811768 with parameters: {'min_child_samples': 10}. Best is trial#63 with value: 0.6739768446811768.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.673977:  80%|########  | 4/5 [01:27<00:22, 22.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687989\n",
      "[400]\tvalid_0's binary_logloss: 0.685664\n",
      "[600]\tvalid_0's binary_logloss: 0.683752\n",
      "[800]\tvalid_0's binary_logloss: 0.682275\n",
      "[1000]\tvalid_0's binary_logloss: 0.681044\n",
      "[1200]\tvalid_0's binary_logloss: 0.679986\n",
      "[1400]\tvalid_0's binary_logloss: 0.67912\n",
      "[1600]\tvalid_0's binary_logloss: 0.678312\n",
      "[1800]\tvalid_0's binary_logloss: 0.677579\n",
      "[2000]\tvalid_0's binary_logloss: 0.676993\n",
      "[2200]\tvalid_0's binary_logloss: 0.676511\n",
      "[2400]\tvalid_0's binary_logloss: 0.676078\n",
      "[2600]\tvalid_0's binary_logloss: 0.675745\n",
      "[2800]\tvalid_0's binary_logloss: 0.67546\n",
      "[3000]\tvalid_0's binary_logloss: 0.675237\n",
      "[3200]\tvalid_0's binary_logloss: 0.675013\n",
      "[3400]\tvalid_0's binary_logloss: 0.674837\n",
      "[3600]\tvalid_0's binary_logloss: 0.674721\n",
      "[3800]\tvalid_0's binary_logloss: 0.674665\n",
      "[4000]\tvalid_0's binary_logloss: 0.67458\n",
      "[4200]\tvalid_0's binary_logloss: 0.674483\n",
      "[4400]\tvalid_0's binary_logloss: 0.67441\n",
      "[4600]\tvalid_0's binary_logloss: 0.674374\n",
      "Early stopping, best iteration is:\n",
      "[4614]\tvalid_0's binary_logloss: 0.674366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.673977:  80%|########  | 4/5 [01:50<00:22, 22.30s/it]\n",
      "min_data_in_leaf, val_score: 0.673977: 100%|##########| 5/5 [01:50<00:00, 22.67s/it][I 2020-09-17 04:31:25,765] Finished trial#64 with value: 0.6743663551802895 with parameters: {'min_child_samples': 25}. Best is trial#63 with value: 0.6739768446811768.\n",
      "/home/tubotu/.local/lib/python3.6/site-packages/optuna/_experimental.py:90: ExperimentalWarning: train is experimental (supported from v0.18.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688876\n",
      "[400]\tvalid_0's binary_logloss: 0.6877\n",
      "[600]\tvalid_0's binary_logloss: 0.68738\n",
      "[800]\tvalid_0's binary_logloss: 0.687131\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.687116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.687116:   0%|          | 0/7 [00:03<?, ?it/s]\n",
      "feature_fraction, val_score: 0.687116:  14%|#4        | 1/7 [00:03<00:21,  3.66s/it][I 2020-09-17 04:31:29,556] Finished trial#0 with value: 0.6871156174443385 with parameters: {'feature_fraction': 0.4}. Best is trial#0 with value: 0.6871156174443385.\n",
      "\n",
      "feature_fraction, val_score: 0.687116:  14%|#4        | 1/7 [00:03<00:21,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688914\n",
      "[400]\tvalid_0's binary_logloss: 0.688141\n",
      "[600]\tvalid_0's binary_logloss: 0.687964\n",
      "Early stopping, best iteration is:\n",
      "[638]\tvalid_0's binary_logloss: 0.687913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.687116:  14%|#4        | 1/7 [00:13<00:21,  3.66s/it]\n",
      "feature_fraction, val_score: 0.687116:  29%|##8       | 2/7 [00:13<00:28,  5.64s/it][I 2020-09-17 04:31:39,804] Finished trial#1 with value: 0.6879131804552983 with parameters: {'feature_fraction': 0.5}. Best is trial#0 with value: 0.6871156174443385.\n",
      "\n",
      "feature_fraction, val_score: 0.687116:  29%|##8       | 2/7 [00:13<00:28,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689925\n",
      "Early stopping, best iteration is:\n",
      "[296]\tvalid_0's binary_logloss: 0.689676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.687116:  29%|##8       | 2/7 [00:15<00:28,  5.64s/it]\n",
      "feature_fraction, val_score: 0.687116:  43%|####2     | 3/7 [00:15<00:17,  4.46s/it][I 2020-09-17 04:31:41,502] Finished trial#2 with value: 0.6896758475312981 with parameters: {'feature_fraction': 0.8}. Best is trial#0 with value: 0.6871156174443385.\n",
      "\n",
      "feature_fraction, val_score: 0.687116:  43%|####2     | 3/7 [00:15<00:17,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid_0's binary_logloss: 0.690752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.687116:  43%|####2     | 3/7 [00:24<00:17,  4.46s/it]\n",
      "feature_fraction, val_score: 0.687116:  57%|#####7    | 4/7 [00:24<00:17,  5.77s/it][I 2020-09-17 04:31:50,323] Finished trial#3 with value: 0.690751839919072 with parameters: {'feature_fraction': 1.0}. Best is trial#0 with value: 0.6871156174443385.\n",
      "\n",
      "feature_fraction, val_score: 0.687116:  57%|#####7    | 4/7 [00:24<00:17,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688938\n",
      "[400]\tvalid_0's binary_logloss: 0.68838\n",
      "Early stopping, best iteration is:\n",
      "[401]\tvalid_0's binary_logloss: 0.688367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.687116:  57%|#####7    | 4/7 [00:26<00:17,  5.77s/it]\n",
      "feature_fraction, val_score: 0.687116:  71%|#######1  | 5/7 [00:26<00:09,  4.68s/it][I 2020-09-17 04:31:52,476] Finished trial#4 with value: 0.6883674982360238 with parameters: {'feature_fraction': 0.6}. Best is trial#0 with value: 0.6871156174443385.\n",
      "\n",
      "feature_fraction, val_score: 0.687116:  71%|#######1  | 5/7 [00:26<00:09,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.690371\n",
      "Early stopping, best iteration is:\n",
      "[274]\tvalid_0's binary_logloss: 0.690088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.687116:  71%|#######1  | 5/7 [00:28<00:09,  4.68s/it]\n",
      "feature_fraction, val_score: 0.687116:  86%|########5 | 6/7 [00:28<00:03,  3.84s/it][I 2020-09-17 04:31:54,353] Finished trial#5 with value: 0.6900875719661624 with parameters: {'feature_fraction': 0.8999999999999999}. Best is trial#0 with value: 0.6871156174443385.\n",
      "\n",
      "feature_fraction, val_score: 0.687116:  86%|########5 | 6/7 [00:28<00:03,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689363\n",
      "[400]\tvalid_0's binary_logloss: 0.688958\n",
      "Early stopping, best iteration is:\n",
      "[356]\tvalid_0's binary_logloss: 0.688941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.687116:  86%|########5 | 6/7 [00:38<00:03,  3.84s/it]\n",
      "feature_fraction, val_score: 0.687116: 100%|##########| 7/7 [00:38<00:00,  5.58s/it][I 2020-09-17 04:32:03,994] Finished trial#6 with value: 0.6889406452994007 with parameters: {'feature_fraction': 0.7}. Best is trial#0 with value: 0.6871156174443385.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "num_leaves, val_score: 0.687116:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.687116:   0%|          | 0/20 [00:11<?, ?it/s]\n",
      "num_leaves, val_score: 0.687116:   5%|5         | 1/20 [00:11<03:39, 11.55s/it][I 2020-09-17 04:32:15,607] Finished trial#7 with value: 0.6878411251474126 with parameters: {'num_leaves': 92}. Best is trial#7 with value: 0.6878411251474126.\n",
      "\n",
      "num_leaves, val_score: 0.687116:   5%|5         | 1/20 [00:11<03:39, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.687116:   5%|5         | 1/20 [00:14<03:39, 11.55s/it]\n",
      "num_leaves, val_score: 0.687116:  10%|#         | 2/20 [00:14<02:40,  8.91s/it][I 2020-09-17 04:32:18,358] Finished trial#8 with value: 0.6878411251474127 with parameters: {'num_leaves': 255}. Best is trial#7 with value: 0.6878411251474126.\n",
      "\n",
      "num_leaves, val_score: 0.687116:  10%|#         | 2/20 [00:14<02:40,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688836\n",
      "[400]\tvalid_0's binary_logloss: 0.687784\n",
      "[600]\tvalid_0's binary_logloss: 0.68735\n",
      "[800]\tvalid_0's binary_logloss: 0.687004\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.686969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686969:  10%|#         | 2/20 [00:25<02:40,  8.91s/it]\n",
      "num_leaves, val_score: 0.686969:  15%|#5        | 3/20 [00:25<02:45,  9.72s/it][I 2020-09-17 04:32:29,974] Finished trial#9 with value: 0.6869693344973429 with parameters: {'num_leaves': 40}. Best is trial#9 with value: 0.6869693344973429.\n",
      "\n",
      "num_leaves, val_score: 0.686969:  15%|#5        | 3/20 [00:25<02:45,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689176\n",
      "[400]\tvalid_0's binary_logloss: 0.688111\n",
      "[600]\tvalid_0's binary_logloss: 0.68759\n",
      "[800]\tvalid_0's binary_logloss: 0.687325\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.687286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686969:  15%|#5        | 3/20 [00:36<02:45,  9.72s/it]\n",
      "num_leaves, val_score: 0.686969:  20%|##        | 4/20 [00:36<02:40, 10.03s/it][I 2020-09-17 04:32:40,700] Finished trial#10 with value: 0.6872863317165241 with parameters: {'num_leaves': 18}. Best is trial#9 with value: 0.6869693344973429.\n",
      "\n",
      "num_leaves, val_score: 0.686969:  20%|##        | 4/20 [00:36<02:40, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688766\n",
      "[400]\tvalid_0's binary_logloss: 0.687743\n",
      "[600]\tvalid_0's binary_logloss: 0.687304\n",
      "[800]\tvalid_0's binary_logloss: 0.686956\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.686919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  20%|##        | 4/20 [00:40<02:40, 10.03s/it]\n",
      "num_leaves, val_score: 0.686919:  25%|##5       | 5/20 [00:40<02:02,  8.19s/it][I 2020-09-17 04:32:44,630] Finished trial#11 with value: 0.6869185973131977 with parameters: {'num_leaves': 44}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  25%|##5       | 5/20 [00:40<02:02,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  25%|##5       | 5/20 [00:51<02:02,  8.19s/it]\n",
      "num_leaves, val_score: 0.686919:  30%|###       | 6/20 [00:51<02:04,  8.88s/it][I 2020-09-17 04:32:55,117] Finished trial#12 with value: 0.6878411251474126 with parameters: {'num_leaves': 119}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  30%|###       | 6/20 [00:51<02:04,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  30%|###       | 6/20 [01:02<02:04,  8.88s/it]\n",
      "num_leaves, val_score: 0.686919:  35%|###5      | 7/20 [01:02<02:04,  9.60s/it][I 2020-09-17 04:33:06,394] Finished trial#13 with value: 0.6878411251474126 with parameters: {'num_leaves': 251}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  35%|###5      | 7/20 [01:02<02:04,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688741\n",
      "[400]\tvalid_0's binary_logloss: 0.687873\n",
      "[600]\tvalid_0's binary_logloss: 0.687414\n",
      "[800]\tvalid_0's binary_logloss: 0.68711\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.687046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  35%|###5      | 7/20 [01:14<02:04,  9.60s/it]\n",
      "num_leaves, val_score: 0.686919:  40%|####      | 8/20 [01:14<02:05, 10.49s/it][I 2020-09-17 04:33:18,943] Finished trial#14 with value: 0.6870457895552948 with parameters: {'num_leaves': 71}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  40%|####      | 8/20 [01:14<02:05, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  40%|####      | 8/20 [01:21<02:05, 10.49s/it]\n",
      "num_leaves, val_score: 0.686919:  45%|####5     | 9/20 [01:21<01:43,  9.42s/it][I 2020-09-17 04:33:25,863] Finished trial#15 with value: 0.6878411251474126 with parameters: {'num_leaves': 170}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  45%|####5     | 9/20 [01:21<01:43,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  45%|####5     | 9/20 [01:28<01:43,  9.42s/it]\n",
      "num_leaves, val_score: 0.686919:  50%|#####     | 10/20 [01:28<01:27,  8.73s/it][I 2020-09-17 04:33:32,990] Finished trial#16 with value: 0.6878411251474126 with parameters: {'num_leaves': 186}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  50%|#####     | 10/20 [01:28<01:27,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689418\n",
      "[400]\tvalid_0's binary_logloss: 0.688448\n",
      "[600]\tvalid_0's binary_logloss: 0.687911\n",
      "[800]\tvalid_0's binary_logloss: 0.687605\n",
      "[1000]\tvalid_0's binary_logloss: 0.68739\n",
      "[1200]\tvalid_0's binary_logloss: 0.687175\n",
      "Early stopping, best iteration is:\n",
      "[1246]\tvalid_0's binary_logloss: 0.68714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  50%|#####     | 10/20 [01:38<01:27,  8.73s/it]\n",
      "num_leaves, val_score: 0.686919:  55%|#####5    | 11/20 [01:38<01:21,  9.07s/it][I 2020-09-17 04:33:42,866] Finished trial#17 with value: 0.6871399721284502 with parameters: {'num_leaves': 4}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  55%|#####5    | 11/20 [01:38<01:21,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688713\n",
      "[400]\tvalid_0's binary_logloss: 0.687849\n",
      "[600]\tvalid_0's binary_logloss: 0.687364\n",
      "[800]\tvalid_0's binary_logloss: 0.68705\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.687019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  55%|#####5    | 11/20 [01:43<01:21,  9.07s/it]\n",
      "num_leaves, val_score: 0.686919:  60%|######    | 12/20 [01:43<01:01,  7.67s/it][I 2020-09-17 04:33:47,259] Finished trial#18 with value: 0.6870185726147732 with parameters: {'num_leaves': 47}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  60%|######    | 12/20 [01:43<01:01,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688799\n",
      "[400]\tvalid_0's binary_logloss: 0.687804\n",
      "[600]\tvalid_0's binary_logloss: 0.687366\n",
      "[800]\tvalid_0's binary_logloss: 0.687037\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.687001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  60%|######    | 12/20 [01:54<01:01,  7.67s/it]\n",
      "num_leaves, val_score: 0.686919:  65%|######5   | 13/20 [01:54<01:01,  8.72s/it][I 2020-09-17 04:33:58,416] Finished trial#19 with value: 0.687001444359531 with parameters: {'num_leaves': 43}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  65%|######5   | 13/20 [01:54<01:01,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689297\n",
      "[400]\tvalid_0's binary_logloss: 0.68841\n",
      "[600]\tvalid_0's binary_logloss: 0.687937\n",
      "[800]\tvalid_0's binary_logloss: 0.687631\n",
      "[1000]\tvalid_0's binary_logloss: 0.687633\n",
      "Early stopping, best iteration is:\n",
      "[951]\tvalid_0's binary_logloss: 0.687502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  65%|######5   | 13/20 [02:04<01:01,  8.72s/it]\n",
      "num_leaves, val_score: 0.686919:  70%|#######   | 14/20 [02:04<00:54,  9.07s/it][I 2020-09-17 04:34:08,311] Finished trial#20 with value: 0.6875015262651524 with parameters: {'num_leaves': 13}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  70%|#######   | 14/20 [02:04<00:54,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  70%|#######   | 14/20 [02:08<00:54,  9.07s/it]\n",
      "num_leaves, val_score: 0.686919:  75%|#######5  | 15/20 [02:08<00:37,  7.50s/it][I 2020-09-17 04:34:12,133] Finished trial#21 with value: 0.6878411251474126 with parameters: {'num_leaves': 89}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  75%|#######5  | 15/20 [02:08<00:37,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688696\n",
      "[400]\tvalid_0's binary_logloss: 0.687813\n",
      "[600]\tvalid_0's binary_logloss: 0.687313\n",
      "[800]\tvalid_0's binary_logloss: 0.687015\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.686986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  75%|#######5  | 15/20 [02:19<00:37,  7.50s/it]\n",
      "num_leaves, val_score: 0.686919:  80%|########  | 16/20 [02:19<00:35,  8.75s/it][I 2020-09-17 04:34:23,817] Finished trial#22 with value: 0.6869855114894335 with parameters: {'num_leaves': 49}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  80%|########  | 16/20 [02:19<00:35,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  80%|########  | 16/20 [02:30<00:35,  8.75s/it]\n",
      "num_leaves, val_score: 0.686919:  85%|########5 | 17/20 [02:30<00:28,  9.38s/it][I 2020-09-17 04:34:34,648] Finished trial#23 with value: 0.6878411251474125 with parameters: {'num_leaves': 133}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  85%|########5 | 17/20 [02:30<00:28,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689643\n",
      "[400]\tvalid_0's binary_logloss: 0.68889\n",
      "[600]\tvalid_0's binary_logloss: 0.688379\n",
      "[800]\tvalid_0's binary_logloss: 0.688072\n",
      "[1000]\tvalid_0's binary_logloss: 0.687856\n",
      "[1200]\tvalid_0's binary_logloss: 0.6878\n",
      "Early stopping, best iteration is:\n",
      "[1161]\tvalid_0's binary_logloss: 0.687772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  85%|########5 | 17/20 [02:33<00:28,  9.38s/it]\n",
      "num_leaves, val_score: 0.686919:  90%|######### | 18/20 [02:33<00:14,  7.43s/it][I 2020-09-17 04:34:37,536] Finished trial#24 with value: 0.6877723875435483 with parameters: {'num_leaves': 3}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  90%|######### | 18/20 [02:33<00:14,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688659\n",
      "[400]\tvalid_0's binary_logloss: 0.687882\n",
      "Early stopping, best iteration is:\n",
      "[415]\tvalid_0's binary_logloss: 0.687841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  90%|######### | 18/20 [02:44<00:14,  7.43s/it]\n",
      "num_leaves, val_score: 0.686919:  95%|#########5| 19/20 [02:44<00:08,  8.40s/it][I 2020-09-17 04:34:48,201] Finished trial#25 with value: 0.6878411251474126 with parameters: {'num_leaves': 134}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "num_leaves, val_score: 0.686919:  95%|#########5| 19/20 [02:44<00:08,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689026\n",
      "[400]\tvalid_0's binary_logloss: 0.687933\n",
      "[600]\tvalid_0's binary_logloss: 0.687467\n",
      "[800]\tvalid_0's binary_logloss: 0.687155\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.687111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.686919:  95%|#########5| 19/20 [02:55<00:08,  8.40s/it]\n",
      "num_leaves, val_score: 0.686919: 100%|##########| 20/20 [02:55<00:00,  9.29s/it][I 2020-09-17 04:34:59,571] Finished trial#26 with value: 0.6871105099585457 with parameters: {'num_leaves': 35}. Best is trial#11 with value: 0.6869185973131977.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "bagging, val_score: 0.686919:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688262\n",
      "[400]\tvalid_0's binary_logloss: 0.687449\n",
      "[600]\tvalid_0's binary_logloss: 0.686918\n",
      "Early stopping, best iteration is:\n",
      "[563]\tvalid_0's binary_logloss: 0.686848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.686848:   0%|          | 0/10 [00:03<?, ?it/s]\n",
      "bagging, val_score: 0.686848:  10%|#         | 1/10 [00:03<00:27,  3.02s/it][I 2020-09-17 04:35:02,656] Finished trial#27 with value: 0.6868476082750822 with parameters: {'bagging_fraction': 0.6147357414972273, 'bagging_freq': 3}. Best is trial#27 with value: 0.6868476082750822.\n",
      "\n",
      "bagging, val_score: 0.686848:  10%|#         | 1/10 [00:03<00:27,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688886\n",
      "[400]\tvalid_0's binary_logloss: 0.687702\n",
      "Early stopping, best iteration is:\n",
      "[408]\tvalid_0's binary_logloss: 0.687624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.686848:  10%|#         | 1/10 [00:12<00:27,  3.02s/it]\n",
      "bagging, val_score: 0.686848:  20%|##        | 2/10 [00:12<00:40,  5.07s/it][I 2020-09-17 04:35:12,518] Finished trial#28 with value: 0.6876235961561988 with parameters: {'bagging_fraction': 0.6041888066398455, 'bagging_freq': 2}. Best is trial#27 with value: 0.6868476082750822.\n",
      "\n",
      "bagging, val_score: 0.686848:  20%|##        | 2/10 [00:12<00:40,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689014\n",
      "[400]\tvalid_0's binary_logloss: 0.688052\n",
      "[600]\tvalid_0's binary_logloss: 0.687283\n",
      "[800]\tvalid_0's binary_logloss: 0.686586\n",
      "Early stopping, best iteration is:\n",
      "[899]\tvalid_0's binary_logloss: 0.686412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.686412:  20%|##        | 2/10 [00:25<00:40,  5.07s/it]\n",
      "bagging, val_score: 0.686412:  30%|###       | 3/10 [00:25<00:50,  7.26s/it][I 2020-09-17 04:35:24,869] Finished trial#29 with value: 0.686411728326986 with parameters: {'bagging_fraction': 0.6427129051945742, 'bagging_freq': 5}. Best is trial#29 with value: 0.686411728326986.\n",
      "\n",
      "bagging, val_score: 0.686412:  30%|###       | 3/10 [00:25<00:50,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688388\n",
      "[400]\tvalid_0's binary_logloss: 0.686904\n",
      "[600]\tvalid_0's binary_logloss: 0.68658\n",
      "[800]\tvalid_0's binary_logloss: 0.685966\n",
      "[1000]\tvalid_0's binary_logloss: 0.685779\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.685717:  30%|###       | 3/10 [00:38<00:50,  7.26s/it]\n",
      "bagging, val_score: 0.685717:  40%|####      | 4/10 [00:38<00:54,  9.16s/it][I 2020-09-17 04:35:38,490] Finished trial#30 with value: 0.6857168788384406 with parameters: {'bagging_fraction': 0.8921215602747798, 'bagging_freq': 6}. Best is trial#30 with value: 0.6857168788384406.\n",
      "\n",
      "bagging, val_score: 0.685717:  40%|####      | 4/10 [00:38<00:54,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688743\n",
      "[400]\tvalid_0's binary_logloss: 0.687486\n",
      "[600]\tvalid_0's binary_logloss: 0.68707\n",
      "[800]\tvalid_0's binary_logloss: 0.686696\n",
      "Early stopping, best iteration is:\n",
      "[875]\tvalid_0's binary_logloss: 0.686397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.685717:  40%|####      | 4/10 [00:51<00:54,  9.16s/it]\n",
      "bagging, val_score: 0.685717:  50%|#####     | 5/10 [00:51<00:50, 10.14s/it][I 2020-09-17 04:35:50,900] Finished trial#31 with value: 0.6863965863004748 with parameters: {'bagging_fraction': 0.8683910571957012, 'bagging_freq': 3}. Best is trial#30 with value: 0.6857168788384406.\n",
      "\n",
      "bagging, val_score: 0.685717:  50%|#####     | 5/10 [00:51<00:50, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689308\n",
      "[400]\tvalid_0's binary_logloss: 0.687763\n",
      "[600]\tvalid_0's binary_logloss: 0.687546\n",
      "Early stopping, best iteration is:\n",
      "[535]\tvalid_0's binary_logloss: 0.68737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.685717:  50%|#####     | 5/10 [00:54<00:50, 10.14s/it]\n",
      "bagging, val_score: 0.685717:  60%|######    | 6/10 [00:54<00:32,  8.01s/it][I 2020-09-17 04:35:53,960] Finished trial#32 with value: 0.6873696375310773 with parameters: {'bagging_fraction': 0.694701408034845, 'bagging_freq': 7}. Best is trial#30 with value: 0.6857168788384406.\n",
      "\n",
      "bagging, val_score: 0.685717:  60%|######    | 6/10 [00:54<00:32,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689028\n",
      "[400]\tvalid_0's binary_logloss: 0.687941\n",
      "[600]\tvalid_0's binary_logloss: 0.687514\n",
      "[800]\tvalid_0's binary_logloss: 0.687005\n",
      "Early stopping, best iteration is:\n",
      "[832]\tvalid_0's binary_logloss: 0.686917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.685717:  60%|######    | 6/10 [01:06<00:32,  8.01s/it]\n",
      "bagging, val_score: 0.685717:  70%|#######   | 7/10 [01:06<00:27,  9.27s/it][I 2020-09-17 04:36:06,167] Finished trial#33 with value: 0.6869165773916026 with parameters: {'bagging_fraction': 0.5448438480222115, 'bagging_freq': 1}. Best is trial#30 with value: 0.6857168788384406.\n",
      "\n",
      "bagging, val_score: 0.685717:  70%|#######   | 7/10 [01:06<00:27,  9.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688899\n",
      "[400]\tvalid_0's binary_logloss: 0.687759\n",
      "[600]\tvalid_0's binary_logloss: 0.68724\n",
      "[800]\tvalid_0's binary_logloss: 0.686669\n",
      "Early stopping, best iteration is:\n",
      "[804]\tvalid_0's binary_logloss: 0.686601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.685717:  70%|#######   | 7/10 [01:20<00:27,  9.27s/it]\n",
      "bagging, val_score: 0.685717:  80%|########  | 8/10 [01:20<00:21, 10.55s/it][I 2020-09-17 04:36:19,685] Finished trial#34 with value: 0.6866012813399549 with parameters: {'bagging_fraction': 0.9767680298690441, 'bagging_freq': 7}. Best is trial#30 with value: 0.6857168788384406.\n",
      "\n",
      "bagging, val_score: 0.685717:  80%|########  | 8/10 [01:20<00:21, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68938\n",
      "[400]\tvalid_0's binary_logloss: 0.68792\n",
      "[600]\tvalid_0's binary_logloss: 0.687061\n",
      "[800]\tvalid_0's binary_logloss: 0.686795\n",
      "Early stopping, best iteration is:\n",
      "[753]\tvalid_0's binary_logloss: 0.686689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.685717:  80%|########  | 8/10 [01:31<00:21, 10.55s/it]\n",
      "bagging, val_score: 0.685717:  90%|######### | 9/10 [01:31<00:10, 10.81s/it][I 2020-09-17 04:36:31,101] Finished trial#35 with value: 0.6866891244079768 with parameters: {'bagging_fraction': 0.5090267144692345, 'bagging_freq': 1}. Best is trial#30 with value: 0.6857168788384406.\n",
      "\n",
      "bagging, val_score: 0.685717:  90%|######### | 9/10 [01:31<00:10, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.6891\n",
      "[400]\tvalid_0's binary_logloss: 0.688061\n",
      "[600]\tvalid_0's binary_logloss: 0.687486\n",
      "[800]\tvalid_0's binary_logloss: 0.686896\n",
      "Early stopping, best iteration is:\n",
      "[793]\tvalid_0's binary_logloss: 0.686842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.685717:  90%|######### | 9/10 [01:43<00:10, 10.81s/it]\n",
      "bagging, val_score: 0.685717: 100%|##########| 10/10 [01:43<00:00, 11.11s/it][I 2020-09-17 04:36:42,905] Finished trial#36 with value: 0.6868417082494802 with parameters: {'bagging_fraction': 0.536832058485271, 'bagging_freq': 7}. Best is trial#30 with value: 0.6857168788384406.\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.685717:   0%|          | 0/3 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688187\n",
      "[400]\tvalid_0's binary_logloss: 0.687091\n",
      "[600]\tvalid_0's binary_logloss: 0.686845\n",
      "[800]\tvalid_0's binary_logloss: 0.686357\n",
      "Early stopping, best iteration is:\n",
      "[846]\tvalid_0's binary_logloss: 0.686306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.685717:   0%|          | 0/3 [00:24<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.685717:  33%|###3      | 1/3 [00:24<00:48, 24.19s/it][I 2020-09-17 04:37:07,161] Finished trial#37 with value: 0.6863055655145496 with parameters: {'feature_fraction': 0.44800000000000006}. Best is trial#37 with value: 0.6863055655145496.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.685717:  33%|###3      | 1/3 [00:24<00:48, 24.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688255\n",
      "[400]\tvalid_0's binary_logloss: 0.686948\n",
      "Early stopping, best iteration is:\n",
      "[468]\tvalid_0's binary_logloss: 0.686757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.685717:  33%|###3      | 1/3 [00:26<00:48, 24.19s/it]\n",
      "feature_fraction_stage2, val_score: 0.685717:  67%|######6   | 2/3 [00:26<00:17, 17.75s/it][I 2020-09-17 04:37:09,884] Finished trial#38 with value: 0.6867572259930613 with parameters: {'feature_fraction': 0.48000000000000004}. Best is trial#37 with value: 0.6863055655145496.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.685717:  67%|######6   | 2/3 [00:26<00:17, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688388\n",
      "[400]\tvalid_0's binary_logloss: 0.686904\n",
      "[600]\tvalid_0's binary_logloss: 0.68658\n",
      "[800]\tvalid_0's binary_logloss: 0.685966\n",
      "[1000]\tvalid_0's binary_logloss: 0.685779\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.685717:  67%|######6   | 2/3 [00:39<00:17, 17.75s/it]\n",
      "feature_fraction_stage2, val_score: 0.685717: 100%|##########| 3/3 [00:39<00:00, 16.18s/it][I 2020-09-17 04:37:22,410] Finished trial#39 with value: 0.6857168788384406 with parameters: {'feature_fraction': 0.41600000000000004}. Best is trial#39 with value: 0.6857168788384406.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "regularization_factors, val_score: 0.685717:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688394\n",
      "[400]\tvalid_0's binary_logloss: 0.686908\n",
      "[600]\tvalid_0's binary_logloss: 0.686569\n",
      "[800]\tvalid_0's binary_logloss: 0.685973\n",
      "Early stopping, best iteration is:\n",
      "[787]\tvalid_0's binary_logloss: 0.685929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685717:   0%|          | 0/20 [00:12<?, ?it/s]\n",
      "regularization_factors, val_score: 0.685717:   5%|5         | 1/20 [00:12<03:48, 12.02s/it][I 2020-09-17 04:37:34,493] Finished trial#40 with value: 0.6859291992103604 with parameters: {'lambda_l1': 1.2591181277489026e-08, 'lambda_l2': 0.0007663605021662913}. Best is trial#40 with value: 0.6859291992103604.\n",
      "\n",
      "regularization_factors, val_score: 0.685717:   5%|5         | 1/20 [00:12<03:48, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688623\n",
      "[400]\tvalid_0's binary_logloss: 0.687177\n",
      "[600]\tvalid_0's binary_logloss: 0.686606\n",
      "[800]\tvalid_0's binary_logloss: 0.686076\n",
      "Early stopping, best iteration is:\n",
      "[782]\tvalid_0's binary_logloss: 0.686018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685717:   5%|5         | 1/20 [00:24<03:48, 12.02s/it]\n",
      "regularization_factors, val_score: 0.685717:  10%|#         | 2/20 [00:24<03:40, 12.26s/it][I 2020-09-17 04:37:47,320] Finished trial#41 with value: 0.6860179050137076 with parameters: {'lambda_l1': 0.1735326222916935, 'lambda_l2': 0.5725288481158454}. Best is trial#40 with value: 0.6859291992103604.\n",
      "\n",
      "regularization_factors, val_score: 0.685717:  10%|#         | 2/20 [00:24<03:40, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688489\n",
      "[400]\tvalid_0's binary_logloss: 0.687008\n",
      "[600]\tvalid_0's binary_logloss: 0.686568\n",
      "[800]\tvalid_0's binary_logloss: 0.685973\n",
      "[1000]\tvalid_0's binary_logloss: 0.685929\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685698:  10%|#         | 2/20 [00:37<03:40, 12.26s/it]\n",
      "regularization_factors, val_score: 0.685698:  15%|#5        | 3/20 [00:37<03:31, 12.41s/it][I 2020-09-17 04:38:00,079] Finished trial#42 with value: 0.6856980451301025 with parameters: {'lambda_l1': 0.2775966239207228, 'lambda_l2': 0.005898619721236923}. Best is trial#42 with value: 0.6856980451301025.\n",
      "\n",
      "regularization_factors, val_score: 0.685698:  15%|#5        | 3/20 [00:37<03:31, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688815\n",
      "[400]\tvalid_0's binary_logloss: 0.687437\n",
      "[600]\tvalid_0's binary_logloss: 0.686857\n",
      "[800]\tvalid_0's binary_logloss: 0.686368\n",
      "[1000]\tvalid_0's binary_logloss: 0.686145\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685698:  15%|#5        | 3/20 [00:50<03:31, 12.41s/it]\n",
      "regularization_factors, val_score: 0.685698:  20%|##        | 4/20 [00:50<03:19, 12.45s/it][I 2020-09-17 04:38:12,602] Finished trial#43 with value: 0.6859854378343081 with parameters: {'lambda_l1': 0.0017426545966252004, 'lambda_l2': 2.447000449894247}. Best is trial#42 with value: 0.6856980451301025.\n",
      "\n",
      "regularization_factors, val_score: 0.685698:  20%|##        | 4/20 [00:50<03:19, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688326\n",
      "[400]\tvalid_0's binary_logloss: 0.686917\n",
      "[600]\tvalid_0's binary_logloss: 0.686396\n",
      "[800]\tvalid_0's binary_logloss: 0.685791\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's binary_logloss: 0.685746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685698:  20%|##        | 4/20 [01:02<03:19, 12.45s/it]\n",
      "regularization_factors, val_score: 0.685698:  25%|##5       | 5/20 [01:02<03:04, 12.33s/it][I 2020-09-17 04:38:24,657] Finished trial#44 with value: 0.6857458943585981 with parameters: {'lambda_l1': 0.054742818147208445, 'lambda_l2': 7.184049355602323e-06}. Best is trial#42 with value: 0.6856980451301025.\n",
      "\n",
      "regularization_factors, val_score: 0.685698:  25%|##5       | 5/20 [01:02<03:04, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688239\n",
      "[400]\tvalid_0's binary_logloss: 0.686829\n",
      "[600]\tvalid_0's binary_logloss: 0.686323\n",
      "[800]\tvalid_0's binary_logloss: 0.685721\n",
      "[1000]\tvalid_0's binary_logloss: 0.685573\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.68548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  25%|##5       | 5/20 [01:15<03:04, 12.33s/it]\n",
      "regularization_factors, val_score: 0.685480:  30%|###       | 6/20 [01:15<02:55, 12.54s/it][I 2020-09-17 04:38:37,684] Finished trial#45 with value: 0.6854803001563868 with parameters: {'lambda_l1': 0.0028767454721176284, 'lambda_l2': 0.16605655130508243}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  30%|###       | 6/20 [01:15<02:55, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688385\n",
      "[400]\tvalid_0's binary_logloss: 0.686907\n",
      "[600]\tvalid_0's binary_logloss: 0.686561\n",
      "[800]\tvalid_0's binary_logloss: 0.685957\n",
      "[1000]\tvalid_0's binary_logloss: 0.685772\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.68572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  30%|###       | 6/20 [01:27<02:55, 12.54s/it]\n",
      "regularization_factors, val_score: 0.685480:  35%|###5      | 7/20 [01:27<02:42, 12.49s/it][I 2020-09-17 04:38:50,078] Finished trial#46 with value: 0.6857198634604749 with parameters: {'lambda_l1': 8.000597557821416e-07, 'lambda_l2': 0.00029055420544979324}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  35%|###5      | 7/20 [01:27<02:42, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688388\n",
      "[400]\tvalid_0's binary_logloss: 0.686904\n",
      "[600]\tvalid_0's binary_logloss: 0.68658\n",
      "[800]\tvalid_0's binary_logloss: 0.685966\n",
      "[1000]\tvalid_0's binary_logloss: 0.685779\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  35%|###5      | 7/20 [01:40<02:42, 12.49s/it]\n",
      "regularization_factors, val_score: 0.685480:  40%|####      | 8/20 [01:40<02:30, 12.57s/it][I 2020-09-17 04:39:02,820] Finished trial#47 with value: 0.685716878897199 with parameters: {'lambda_l1': 3.7137536278502023e-07, 'lambda_l2': 3.783806732191558e-07}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  40%|####      | 8/20 [01:40<02:30, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688823\n",
      "[400]\tvalid_0's binary_logloss: 0.687528\n",
      "[600]\tvalid_0's binary_logloss: 0.686863\n",
      "[800]\tvalid_0's binary_logloss: 0.686408\n",
      "[1000]\tvalid_0's binary_logloss: 0.686299\n",
      "Early stopping, best iteration is:\n",
      "[935]\tvalid_0's binary_logloss: 0.686138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  40%|####      | 8/20 [01:52<02:30, 12.57s/it]\n",
      "regularization_factors, val_score: 0.685480:  45%|####5     | 9/20 [01:52<02:18, 12.58s/it][I 2020-09-17 04:39:15,430] Finished trial#48 with value: 0.6861381146884278 with parameters: {'lambda_l1': 2.6516143007045056e-06, 'lambda_l2': 4.823660069084337}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  45%|####5     | 9/20 [01:53<02:18, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688396\n",
      "[400]\tvalid_0's binary_logloss: 0.686923\n",
      "[600]\tvalid_0's binary_logloss: 0.686564\n",
      "[800]\tvalid_0's binary_logloss: 0.685956\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's binary_logloss: 0.6859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  45%|####5     | 9/20 [02:05<02:18, 12.58s/it]\n",
      "regularization_factors, val_score: 0.685480:  50%|#####     | 10/20 [02:05<02:05, 12.52s/it][I 2020-09-17 04:39:27,823] Finished trial#49 with value: 0.6858995802066722 with parameters: {'lambda_l1': 4.2111000406158835e-08, 'lambda_l2': 0.0030128819626991598}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  50%|#####     | 10/20 [02:05<02:05, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689403\n",
      "[400]\tvalid_0's binary_logloss: 0.688407\n",
      "[600]\tvalid_0's binary_logloss: 0.687887\n",
      "[800]\tvalid_0's binary_logloss: 0.687493\n",
      "[1000]\tvalid_0's binary_logloss: 0.687266\n",
      "[1200]\tvalid_0's binary_logloss: 0.687085\n",
      "Early stopping, best iteration is:\n",
      "[1204]\tvalid_0's binary_logloss: 0.687077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  50%|#####     | 10/20 [02:18<02:05, 12.52s/it]\n",
      "regularization_factors, val_score: 0.685480:  55%|#####5    | 11/20 [02:18<01:54, 12.72s/it][I 2020-09-17 04:39:40,987] Finished trial#50 with value: 0.6870772639461044 with parameters: {'lambda_l1': 9.452657456751309, 'lambda_l2': 0.14161389498238722}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  55%|#####5    | 11/20 [02:18<01:54, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688366\n",
      "[400]\tvalid_0's binary_logloss: 0.686889\n",
      "[600]\tvalid_0's binary_logloss: 0.686434\n",
      "[800]\tvalid_0's binary_logloss: 0.685778\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's binary_logloss: 0.685719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  55%|#####5    | 11/20 [02:41<01:54, 12.72s/it]\n",
      "regularization_factors, val_score: 0.685480:  60%|######    | 12/20 [02:41<02:06, 15.77s/it][I 2020-09-17 04:40:03,883] Finished trial#51 with value: 0.6857194122283123 with parameters: {'lambda_l1': 0.0003856456387138771, 'lambda_l2': 0.027250480802558506}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  60%|######    | 12/20 [02:41<02:06, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688997\n",
      "[400]\tvalid_0's binary_logloss: 0.687726\n",
      "[600]\tvalid_0's binary_logloss: 0.687156\n",
      "[800]\tvalid_0's binary_logloss: 0.686835\n",
      "[1000]\tvalid_0's binary_logloss: 0.686701\n",
      "[1200]\tvalid_0's binary_logloss: 0.68658\n",
      "Early stopping, best iteration is:\n",
      "[1199]\tvalid_0's binary_logloss: 0.686566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  60%|######    | 12/20 [02:54<02:06, 15.77s/it]\n",
      "regularization_factors, val_score: 0.685480:  65%|######5   | 13/20 [02:54<01:45, 15.01s/it][I 2020-09-17 04:40:17,128] Finished trial#52 with value: 0.6865660223796862 with parameters: {'lambda_l1': 3.35184798535037, 'lambda_l2': 0.015142480302519312}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  65%|######5   | 13/20 [02:54<01:45, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688407\n",
      "[400]\tvalid_0's binary_logloss: 0.686954\n",
      "[600]\tvalid_0's binary_logloss: 0.686515\n",
      "[800]\tvalid_0's binary_logloss: 0.685864\n",
      "Early stopping, best iteration is:\n",
      "[783]\tvalid_0's binary_logloss: 0.685806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685480:  65%|######5   | 13/20 [03:06<01:45, 15.01s/it]\n",
      "regularization_factors, val_score: 0.685480:  70%|#######   | 14/20 [03:06<01:24, 14.13s/it][I 2020-09-17 04:40:29,215] Finished trial#53 with value: 0.685805604538347 with parameters: {'lambda_l1': 0.01590118906589442, 'lambda_l2': 2.5000820267980687e-05}. Best is trial#45 with value: 0.6854803001563868.\n",
      "\n",
      "regularization_factors, val_score: 0.685480:  70%|#######   | 14/20 [03:06<01:24, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688287\n",
      "[400]\tvalid_0's binary_logloss: 0.686853\n",
      "[600]\tvalid_0's binary_logloss: 0.686337\n",
      "[800]\tvalid_0's binary_logloss: 0.685675\n",
      "[1000]\tvalid_0's binary_logloss: 0.685524\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685437:  70%|#######   | 14/20 [03:19<01:24, 14.13s/it]\n",
      "regularization_factors, val_score: 0.685437:  75%|#######5  | 15/20 [03:19<01:08, 13.74s/it][I 2020-09-17 04:40:42,021] Finished trial#54 with value: 0.685436755165546 with parameters: {'lambda_l1': 2.1133732596940643e-05, 'lambda_l2': 0.09333719258168414}. Best is trial#54 with value: 0.685436755165546.\n",
      "\n",
      "regularization_factors, val_score: 0.685437:  75%|#######5  | 15/20 [03:19<01:08, 13.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688928\n",
      "[400]\tvalid_0's binary_logloss: 0.687653\n",
      "[600]\tvalid_0's binary_logloss: 0.687018\n",
      "[800]\tvalid_0's binary_logloss: 0.68648\n",
      "[1000]\tvalid_0's binary_logloss: 0.686189\n",
      "[1200]\tvalid_0's binary_logloss: 0.685941\n",
      "Early stopping, best iteration is:\n",
      "[1197]\tvalid_0's binary_logloss: 0.685936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685437:  75%|#######5  | 15/20 [03:32<01:08, 13.74s/it]\n",
      "regularization_factors, val_score: 0.685437:  80%|########  | 16/20 [03:32<00:54, 13.57s/it][I 2020-09-17 04:40:55,200] Finished trial#55 with value: 0.6859361941007892 with parameters: {'lambda_l1': 2.433982426496836e-05, 'lambda_l2': 7.057139942608938}. Best is trial#54 with value: 0.685436755165546.\n",
      "\n",
      "regularization_factors, val_score: 0.685437:  80%|########  | 16/20 [03:32<00:54, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688294\n",
      "[400]\tvalid_0's binary_logloss: 0.686867\n",
      "[600]\tvalid_0's binary_logloss: 0.686418\n",
      "[800]\tvalid_0's binary_logloss: 0.685749\n",
      "[1000]\tvalid_0's binary_logloss: 0.68559\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.68551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685437:  80%|########  | 16/20 [03:46<00:54, 13.57s/it]\n",
      "regularization_factors, val_score: 0.685437:  85%|########5 | 17/20 [03:46<00:40, 13.57s/it][I 2020-09-17 04:41:08,780] Finished trial#56 with value: 0.6855104673331901 with parameters: {'lambda_l1': 7.10672716312129e-05, 'lambda_l2': 0.11962586858291453}. Best is trial#54 with value: 0.685436755165546.\n",
      "\n",
      "regularization_factors, val_score: 0.685437:  85%|########5 | 17/20 [03:46<00:40, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688424\n",
      "[400]\tvalid_0's binary_logloss: 0.686987\n",
      "[600]\tvalid_0's binary_logloss: 0.686523\n",
      "[800]\tvalid_0's binary_logloss: 0.685863\n",
      "[1000]\tvalid_0's binary_logloss: 0.685795\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685437:  85%|########5 | 17/20 [03:58<00:40, 13.57s/it]\n",
      "regularization_factors, val_score: 0.685437:  90%|######### | 18/20 [03:58<00:26, 13.26s/it][I 2020-09-17 04:41:21,290] Finished trial#57 with value: 0.6856518118996411 with parameters: {'lambda_l1': 0.001435406636773427, 'lambda_l2': 0.3814331631821218}. Best is trial#54 with value: 0.685436755165546.\n",
      "\n",
      "regularization_factors, val_score: 0.685437:  90%|######### | 18/20 [03:58<00:26, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688385\n",
      "[400]\tvalid_0's binary_logloss: 0.686907\n",
      "[600]\tvalid_0's binary_logloss: 0.686561\n",
      "[800]\tvalid_0's binary_logloss: 0.685957\n",
      "[1000]\tvalid_0's binary_logloss: 0.685772\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.68572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685437:  90%|######### | 18/20 [04:11<00:26, 13.26s/it]\n",
      "regularization_factors, val_score: 0.685437:  95%|#########5| 19/20 [04:11<00:13, 13.13s/it][I 2020-09-17 04:41:34,147] Finished trial#58 with value: 0.6857198716916033 with parameters: {'lambda_l1': 2.1711396870552114e-05, 'lambda_l2': 0.00047070828990556583}. Best is trial#54 with value: 0.685436755165546.\n",
      "\n",
      "regularization_factors, val_score: 0.685437:  95%|#########5| 19/20 [04:11<00:13, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688335\n",
      "[400]\tvalid_0's binary_logloss: 0.686925\n",
      "[600]\tvalid_0's binary_logloss: 0.686481\n",
      "[800]\tvalid_0's binary_logloss: 0.68588\n",
      "[1000]\tvalid_0's binary_logloss: 0.685656\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.685437:  95%|#########5| 19/20 [04:24<00:13, 13.13s/it]\n",
      "regularization_factors, val_score: 0.685437: 100%|##########| 20/20 [04:24<00:00, 12.99s/it][I 2020-09-17 04:41:46,786] Finished trial#59 with value: 0.6855747448370123 with parameters: {'lambda_l1': 0.007599498453557788, 'lambda_l2': 0.05163437945564092}. Best is trial#54 with value: 0.685436755165546.\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.685437:   0%|          | 0/5 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688635\n",
      "[400]\tvalid_0's binary_logloss: 0.687306\n",
      "[600]\tvalid_0's binary_logloss: 0.686849\n",
      "[800]\tvalid_0's binary_logloss: 0.686557\n",
      "[1000]\tvalid_0's binary_logloss: 0.686321\n",
      "Early stopping, best iteration is:\n",
      "[1029]\tvalid_0's binary_logloss: 0.686247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.685437:   0%|          | 0/5 [00:12<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.685437:  20%|##        | 1/5 [00:12<00:50, 12.72s/it][I 2020-09-17 04:41:59,571] Finished trial#60 with value: 0.6862468762248398 with parameters: {'min_child_samples': 50}. Best is trial#60 with value: 0.6862468762248398.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.685437:  20%|##        | 1/5 [00:12<00:50, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688902\n",
      "[400]\tvalid_0's binary_logloss: 0.687795\n",
      "[600]\tvalid_0's binary_logloss: 0.68724\n",
      "[800]\tvalid_0's binary_logloss: 0.687076\n",
      "[1000]\tvalid_0's binary_logloss: 0.687012\n",
      "Early stopping, best iteration is:\n",
      "[943]\tvalid_0's binary_logloss: 0.686971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.685437:  20%|##        | 1/5 [00:23<00:50, 12.72s/it]\n",
      "min_data_in_leaf, val_score: 0.685437:  40%|####      | 2/5 [00:23<00:36, 12.26s/it][I 2020-09-17 04:42:10,752] Finished trial#61 with value: 0.6869708531542443 with parameters: {'min_child_samples': 100}. Best is trial#60 with value: 0.6862468762248398.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.685437:  40%|####      | 2/5 [00:23<00:36, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688367\n",
      "[400]\tvalid_0's binary_logloss: 0.687372\n",
      "[600]\tvalid_0's binary_logloss: 0.686952\n",
      "[800]\tvalid_0's binary_logloss: 0.686317\n",
      "[1000]\tvalid_0's binary_logloss: 0.68598\n",
      "Early stopping, best iteration is:\n",
      "[943]\tvalid_0's binary_logloss: 0.685905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.685437:  40%|####      | 2/5 [00:37<00:36, 12.26s/it]\n",
      "min_data_in_leaf, val_score: 0.685437:  60%|######    | 3/5 [00:37<00:25, 12.63s/it][I 2020-09-17 04:42:24,256] Finished trial#62 with value: 0.6859053416372233 with parameters: {'min_child_samples': 10}. Best is trial#62 with value: 0.6859053416372233.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.685437:  60%|######    | 3/5 [00:37<00:25, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688463\n",
      "[400]\tvalid_0's binary_logloss: 0.68703\n",
      "[600]\tvalid_0's binary_logloss: 0.686449\n",
      "[800]\tvalid_0's binary_logloss: 0.685835\n",
      "[1000]\tvalid_0's binary_logloss: 0.685472\n",
      "Early stopping, best iteration is:\n",
      "[1083]\tvalid_0's binary_logloss: 0.685328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.685328:  60%|######    | 3/5 [00:50<00:25, 12.63s/it]\n",
      "min_data_in_leaf, val_score: 0.685328:  80%|########  | 4/5 [00:50<00:12, 12.88s/it][I 2020-09-17 04:42:37,709] Finished trial#63 with value: 0.6853282133968825 with parameters: {'min_child_samples': 5}. Best is trial#63 with value: 0.6853282133968825.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.685328:  80%|########  | 4/5 [00:50<00:12, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688531\n",
      "[400]\tvalid_0's binary_logloss: 0.687157\n",
      "[600]\tvalid_0's binary_logloss: 0.686713\n",
      "[800]\tvalid_0's binary_logloss: 0.686058\n",
      "[1000]\tvalid_0's binary_logloss: 0.686155\n",
      "Early stopping, best iteration is:\n",
      "[939]\tvalid_0's binary_logloss: 0.685837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.685328:  80%|########  | 4/5 [01:04<00:12, 12.88s/it]\n",
      "min_data_in_leaf, val_score: 0.685328: 100%|##########| 5/5 [01:04<00:00, 13.02s/it][I 2020-09-17 04:42:51,043] Finished trial#64 with value: 0.6858374390035811 with parameters: {'min_child_samples': 25}. Best is trial#63 with value: 0.6853282133968825.\n",
      "/home/tubotu/.local/lib/python3.6/site-packages/optuna/_experimental.py:90: ExperimentalWarning: train is experimental (supported from v0.18.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687231\n",
      "[400]\tvalid_0's binary_logloss: 0.685384\n",
      "[600]\tvalid_0's binary_logloss: 0.6844\n",
      "Early stopping, best iteration is:\n",
      "[623]\tvalid_0's binary_logloss: 0.684202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.684202:   0%|          | 0/7 [00:10<?, ?it/s]\n",
      "feature_fraction, val_score: 0.684202:  14%|#4        | 1/7 [00:10<01:05, 10.92s/it][I 2020-09-17 04:43:02,059] Finished trial#0 with value: 0.6842021969836948 with parameters: {'feature_fraction': 0.7}. Best is trial#0 with value: 0.6842021969836948.\n",
      "\n",
      "feature_fraction, val_score: 0.684202:  14%|#4        | 1/7 [00:10<01:05, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68813\n",
      "[400]\tvalid_0's binary_logloss: 0.68636\n",
      "[600]\tvalid_0's binary_logloss: 0.684999\n",
      "[800]\tvalid_0's binary_logloss: 0.684218\n",
      "[1000]\tvalid_0's binary_logloss: 0.684057\n",
      "Early stopping, best iteration is:\n",
      "[947]\tvalid_0's binary_logloss: 0.683817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.683817:  14%|#4        | 1/7 [00:23<01:05, 10.92s/it]\n",
      "feature_fraction, val_score: 0.683817:  29%|##8       | 2/7 [00:23<00:57, 11.40s/it][I 2020-09-17 04:43:14,591] Finished trial#1 with value: 0.6838166261421623 with parameters: {'feature_fraction': 0.5}. Best is trial#1 with value: 0.6838166261421623.\n",
      "\n",
      "feature_fraction, val_score: 0.683817:  29%|##8       | 2/7 [00:23<00:57, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687403\n",
      "[400]\tvalid_0's binary_logloss: 0.686075\n",
      "[600]\tvalid_0's binary_logloss: 0.685543\n",
      "[800]\tvalid_0's binary_logloss: 0.685083\n",
      "[1000]\tvalid_0's binary_logloss: 0.684161\n",
      "Early stopping, best iteration is:\n",
      "[946]\tvalid_0's binary_logloss: 0.684035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.683817:  29%|##8       | 2/7 [00:35<00:57, 11.40s/it]\n",
      "feature_fraction, val_score: 0.683817:  43%|####2     | 3/7 [00:35<00:46, 11.68s/it][I 2020-09-17 04:43:26,908] Finished trial#2 with value: 0.6840348914403639 with parameters: {'feature_fraction': 0.8999999999999999}. Best is trial#1 with value: 0.6838166261421623.\n",
      "\n",
      "feature_fraction, val_score: 0.683817:  43%|####2     | 3/7 [00:35<00:46, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687472\n",
      "[400]\tvalid_0's binary_logloss: 0.685661\n",
      "[600]\tvalid_0's binary_logloss: 0.684412\n",
      "[800]\tvalid_0's binary_logloss: 0.684061\n",
      "[1000]\tvalid_0's binary_logloss: 0.683957\n",
      "Early stopping, best iteration is:\n",
      "[947]\tvalid_0's binary_logloss: 0.683821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.683817:  43%|####2     | 3/7 [00:48<00:46, 11.68s/it]\n",
      "feature_fraction, val_score: 0.683817:  57%|#####7    | 4/7 [00:48<00:35, 11.93s/it][I 2020-09-17 04:43:39,418] Finished trial#3 with value: 0.6838214588388286 with parameters: {'feature_fraction': 0.6}. Best is trial#1 with value: 0.6838166261421623.\n",
      "\n",
      "feature_fraction, val_score: 0.683817:  57%|#####7    | 4/7 [00:48<00:35, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687278\n",
      "[400]\tvalid_0's binary_logloss: 0.685878\n",
      "[600]\tvalid_0's binary_logloss: 0.685021\n",
      "[800]\tvalid_0's binary_logloss: 0.684784\n",
      "[1000]\tvalid_0's binary_logloss: 0.684042\n",
      "Early stopping, best iteration is:\n",
      "[1002]\tvalid_0's binary_logloss: 0.68401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.683817:  57%|#####7    | 4/7 [01:01<00:35, 11.93s/it]\n",
      "feature_fraction, val_score: 0.683817:  71%|#######1  | 5/7 [01:01<00:24, 12.28s/it][I 2020-09-17 04:43:52,542] Finished trial#4 with value: 0.6840097601875624 with parameters: {'feature_fraction': 0.8}. Best is trial#1 with value: 0.6838166261421623.\n",
      "\n",
      "feature_fraction, val_score: 0.683817:  71%|#######1  | 5/7 [01:01<00:24, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687473\n",
      "[400]\tvalid_0's binary_logloss: 0.686315\n",
      "[600]\tvalid_0's binary_logloss: 0.685647\n",
      "Early stopping, best iteration is:\n",
      "[545]\tvalid_0's binary_logloss: 0.685552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.683817:  71%|#######1  | 5/7 [01:12<00:24, 12.28s/it]\n",
      "feature_fraction, val_score: 0.683817:  86%|########5 | 6/7 [01:12<00:11, 11.98s/it][I 2020-09-17 04:44:03,822] Finished trial#5 with value: 0.6855516480764356 with parameters: {'feature_fraction': 1.0}. Best is trial#1 with value: 0.6838166261421623.\n",
      "\n",
      "feature_fraction, val_score: 0.683817:  86%|########5 | 6/7 [01:12<00:11, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68797\n",
      "[400]\tvalid_0's binary_logloss: 0.685851\n",
      "[600]\tvalid_0's binary_logloss: 0.684551\n",
      "[800]\tvalid_0's binary_logloss: 0.683414\n",
      "[1000]\tvalid_0's binary_logloss: 0.682819\n",
      "Early stopping, best iteration is:\n",
      "[1024]\tvalid_0's binary_logloss: 0.682711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.682711:  86%|########5 | 6/7 [01:22<00:11, 11.98s/it]\n",
      "feature_fraction, val_score: 0.682711: 100%|##########| 7/7 [01:22<00:00, 11.48s/it][I 2020-09-17 04:44:14,109] Finished trial#6 with value: 0.6827112035533208 with parameters: {'feature_fraction': 0.4}. Best is trial#6 with value: 0.6827112035533208.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "num_leaves, val_score: 0.682711:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682711:   0%|          | 0/20 [00:15<?, ?it/s]\n",
      "num_leaves, val_score: 0.682711:   5%|5         | 1/20 [00:15<05:02, 15.92s/it][I 2020-09-17 04:44:30,096] Finished trial#7 with value: 0.6830801512385353 with parameters: {'num_leaves': 99}. Best is trial#7 with value: 0.6830801512385353.\n",
      "\n",
      "num_leaves, val_score: 0.682711:   5%|5         | 1/20 [00:15<05:02, 15.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687964\n",
      "[400]\tvalid_0's binary_logloss: 0.685858\n",
      "[600]\tvalid_0's binary_logloss: 0.684531\n",
      "[800]\tvalid_0's binary_logloss: 0.683401\n",
      "[1000]\tvalid_0's binary_logloss: 0.682849\n",
      "Early stopping, best iteration is:\n",
      "[1037]\tvalid_0's binary_logloss: 0.682731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682711:   5%|5         | 1/20 [00:28<05:02, 15.92s/it]\n",
      "num_leaves, val_score: 0.682711:  10%|#         | 2/20 [00:28<04:27, 14.88s/it][I 2020-09-17 04:44:42,524] Finished trial#8 with value: 0.6827314176876736 with parameters: {'num_leaves': 30}. Best is trial#8 with value: 0.6827314176876736.\n",
      "\n",
      "num_leaves, val_score: 0.682711:  10%|#         | 2/20 [00:28<04:27, 14.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687821\n",
      "[400]\tvalid_0's binary_logloss: 0.685708\n",
      "[600]\tvalid_0's binary_logloss: 0.684434\n",
      "[800]\tvalid_0's binary_logloss: 0.683267\n",
      "[1000]\tvalid_0's binary_logloss: 0.682677\n",
      "Early stopping, best iteration is:\n",
      "[1024]\tvalid_0's binary_logloss: 0.682558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  10%|#         | 2/20 [00:40<04:27, 14.88s/it]\n",
      "num_leaves, val_score: 0.682558:  15%|#5        | 3/20 [00:40<04:01, 14.19s/it][I 2020-09-17 04:44:55,114] Finished trial#9 with value: 0.6825583666040669 with parameters: {'num_leaves': 35}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  15%|#5        | 3/20 [00:41<04:01, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  15%|#5        | 3/20 [00:55<04:01, 14.19s/it]\n",
      "num_leaves, val_score: 0.682558:  20%|##        | 4/20 [00:55<03:50, 14.43s/it][I 2020-09-17 04:45:10,104] Finished trial#10 with value: 0.6830801512385352 with parameters: {'num_leaves': 226}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  20%|##        | 4/20 [00:55<03:50, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  20%|##        | 4/20 [01:18<03:50, 14.43s/it]\n",
      "num_leaves, val_score: 0.682558:  25%|##5       | 5/20 [01:18<04:13, 16.90s/it][I 2020-09-17 04:45:32,765] Finished trial#11 with value: 0.6830801512385353 with parameters: {'num_leaves': 163}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  25%|##5       | 5/20 [01:18<04:13, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68754\n",
      "[400]\tvalid_0's binary_logloss: 0.685275\n",
      "[600]\tvalid_0's binary_logloss: 0.683998\n",
      "[800]\tvalid_0's binary_logloss: 0.683219\n",
      "[1000]\tvalid_0's binary_logloss: 0.682723\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  25%|##5       | 5/20 [01:32<04:13, 16.90s/it]\n",
      "num_leaves, val_score: 0.682558:  30%|###       | 6/20 [01:32<03:42, 15.88s/it][I 2020-09-17 04:45:46,261] Finished trial#12 with value: 0.6825902720266499 with parameters: {'num_leaves': 59}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  30%|###       | 6/20 [01:32<03:42, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  30%|###       | 6/20 [01:47<03:42, 15.88s/it]\n",
      "num_leaves, val_score: 0.682558:  35%|###5      | 7/20 [01:47<03:23, 15.68s/it][I 2020-09-17 04:46:01,487] Finished trial#13 with value: 0.6830801512385353 with parameters: {'num_leaves': 184}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  35%|###5      | 7/20 [01:47<03:23, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  35%|###5      | 7/20 [02:10<03:23, 15.68s/it]\n",
      "num_leaves, val_score: 0.682558:  40%|####      | 8/20 [02:10<03:34, 17.90s/it][I 2020-09-17 04:46:24,558] Finished trial#14 with value: 0.6830801512385352 with parameters: {'num_leaves': 116}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  40%|####      | 8/20 [02:10<03:34, 17.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  40%|####      | 8/20 [02:24<03:34, 17.90s/it]\n",
      "num_leaves, val_score: 0.682558:  45%|####5     | 9/20 [02:24<03:05, 16.89s/it][I 2020-09-17 04:46:39,106] Finished trial#15 with value: 0.6830801512385352 with parameters: {'num_leaves': 181}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  45%|####5     | 9/20 [02:24<03:05, 16.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687581\n",
      "[400]\tvalid_0's binary_logloss: 0.68537\n",
      "[600]\tvalid_0's binary_logloss: 0.684162\n",
      "[800]\tvalid_0's binary_logloss: 0.68325\n",
      "[1000]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[1043]\tvalid_0's binary_logloss: 0.68278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682558:  45%|####5     | 9/20 [02:38<03:05, 16.89s/it]\n",
      "num_leaves, val_score: 0.682558:  50%|#####     | 10/20 [02:38<02:39, 15.99s/it][I 2020-09-17 04:46:52,995] Finished trial#16 with value: 0.6827804547999765 with parameters: {'num_leaves': 58}. Best is trial#9 with value: 0.6825583666040669.\n",
      "\n",
      "num_leaves, val_score: 0.682558:  50%|#####     | 10/20 [02:38<02:39, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688751\n",
      "[400]\tvalid_0's binary_logloss: 0.687017\n",
      "[600]\tvalid_0's binary_logloss: 0.685563\n",
      "[800]\tvalid_0's binary_logloss: 0.684447\n",
      "[1000]\tvalid_0's binary_logloss: 0.683665\n",
      "[1200]\tvalid_0's binary_logloss: 0.683268\n",
      "[1400]\tvalid_0's binary_logloss: 0.682727\n",
      "[1600]\tvalid_0's binary_logloss: 0.682416\n",
      "[1800]\tvalid_0's binary_logloss: 0.682239\n",
      "Early stopping, best iteration is:\n",
      "[1776]\tvalid_0's binary_logloss: 0.682225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682225:  50%|#####     | 10/20 [02:50<02:39, 15.99s/it]\n",
      "num_leaves, val_score: 0.682225:  55%|#####5    | 11/20 [02:50<02:11, 14.58s/it][I 2020-09-17 04:47:04,287] Finished trial#17 with value: 0.6822245202641402 with parameters: {'num_leaves': 3}. Best is trial#17 with value: 0.6822245202641402.\n",
      "\n",
      "num_leaves, val_score: 0.682225:  55%|#####5    | 11/20 [02:50<02:11, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688424\n",
      "[400]\tvalid_0's binary_logloss: 0.686662\n",
      "[600]\tvalid_0's binary_logloss: 0.685238\n",
      "[800]\tvalid_0's binary_logloss: 0.684213\n",
      "[1000]\tvalid_0's binary_logloss: 0.683551\n",
      "[1200]\tvalid_0's binary_logloss: 0.683424\n",
      "[1400]\tvalid_0's binary_logloss: 0.683185\n",
      "[1600]\tvalid_0's binary_logloss: 0.682906\n",
      "[1800]\tvalid_0's binary_logloss: 0.682778\n",
      "Early stopping, best iteration is:\n",
      "[1775]\tvalid_0's binary_logloss: 0.682745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682225:  55%|#####5    | 11/20 [03:00<02:11, 14.58s/it]\n",
      "num_leaves, val_score: 0.682225:  60%|######    | 12/20 [03:00<01:46, 13.26s/it][I 2020-09-17 04:47:14,466] Finished trial#18 with value: 0.682744897289478 with parameters: {'num_leaves': 5}. Best is trial#17 with value: 0.6822245202641402.\n",
      "\n",
      "num_leaves, val_score: 0.682225:  60%|######    | 12/20 [03:00<01:46, 13.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688487\n",
      "[400]\tvalid_0's binary_logloss: 0.686685\n",
      "[600]\tvalid_0's binary_logloss: 0.685235\n",
      "[800]\tvalid_0's binary_logloss: 0.684089\n",
      "[1000]\tvalid_0's binary_logloss: 0.683354\n",
      "[1200]\tvalid_0's binary_logloss: 0.683051\n",
      "[1400]\tvalid_0's binary_logloss: 0.682605\n",
      "[1600]\tvalid_0's binary_logloss: 0.682293\n",
      "[1800]\tvalid_0's binary_logloss: 0.682195\n",
      "Early stopping, best iteration is:\n",
      "[1888]\tvalid_0's binary_logloss: 0.682164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  60%|######    | 12/20 [03:01<01:46, 13.26s/it]\n",
      "num_leaves, val_score: 0.682164:  65%|######5   | 13/20 [03:01<01:06,  9.53s/it][I 2020-09-17 04:47:15,284] Finished trial#19 with value: 0.682164201477549 with parameters: {'num_leaves': 4}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "num_leaves, val_score: 0.682164:  65%|######5   | 13/20 [03:01<01:06,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688404\n",
      "[400]\tvalid_0's binary_logloss: 0.686618\n",
      "[600]\tvalid_0's binary_logloss: 0.685279\n",
      "[800]\tvalid_0's binary_logloss: 0.684273\n",
      "[1000]\tvalid_0's binary_logloss: 0.683687\n",
      "[1200]\tvalid_0's binary_logloss: 0.683551\n",
      "Early stopping, best iteration is:\n",
      "[1254]\tvalid_0's binary_logloss: 0.683459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  65%|######5   | 13/20 [03:08<01:06,  9.53s/it]\n",
      "num_leaves, val_score: 0.682164:  70%|#######   | 14/20 [03:08<00:53,  8.95s/it][I 2020-09-17 04:47:22,877] Finished trial#20 with value: 0.6834589893693377 with parameters: {'num_leaves': 6}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "num_leaves, val_score: 0.682164:  70%|#######   | 14/20 [03:08<00:53,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687703\n",
      "[400]\tvalid_0's binary_logloss: 0.685425\n",
      "[600]\tvalid_0's binary_logloss: 0.684448\n",
      "[800]\tvalid_0's binary_logloss: 0.683537\n",
      "[1000]\tvalid_0's binary_logloss: 0.683254\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.683118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  70%|#######   | 14/20 [03:26<00:53,  8.95s/it]\n",
      "num_leaves, val_score: 0.682164:  75%|#######5  | 15/20 [03:26<00:58, 11.64s/it][I 2020-09-17 04:47:40,806] Finished trial#21 with value: 0.6831179985652861 with parameters: {'num_leaves': 81}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "num_leaves, val_score: 0.682164:  75%|#######5  | 15/20 [03:26<00:58, 11.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688487\n",
      "[400]\tvalid_0's binary_logloss: 0.686685\n",
      "[600]\tvalid_0's binary_logloss: 0.685235\n",
      "[800]\tvalid_0's binary_logloss: 0.684089\n",
      "[1000]\tvalid_0's binary_logloss: 0.683354\n",
      "[1200]\tvalid_0's binary_logloss: 0.683051\n",
      "[1400]\tvalid_0's binary_logloss: 0.682605\n",
      "[1600]\tvalid_0's binary_logloss: 0.682293\n",
      "[1800]\tvalid_0's binary_logloss: 0.682195\n",
      "Early stopping, best iteration is:\n",
      "[1888]\tvalid_0's binary_logloss: 0.682164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  75%|#######5  | 15/20 [03:38<00:58, 11.64s/it]\n",
      "num_leaves, val_score: 0.682164:  80%|########  | 16/20 [03:38<00:47, 11.82s/it][I 2020-09-17 04:47:53,040] Finished trial#22 with value: 0.682164201477549 with parameters: {'num_leaves': 4}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "num_leaves, val_score: 0.682164:  80%|########  | 16/20 [03:38<00:47, 11.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  80%|########  | 16/20 [03:54<00:47, 11.82s/it]\n",
      "num_leaves, val_score: 0.682164:  85%|########5 | 17/20 [03:54<00:38, 12.90s/it][I 2020-09-17 04:48:08,470] Finished trial#23 with value: 0.6830801512385352 with parameters: {'num_leaves': 252}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "num_leaves, val_score: 0.682164:  85%|########5 | 17/20 [03:54<00:38, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687779\n",
      "[400]\tvalid_0's binary_logloss: 0.685739\n",
      "[600]\tvalid_0's binary_logloss: 0.684536\n",
      "[800]\tvalid_0's binary_logloss: 0.683452\n",
      "[1000]\tvalid_0's binary_logloss: 0.682825\n",
      "Early stopping, best iteration is:\n",
      "[1024]\tvalid_0's binary_logloss: 0.682695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  85%|########5 | 17/20 [04:06<00:38, 12.90s/it]\n",
      "num_leaves, val_score: 0.682164:  90%|######### | 18/20 [04:06<00:25, 12.76s/it][I 2020-09-17 04:48:20,879] Finished trial#24 with value: 0.6826947345275894 with parameters: {'num_leaves': 36}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "num_leaves, val_score: 0.682164:  90%|######### | 18/20 [04:06<00:25, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687705\n",
      "[400]\tvalid_0's binary_logloss: 0.685422\n",
      "[600]\tvalid_0's binary_logloss: 0.684437\n",
      "[800]\tvalid_0's binary_logloss: 0.683534\n",
      "[1000]\tvalid_0's binary_logloss: 0.683227\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  90%|######### | 18/20 [04:29<00:25, 12.76s/it]\n",
      "num_leaves, val_score: 0.682164:  95%|#########5| 19/20 [04:29<00:15, 15.74s/it][I 2020-09-17 04:48:43,578] Finished trial#25 with value: 0.6830801512385352 with parameters: {'num_leaves': 128}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "num_leaves, val_score: 0.682164:  95%|#########5| 19/20 [04:29<00:15, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687584\n",
      "[400]\tvalid_0's binary_logloss: 0.685336\n",
      "[600]\tvalid_0's binary_logloss: 0.684187\n",
      "[800]\tvalid_0's binary_logloss: 0.683132\n",
      "[1000]\tvalid_0's binary_logloss: 0.682863\n",
      "Early stopping, best iteration is:\n",
      "[968]\tvalid_0's binary_logloss: 0.68271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.682164:  95%|#########5| 19/20 [04:43<00:15, 15.74s/it]\n",
      "num_leaves, val_score: 0.682164: 100%|##########| 20/20 [04:43<00:00, 15.17s/it][I 2020-09-17 04:48:57,432] Finished trial#26 with value: 0.6827103150057645 with parameters: {'num_leaves': 67}. Best is trial#19 with value: 0.682164201477549.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "bagging, val_score: 0.682164:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688742\n",
      "[400]\tvalid_0's binary_logloss: 0.68718\n",
      "[600]\tvalid_0's binary_logloss: 0.686093\n",
      "[800]\tvalid_0's binary_logloss: 0.684987\n",
      "[1000]\tvalid_0's binary_logloss: 0.684248\n",
      "[1200]\tvalid_0's binary_logloss: 0.683759\n",
      "[1400]\tvalid_0's binary_logloss: 0.683306\n",
      "[1600]\tvalid_0's binary_logloss: 0.683009\n",
      "Early stopping, best iteration is:\n",
      "[1511]\tvalid_0's binary_logloss: 0.6829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682164:   0%|          | 0/10 [00:11<?, ?it/s]\n",
      "bagging, val_score: 0.682164:  10%|#         | 1/10 [00:11<01:45, 11.75s/it][I 2020-09-17 04:49:09,244] Finished trial#27 with value: 0.6828998004740806 with parameters: {'bagging_fraction': 0.5492277770381448, 'bagging_freq': 7}. Best is trial#27 with value: 0.6828998004740806.\n",
      "\n",
      "bagging, val_score: 0.682164:  10%|#         | 1/10 [00:11<01:45, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688832\n",
      "[400]\tvalid_0's binary_logloss: 0.687328\n",
      "[600]\tvalid_0's binary_logloss: 0.686206\n",
      "[800]\tvalid_0's binary_logloss: 0.685249\n",
      "[1000]\tvalid_0's binary_logloss: 0.684731\n",
      "[1200]\tvalid_0's binary_logloss: 0.684333\n",
      "[1400]\tvalid_0's binary_logloss: 0.684028\n",
      "[1600]\tvalid_0's binary_logloss: 0.683599\n",
      "[1800]\tvalid_0's binary_logloss: 0.683349\n",
      "Early stopping, best iteration is:\n",
      "[1855]\tvalid_0's binary_logloss: 0.683317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682164:  10%|#         | 1/10 [00:17<01:45, 11.75s/it]\n",
      "bagging, val_score: 0.682164:  20%|##        | 2/10 [00:17<01:20, 10.07s/it][I 2020-09-17 04:49:15,404] Finished trial#28 with value: 0.6833165222577409 with parameters: {'bagging_fraction': 0.5163430003482066, 'bagging_freq': 4}. Best is trial#27 with value: 0.6828998004740806.\n",
      "\n",
      "bagging, val_score: 0.682164:  20%|##        | 2/10 [00:17<01:20, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688834\n",
      "[400]\tvalid_0's binary_logloss: 0.687281\n",
      "[600]\tvalid_0's binary_logloss: 0.686088\n",
      "[800]\tvalid_0's binary_logloss: 0.685231\n",
      "[1000]\tvalid_0's binary_logloss: 0.684595\n",
      "[1200]\tvalid_0's binary_logloss: 0.684251\n",
      "[1400]\tvalid_0's binary_logloss: 0.683886\n",
      "[1600]\tvalid_0's binary_logloss: 0.683652\n",
      "Early stopping, best iteration is:\n",
      "[1536]\tvalid_0's binary_logloss: 0.683605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682164:  20%|##        | 2/10 [00:24<01:20, 10.07s/it]\n",
      "bagging, val_score: 0.682164:  30%|###       | 3/10 [00:24<01:03,  9.14s/it][I 2020-09-17 04:49:22,370] Finished trial#29 with value: 0.6836046658545692 with parameters: {'bagging_fraction': 0.5717463916858574, 'bagging_freq': 3}. Best is trial#27 with value: 0.6828998004740806.\n",
      "\n",
      "bagging, val_score: 0.682164:  30%|###       | 3/10 [00:24<01:03,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688572\n",
      "[400]\tvalid_0's binary_logloss: 0.686899\n",
      "[600]\tvalid_0's binary_logloss: 0.685627\n",
      "[800]\tvalid_0's binary_logloss: 0.684582\n",
      "[1000]\tvalid_0's binary_logloss: 0.683757\n",
      "[1200]\tvalid_0's binary_logloss: 0.683282\n",
      "[1400]\tvalid_0's binary_logloss: 0.682881\n",
      "[1600]\tvalid_0's binary_logloss: 0.68261\n",
      "[1800]\tvalid_0's binary_logloss: 0.682429\n",
      "[2000]\tvalid_0's binary_logloss: 0.682289\n",
      "[2200]\tvalid_0's binary_logloss: 0.682161\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682136:  30%|###       | 3/10 [00:38<01:03,  9.14s/it]\n",
      "bagging, val_score: 0.682136:  40%|####      | 4/10 [00:38<01:02, 10.37s/it][I 2020-09-17 04:49:35,602] Finished trial#30 with value: 0.6821358671912241 with parameters: {'bagging_fraction': 0.9566738226288709, 'bagging_freq': 7}. Best is trial#30 with value: 0.6821358671912241.\n",
      "\n",
      "bagging, val_score: 0.682136:  40%|####      | 4/10 [00:38<01:02, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688913\n",
      "[400]\tvalid_0's binary_logloss: 0.687352\n",
      "[600]\tvalid_0's binary_logloss: 0.686282\n",
      "[800]\tvalid_0's binary_logloss: 0.685482\n",
      "[1000]\tvalid_0's binary_logloss: 0.68482\n",
      "[1200]\tvalid_0's binary_logloss: 0.684385\n",
      "[1400]\tvalid_0's binary_logloss: 0.684085\n",
      "[1600]\tvalid_0's binary_logloss: 0.683582\n",
      "[1800]\tvalid_0's binary_logloss: 0.683182\n",
      "[2000]\tvalid_0's binary_logloss: 0.683283\n",
      "Early stopping, best iteration is:\n",
      "[1932]\tvalid_0's binary_logloss: 0.683105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682136:  40%|####      | 4/10 [00:50<01:02, 10.37s/it]\n",
      "bagging, val_score: 0.682136:  50%|#####     | 5/10 [00:50<00:55, 11.04s/it][I 2020-09-17 04:49:48,219] Finished trial#31 with value: 0.6831054164924535 with parameters: {'bagging_fraction': 0.4677743588395532, 'bagging_freq': 2}. Best is trial#30 with value: 0.6821358671912241.\n",
      "\n",
      "bagging, val_score: 0.682136:  50%|#####     | 5/10 [00:50<00:55, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688497\n",
      "[400]\tvalid_0's binary_logloss: 0.686811\n",
      "[600]\tvalid_0's binary_logloss: 0.685478\n",
      "[800]\tvalid_0's binary_logloss: 0.684434\n",
      "[1000]\tvalid_0's binary_logloss: 0.683622\n",
      "[1200]\tvalid_0's binary_logloss: 0.683187\n",
      "[1400]\tvalid_0's binary_logloss: 0.682697\n",
      "[1600]\tvalid_0's binary_logloss: 0.682532\n",
      "[1800]\tvalid_0's binary_logloss: 0.682396\n",
      "[2000]\tvalid_0's binary_logloss: 0.682335\n",
      "[2200]\tvalid_0's binary_logloss: 0.682278\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682136:  50%|#####     | 5/10 [01:25<00:55, 11.04s/it]\n",
      "bagging, val_score: 0.682136:  60%|######    | 6/10 [01:25<01:12, 18.11s/it][I 2020-09-17 04:50:22,821] Finished trial#32 with value: 0.6821861784054917 with parameters: {'bagging_fraction': 0.9624285950604305, 'bagging_freq': 7}. Best is trial#30 with value: 0.6821358671912241.\n",
      "\n",
      "bagging, val_score: 0.682136:  60%|######    | 6/10 [01:25<01:12, 18.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688827\n",
      "[400]\tvalid_0's binary_logloss: 0.687343\n",
      "[600]\tvalid_0's binary_logloss: 0.686306\n",
      "[800]\tvalid_0's binary_logloss: 0.685244\n",
      "[1000]\tvalid_0's binary_logloss: 0.684525\n",
      "[1200]\tvalid_0's binary_logloss: 0.68408\n",
      "[1400]\tvalid_0's binary_logloss: 0.683699\n",
      "[1600]\tvalid_0's binary_logloss: 0.683378\n",
      "[1800]\tvalid_0's binary_logloss: 0.683244\n",
      "Early stopping, best iteration is:\n",
      "[1886]\tvalid_0's binary_logloss: 0.683118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682136:  60%|######    | 6/10 [01:46<01:12, 18.11s/it]\n",
      "bagging, val_score: 0.682136:  70%|#######   | 7/10 [01:46<00:57, 19.08s/it][I 2020-09-17 04:50:44,172] Finished trial#33 with value: 0.6831179547700935 with parameters: {'bagging_fraction': 0.5040385377743214, 'bagging_freq': 3}. Best is trial#30 with value: 0.6821358671912241.\n",
      "\n",
      "bagging, val_score: 0.682136:  70%|#######   | 7/10 [01:46<00:57, 19.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688673\n",
      "[400]\tvalid_0's binary_logloss: 0.686892\n",
      "[600]\tvalid_0's binary_logloss: 0.685595\n",
      "[800]\tvalid_0's binary_logloss: 0.684592\n",
      "[1000]\tvalid_0's binary_logloss: 0.683867\n",
      "[1200]\tvalid_0's binary_logloss: 0.683403\n",
      "[1400]\tvalid_0's binary_logloss: 0.682818\n",
      "[1600]\tvalid_0's binary_logloss: 0.682724\n",
      "[1800]\tvalid_0's binary_logloss: 0.682604\n",
      "[2000]\tvalid_0's binary_logloss: 0.682519\n",
      "Early stopping, best iteration is:\n",
      "[1968]\tvalid_0's binary_logloss: 0.68247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682136:  70%|#######   | 7/10 [02:00<00:57, 19.08s/it]\n",
      "bagging, val_score: 0.682136:  80%|########  | 8/10 [02:00<00:34, 17.39s/it][I 2020-09-17 04:50:57,626] Finished trial#34 with value: 0.6824697964905346 with parameters: {'bagging_fraction': 0.9004273068614347, 'bagging_freq': 6}. Best is trial#30 with value: 0.6821358671912241.\n",
      "\n",
      "bagging, val_score: 0.682136:  80%|########  | 8/10 [02:00<00:34, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68872\n",
      "[400]\tvalid_0's binary_logloss: 0.68706\n",
      "[600]\tvalid_0's binary_logloss: 0.686147\n",
      "[800]\tvalid_0's binary_logloss: 0.684999\n",
      "[1000]\tvalid_0's binary_logloss: 0.684401\n",
      "[1200]\tvalid_0's binary_logloss: 0.684132\n",
      "[1400]\tvalid_0's binary_logloss: 0.683781\n",
      "[1600]\tvalid_0's binary_logloss: 0.683559\n",
      "Early stopping, best iteration is:\n",
      "[1504]\tvalid_0's binary_logloss: 0.683448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682136:  80%|########  | 8/10 [02:22<00:34, 17.39s/it]\n",
      "bagging, val_score: 0.682136:  90%|######### | 9/10 [02:22<00:19, 19.03s/it][I 2020-09-17 04:51:20,472] Finished trial#35 with value: 0.6834482997134096 with parameters: {'bagging_fraction': 0.5431518303601914, 'bagging_freq': 7}. Best is trial#30 with value: 0.6821358671912241.\n",
      "\n",
      "bagging, val_score: 0.682136:  90%|######### | 9/10 [02:23<00:19, 19.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688855\n",
      "[400]\tvalid_0's binary_logloss: 0.687142\n",
      "[600]\tvalid_0's binary_logloss: 0.686047\n",
      "[800]\tvalid_0's binary_logloss: 0.684977\n",
      "[1000]\tvalid_0's binary_logloss: 0.684324\n",
      "[1200]\tvalid_0's binary_logloss: 0.683943\n",
      "[1400]\tvalid_0's binary_logloss: 0.683619\n",
      "[1600]\tvalid_0's binary_logloss: 0.683374\n",
      "[1800]\tvalid_0's binary_logloss: 0.68312\n",
      "[2000]\tvalid_0's binary_logloss: 0.683057\n",
      "Early stopping, best iteration is:\n",
      "[2058]\tvalid_0's binary_logloss: 0.682992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.682136:  90%|######### | 9/10 [02:35<00:19, 19.03s/it]\n",
      "bagging, val_score: 0.682136: 100%|##########| 10/10 [02:35<00:00, 17.05s/it][I 2020-09-17 04:51:32,913] Finished trial#36 with value: 0.6829918732713373 with parameters: {'bagging_fraction': 0.671295408320371, 'bagging_freq': 1}. Best is trial#30 with value: 0.6821358671912241.\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.682136:   0%|          | 0/3 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688387\n",
      "[400]\tvalid_0's binary_logloss: 0.686663\n",
      "[600]\tvalid_0's binary_logloss: 0.685359\n",
      "[800]\tvalid_0's binary_logloss: 0.684374\n",
      "[1000]\tvalid_0's binary_logloss: 0.683631\n",
      "[1200]\tvalid_0's binary_logloss: 0.683165\n",
      "[1400]\tvalid_0's binary_logloss: 0.682791\n",
      "[1600]\tvalid_0's binary_logloss: 0.682525\n",
      "[1800]\tvalid_0's binary_logloss: 0.682375\n",
      "[2000]\tvalid_0's binary_logloss: 0.682309\n",
      "Early stopping, best iteration is:\n",
      "[1990]\tvalid_0's binary_logloss: 0.682291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.682136:   0%|          | 0/3 [00:12<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.682136:  33%|###3      | 1/3 [00:12<00:25, 12.52s/it][I 2020-09-17 04:51:45,504] Finished trial#37 with value: 0.6822908375647464 with parameters: {'feature_fraction': 0.44800000000000006}. Best is trial#37 with value: 0.6822908375647464.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.682136:  33%|###3      | 1/3 [00:12<00:25, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688258\n",
      "[400]\tvalid_0's binary_logloss: 0.686505\n",
      "[600]\tvalid_0's binary_logloss: 0.685122\n",
      "[800]\tvalid_0's binary_logloss: 0.684221\n",
      "[1000]\tvalid_0's binary_logloss: 0.683529\n",
      "[1200]\tvalid_0's binary_logloss: 0.683005\n",
      "[1400]\tvalid_0's binary_logloss: 0.682699\n",
      "[1600]\tvalid_0's binary_logloss: 0.682417\n",
      "[1800]\tvalid_0's binary_logloss: 0.682285\n",
      "Early stopping, best iteration is:\n",
      "[1809]\tvalid_0's binary_logloss: 0.68226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.682136:  33%|###3      | 1/3 [00:21<00:25, 12.52s/it]\n",
      "feature_fraction_stage2, val_score: 0.682136:  67%|######6   | 2/3 [00:21<00:11, 11.31s/it][I 2020-09-17 04:51:53,983] Finished trial#38 with value: 0.6822596942865101 with parameters: {'feature_fraction': 0.48000000000000004}. Best is trial#38 with value: 0.6822596942865101.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.682136:  67%|######6   | 2/3 [00:21<00:11, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688572\n",
      "[400]\tvalid_0's binary_logloss: 0.686899\n",
      "[600]\tvalid_0's binary_logloss: 0.685627\n",
      "[800]\tvalid_0's binary_logloss: 0.684582\n",
      "[1000]\tvalid_0's binary_logloss: 0.683757\n",
      "[1200]\tvalid_0's binary_logloss: 0.683282\n",
      "[1400]\tvalid_0's binary_logloss: 0.682881\n",
      "[1600]\tvalid_0's binary_logloss: 0.68261\n",
      "[1800]\tvalid_0's binary_logloss: 0.682429\n",
      "[2000]\tvalid_0's binary_logloss: 0.682289\n",
      "[2200]\tvalid_0's binary_logloss: 0.682161\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.682136:  67%|######6   | 2/3 [00:31<00:11, 11.31s/it]\n",
      "feature_fraction_stage2, val_score: 0.682136: 100%|##########| 3/3 [00:31<00:00, 10.96s/it][I 2020-09-17 04:52:04,126] Finished trial#39 with value: 0.6821358671912242 with parameters: {'feature_fraction': 0.41600000000000004}. Best is trial#39 with value: 0.6821358671912242.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "regularization_factors, val_score: 0.682136:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688573\n",
      "[400]\tvalid_0's binary_logloss: 0.686912\n",
      "[600]\tvalid_0's binary_logloss: 0.685626\n",
      "[800]\tvalid_0's binary_logloss: 0.684585\n",
      "[1000]\tvalid_0's binary_logloss: 0.683766\n",
      "[1200]\tvalid_0's binary_logloss: 0.683291\n",
      "[1400]\tvalid_0's binary_logloss: 0.682901\n",
      "[1600]\tvalid_0's binary_logloss: 0.682622\n",
      "[1800]\tvalid_0's binary_logloss: 0.682431\n",
      "[2000]\tvalid_0's binary_logloss: 0.682315\n",
      "[2200]\tvalid_0's binary_logloss: 0.682184\n",
      "Early stopping, best iteration is:\n",
      "[2277]\tvalid_0's binary_logloss: 0.682159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.682136:   0%|          | 0/20 [00:10<?, ?it/s]\n",
      "regularization_factors, val_score: 0.682136:   5%|5         | 1/20 [00:10<03:20, 10.56s/it][I 2020-09-17 04:52:14,742] Finished trial#40 with value: 0.6821592909392237 with parameters: {'lambda_l1': 0.0016042056580580562, 'lambda_l2': 0.034577266682892016}. Best is trial#40 with value: 0.6821592909392237.\n",
      "\n",
      "regularization_factors, val_score: 0.682136:   5%|5         | 1/20 [00:10<03:20, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688576\n",
      "[400]\tvalid_0's binary_logloss: 0.686914\n",
      "[600]\tvalid_0's binary_logloss: 0.685644\n",
      "[800]\tvalid_0's binary_logloss: 0.6846\n",
      "[1000]\tvalid_0's binary_logloss: 0.683789\n",
      "[1200]\tvalid_0's binary_logloss: 0.683314\n",
      "[1400]\tvalid_0's binary_logloss: 0.682899\n",
      "[1600]\tvalid_0's binary_logloss: 0.682615\n",
      "[1800]\tvalid_0's binary_logloss: 0.682432\n",
      "[2000]\tvalid_0's binary_logloss: 0.682288\n",
      "[2200]\tvalid_0's binary_logloss: 0.682159\n",
      "Early stopping, best iteration is:\n",
      "[2277]\tvalid_0's binary_logloss: 0.682141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.682136:   5%|5         | 1/20 [00:23<03:20, 10.56s/it]\n",
      "regularization_factors, val_score: 0.682136:  10%|#         | 2/20 [00:23<03:21, 11.22s/it][I 2020-09-17 04:52:27,495] Finished trial#41 with value: 0.6821406004384576 with parameters: {'lambda_l1': 0.07190744159052972, 'lambda_l2': 5.27842764959292e-08}. Best is trial#41 with value: 0.6821406004384576.\n",
      "\n",
      "regularization_factors, val_score: 0.682136:  10%|#         | 2/20 [00:23<03:21, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688572\n",
      "[400]\tvalid_0's binary_logloss: 0.686899\n",
      "[600]\tvalid_0's binary_logloss: 0.685627\n",
      "[800]\tvalid_0's binary_logloss: 0.684582\n",
      "[1000]\tvalid_0's binary_logloss: 0.683757\n",
      "[1200]\tvalid_0's binary_logloss: 0.683282\n",
      "[1400]\tvalid_0's binary_logloss: 0.682881\n",
      "[1600]\tvalid_0's binary_logloss: 0.68261\n",
      "[1800]\tvalid_0's binary_logloss: 0.682429\n",
      "[2000]\tvalid_0's binary_logloss: 0.682289\n",
      "[2200]\tvalid_0's binary_logloss: 0.682161\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.682136:  10%|#         | 2/20 [00:35<03:21, 11.22s/it]\n",
      "regularization_factors, val_score: 0.682136:  15%|#5        | 3/20 [00:35<03:18, 11.65s/it][I 2020-09-17 04:52:40,165] Finished trial#42 with value: 0.6821358680301925 with parameters: {'lambda_l1': 3.852384006982464e-07, 'lambda_l2': 0.00012262874767480483}. Best is trial#42 with value: 0.6821358680301925.\n",
      "\n",
      "regularization_factors, val_score: 0.682136:  15%|#5        | 3/20 [00:36<03:18, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688573\n",
      "[400]\tvalid_0's binary_logloss: 0.686912\n",
      "[600]\tvalid_0's binary_logloss: 0.685625\n",
      "[800]\tvalid_0's binary_logloss: 0.684583\n",
      "[1000]\tvalid_0's binary_logloss: 0.683764\n",
      "[1200]\tvalid_0's binary_logloss: 0.683289\n",
      "[1400]\tvalid_0's binary_logloss: 0.682899\n",
      "[1600]\tvalid_0's binary_logloss: 0.682619\n",
      "[1800]\tvalid_0's binary_logloss: 0.682429\n",
      "[2000]\tvalid_0's binary_logloss: 0.682292\n",
      "[2200]\tvalid_0's binary_logloss: 0.682157\n",
      "Early stopping, best iteration is:\n",
      "[2277]\tvalid_0's binary_logloss: 0.682132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.682132:  15%|#5        | 3/20 [00:48<03:18, 11.65s/it]\n",
      "regularization_factors, val_score: 0.682132:  20%|##        | 4/20 [00:48<03:12, 12.01s/it][I 2020-09-17 04:52:52,998] Finished trial#43 with value: 0.6821324587210684 with parameters: {'lambda_l1': 0.002806362950861099, 'lambda_l2': 0.030417046398129245}. Best is trial#43 with value: 0.6821324587210684.\n",
      "\n",
      "regularization_factors, val_score: 0.682132:  20%|##        | 4/20 [00:48<03:12, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688572\n",
      "[400]\tvalid_0's binary_logloss: 0.686899\n",
      "[600]\tvalid_0's binary_logloss: 0.685627\n",
      "[800]\tvalid_0's binary_logloss: 0.684582\n",
      "[1000]\tvalid_0's binary_logloss: 0.683757\n",
      "[1200]\tvalid_0's binary_logloss: 0.683282\n",
      "[1400]\tvalid_0's binary_logloss: 0.682881\n",
      "[1600]\tvalid_0's binary_logloss: 0.68261\n",
      "[1800]\tvalid_0's binary_logloss: 0.682429\n",
      "[2000]\tvalid_0's binary_logloss: 0.682289\n",
      "[2200]\tvalid_0's binary_logloss: 0.682161\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.682132:  20%|##        | 4/20 [01:01<03:12, 12.01s/it]\n",
      "regularization_factors, val_score: 0.682132:  25%|##5       | 5/20 [01:01<03:03, 12.26s/it][I 2020-09-17 04:53:05,839] Finished trial#44 with value: 0.6821358715624968 with parameters: {'lambda_l1': 0.00021766825578545262, 'lambda_l2': 1.4016671389628954e-05}. Best is trial#43 with value: 0.6821324587210684.\n",
      "\n",
      "regularization_factors, val_score: 0.682132:  25%|##5       | 5/20 [01:01<03:03, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688572\n",
      "[400]\tvalid_0's binary_logloss: 0.686907\n",
      "[600]\tvalid_0's binary_logloss: 0.685636\n",
      "[800]\tvalid_0's binary_logloss: 0.684594\n",
      "[1000]\tvalid_0's binary_logloss: 0.683769\n",
      "[1200]\tvalid_0's binary_logloss: 0.683294\n",
      "[1400]\tvalid_0's binary_logloss: 0.6829\n",
      "[1600]\tvalid_0's binary_logloss: 0.682622\n",
      "[1800]\tvalid_0's binary_logloss: 0.682432\n",
      "[2000]\tvalid_0's binary_logloss: 0.682309\n",
      "[2200]\tvalid_0's binary_logloss: 0.682191\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.682132:  25%|##5       | 5/20 [01:14<03:03, 12.26s/it]\n",
      "regularization_factors, val_score: 0.682132:  30%|###       | 6/20 [01:14<02:54, 12.50s/it][I 2020-09-17 04:53:18,904] Finished trial#45 with value: 0.6821659251289567 with parameters: {'lambda_l1': 0.0032462153556944615, 'lambda_l2': 9.801495593197233e-06}. Best is trial#43 with value: 0.6821324587210684.\n",
      "\n",
      "regularization_factors, val_score: 0.682132:  30%|###       | 6/20 [01:14<02:54, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688528\n",
      "[400]\tvalid_0's binary_logloss: 0.686831\n",
      "[600]\tvalid_0's binary_logloss: 0.685519\n",
      "[800]\tvalid_0's binary_logloss: 0.684468\n",
      "[1000]\tvalid_0's binary_logloss: 0.683614\n",
      "[1200]\tvalid_0's binary_logloss: 0.68317\n",
      "[1400]\tvalid_0's binary_logloss: 0.682787\n",
      "[1600]\tvalid_0's binary_logloss: 0.682512\n",
      "[1800]\tvalid_0's binary_logloss: 0.682337\n",
      "[2000]\tvalid_0's binary_logloss: 0.682207\n",
      "[2200]\tvalid_0's binary_logloss: 0.682113\n",
      "Early stopping, best iteration is:\n",
      "[2277]\tvalid_0's binary_logloss: 0.682062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.682062:  30%|###       | 6/20 [01:27<02:54, 12.50s/it]\n",
      "regularization_factors, val_score: 0.682062:  35%|###5      | 7/20 [01:27<02:43, 12.57s/it][I 2020-09-17 04:53:31,658] Finished trial#46 with value: 0.682062351090496 with parameters: {'lambda_l1': 0.26234067265858046, 'lambda_l2': 0.003477657805247015}. Best is trial#46 with value: 0.682062351090496.\n",
      "\n",
      "regularization_factors, val_score: 0.682062:  35%|###5      | 7/20 [01:27<02:43, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688541\n",
      "[400]\tvalid_0's binary_logloss: 0.68678\n",
      "[600]\tvalid_0's binary_logloss: 0.685397\n",
      "[800]\tvalid_0's binary_logloss: 0.684344\n",
      "[1000]\tvalid_0's binary_logloss: 0.683499\n",
      "[1200]\tvalid_0's binary_logloss: 0.683004\n",
      "[1400]\tvalid_0's binary_logloss: 0.682611\n",
      "[1600]\tvalid_0's binary_logloss: 0.682257\n",
      "[1800]\tvalid_0's binary_logloss: 0.682053\n",
      "[2000]\tvalid_0's binary_logloss: 0.681833\n",
      "[2200]\tvalid_0's binary_logloss: 0.6817\n",
      "Early stopping, best iteration is:\n",
      "[2284]\tvalid_0's binary_logloss: 0.681655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.681655:  35%|###5      | 7/20 [01:40<02:43, 12.57s/it]\n",
      "regularization_factors, val_score: 0.681655:  40%|####      | 8/20 [01:40<02:33, 12.81s/it][I 2020-09-17 04:53:45,012] Finished trial#47 with value: 0.6816548854546588 with parameters: {'lambda_l1': 0.9837531214774556, 'lambda_l2': 0.8946320510668323}. Best is trial#47 with value: 0.6816548854546588.\n",
      "\n",
      "regularization_factors, val_score: 0.681655:  40%|####      | 8/20 [01:40<02:33, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688572\n",
      "[400]\tvalid_0's binary_logloss: 0.686899\n",
      "[600]\tvalid_0's binary_logloss: 0.685627\n",
      "[800]\tvalid_0's binary_logloss: 0.684582\n",
      "[1000]\tvalid_0's binary_logloss: 0.683757\n",
      "[1200]\tvalid_0's binary_logloss: 0.683282\n",
      "[1400]\tvalid_0's binary_logloss: 0.682881\n",
      "[1600]\tvalid_0's binary_logloss: 0.68261\n",
      "[1800]\tvalid_0's binary_logloss: 0.682429\n",
      "[2000]\tvalid_0's binary_logloss: 0.682289\n",
      "[2200]\tvalid_0's binary_logloss: 0.682161\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.681655:  40%|####      | 8/20 [01:53<02:33, 12.81s/it]\n",
      "regularization_factors, val_score: 0.681655:  45%|####5     | 9/20 [01:53<02:21, 12.87s/it][I 2020-09-17 04:53:58,017] Finished trial#48 with value: 0.6821358671931019 with parameters: {'lambda_l1': 8.715707706460233e-08, 'lambda_l2': 1.162046020066832e-08}. Best is trial#47 with value: 0.6816548854546588.\n",
      "\n",
      "regularization_factors, val_score: 0.681655:  45%|####5     | 9/20 [01:53<02:21, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688545\n",
      "[400]\tvalid_0's binary_logloss: 0.686884\n",
      "[600]\tvalid_0's binary_logloss: 0.685593\n",
      "[800]\tvalid_0's binary_logloss: 0.684551\n",
      "[1000]\tvalid_0's binary_logloss: 0.683712\n",
      "[1200]\tvalid_0's binary_logloss: 0.68325\n",
      "[1400]\tvalid_0's binary_logloss: 0.68287\n",
      "[1600]\tvalid_0's binary_logloss: 0.682584\n",
      "[1800]\tvalid_0's binary_logloss: 0.68242\n",
      "[2000]\tvalid_0's binary_logloss: 0.682277\n",
      "[2200]\tvalid_0's binary_logloss: 0.682183\n",
      "Early stopping, best iteration is:\n",
      "[2277]\tvalid_0's binary_logloss: 0.682152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.681655:  45%|####5     | 9/20 [02:07<02:21, 12.87s/it]\n",
      "regularization_factors, val_score: 0.681655:  50%|#####     | 10/20 [02:07<02:09, 12.97s/it][I 2020-09-17 04:54:11,224] Finished trial#49 with value: 0.6821516425323602 with parameters: {'lambda_l1': 0.1849723999610793, 'lambda_l2': 0.00029487568330527727}. Best is trial#47 with value: 0.6816548854546588.\n",
      "\n",
      "regularization_factors, val_score: 0.681655:  50%|#####     | 10/20 [02:07<02:09, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68877\n",
      "[400]\tvalid_0's binary_logloss: 0.68712\n",
      "[600]\tvalid_0's binary_logloss: 0.685788\n",
      "[800]\tvalid_0's binary_logloss: 0.68467\n",
      "[1000]\tvalid_0's binary_logloss: 0.683738\n",
      "[1200]\tvalid_0's binary_logloss: 0.683033\n",
      "[1400]\tvalid_0's binary_logloss: 0.68241\n",
      "[1600]\tvalid_0's binary_logloss: 0.681937\n",
      "[1800]\tvalid_0's binary_logloss: 0.68156\n",
      "[2000]\tvalid_0's binary_logloss: 0.681308\n",
      "[2200]\tvalid_0's binary_logloss: 0.681171\n",
      "[2400]\tvalid_0's binary_logloss: 0.68107\n",
      "[2600]\tvalid_0's binary_logloss: 0.68095\n",
      "Early stopping, best iteration is:\n",
      "[2674]\tvalid_0's binary_logloss: 0.680887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  50%|#####     | 10/20 [02:24<02:09, 12.97s/it]\n",
      "regularization_factors, val_score: 0.680887:  55%|#####5    | 11/20 [02:24<02:08, 14.25s/it][I 2020-09-17 04:54:28,487] Finished trial#50 with value: 0.6808869289061315 with parameters: {'lambda_l1': 5.741671902061814, 'lambda_l2': 3.656941511091706}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  55%|#####5    | 11/20 [02:24<02:08, 14.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689111\n",
      "[400]\tvalid_0's binary_logloss: 0.687702\n",
      "[600]\tvalid_0's binary_logloss: 0.68653\n",
      "[800]\tvalid_0's binary_logloss: 0.685477\n",
      "[1000]\tvalid_0's binary_logloss: 0.684588\n",
      "[1200]\tvalid_0's binary_logloss: 0.683873\n",
      "[1400]\tvalid_0's binary_logloss: 0.683233\n",
      "[1600]\tvalid_0's binary_logloss: 0.682761\n",
      "[1800]\tvalid_0's binary_logloss: 0.682353\n",
      "[2000]\tvalid_0's binary_logloss: 0.681991\n",
      "[2200]\tvalid_0's binary_logloss: 0.681775\n",
      "[2400]\tvalid_0's binary_logloss: 0.681504\n",
      "[2600]\tvalid_0's binary_logloss: 0.681334\n",
      "[2800]\tvalid_0's binary_logloss: 0.681209\n",
      "Early stopping, best iteration is:\n",
      "[2813]\tvalid_0's binary_logloss: 0.681201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  55%|#####5    | 11/20 [02:41<02:08, 14.25s/it]\n",
      "regularization_factors, val_score: 0.680887:  60%|######    | 12/20 [02:41<02:01, 15.24s/it][I 2020-09-17 04:54:46,030] Finished trial#51 with value: 0.6812011669963316 with parameters: {'lambda_l1': 9.7519951075255, 'lambda_l2': 7.793372410354122}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  60%|######    | 12/20 [02:41<02:01, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688922\n",
      "[400]\tvalid_0's binary_logloss: 0.687374\n",
      "[600]\tvalid_0's binary_logloss: 0.686119\n",
      "[800]\tvalid_0's binary_logloss: 0.685003\n",
      "[1000]\tvalid_0's binary_logloss: 0.684081\n",
      "[1200]\tvalid_0's binary_logloss: 0.683351\n",
      "[1400]\tvalid_0's binary_logloss: 0.682695\n",
      "[1600]\tvalid_0's binary_logloss: 0.682243\n",
      "[1800]\tvalid_0's binary_logloss: 0.681861\n",
      "[2000]\tvalid_0's binary_logloss: 0.681536\n",
      "[2200]\tvalid_0's binary_logloss: 0.681371\n",
      "[2400]\tvalid_0's binary_logloss: 0.681174\n",
      "Early stopping, best iteration is:\n",
      "[2394]\tvalid_0's binary_logloss: 0.681155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  60%|######    | 12/20 [02:54<02:01, 15.24s/it]\n",
      "regularization_factors, val_score: 0.680887:  65%|######5   | 13/20 [02:54<01:40, 14.42s/it][I 2020-09-17 04:54:58,555] Finished trial#52 with value: 0.6811549865357391 with parameters: {'lambda_l1': 7.7412008338227425, 'lambda_l2': 5.715460859183949}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  65%|######5   | 13/20 [02:54<01:40, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688732\n",
      "[400]\tvalid_0's binary_logloss: 0.687076\n",
      "[600]\tvalid_0's binary_logloss: 0.685721\n",
      "[800]\tvalid_0's binary_logloss: 0.684606\n",
      "[1000]\tvalid_0's binary_logloss: 0.68366\n",
      "[1200]\tvalid_0's binary_logloss: 0.682923\n",
      "[1400]\tvalid_0's binary_logloss: 0.682281\n",
      "[1600]\tvalid_0's binary_logloss: 0.681831\n",
      "[1800]\tvalid_0's binary_logloss: 0.681471\n",
      "[2000]\tvalid_0's binary_logloss: 0.681209\n",
      "[2200]\tvalid_0's binary_logloss: 0.681085\n",
      "[2400]\tvalid_0's binary_logloss: 0.680986\n",
      "Early stopping, best iteration is:\n",
      "[2394]\tvalid_0's binary_logloss: 0.680962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  65%|######5   | 13/20 [03:06<01:40, 14.42s/it]\n",
      "regularization_factors, val_score: 0.680887:  70%|#######   | 14/20 [03:06<01:22, 13.82s/it][I 2020-09-17 04:55:10,966] Finished trial#53 with value: 0.6809623443163345 with parameters: {'lambda_l1': 5.941374771519879, 'lambda_l2': 1.7238708045729165}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  70%|#######   | 14/20 [03:06<01:22, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68853\n",
      "[400]\tvalid_0's binary_logloss: 0.686856\n",
      "[600]\tvalid_0's binary_logloss: 0.685553\n",
      "[800]\tvalid_0's binary_logloss: 0.684496\n",
      "[1000]\tvalid_0's binary_logloss: 0.68366\n",
      "[1200]\tvalid_0's binary_logloss: 0.683212\n",
      "[1400]\tvalid_0's binary_logloss: 0.682838\n",
      "[1600]\tvalid_0's binary_logloss: 0.68255\n",
      "[1800]\tvalid_0's binary_logloss: 0.682356\n",
      "[2000]\tvalid_0's binary_logloss: 0.682213\n",
      "[2200]\tvalid_0's binary_logloss: 0.682113\n",
      "Early stopping, best iteration is:\n",
      "[2277]\tvalid_0's binary_logloss: 0.682084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  70%|#######   | 14/20 [03:20<01:22, 13.82s/it]\n",
      "regularization_factors, val_score: 0.680887:  75%|#######5  | 15/20 [03:20<01:09, 13.85s/it][I 2020-09-17 04:55:24,885] Finished trial#54 with value: 0.6820835104648532 with parameters: {'lambda_l1': 9.903925854177135e-06, 'lambda_l2': 0.32058118769863597}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  75%|#######5  | 15/20 [03:20<01:09, 13.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688848\n",
      "[400]\tvalid_0's binary_logloss: 0.68723\n",
      "[600]\tvalid_0's binary_logloss: 0.685949\n",
      "[800]\tvalid_0's binary_logloss: 0.684809\n",
      "[1000]\tvalid_0's binary_logloss: 0.683891\n",
      "[1200]\tvalid_0's binary_logloss: 0.68319\n",
      "[1400]\tvalid_0's binary_logloss: 0.682554\n",
      "[1600]\tvalid_0's binary_logloss: 0.682083\n",
      "[1800]\tvalid_0's binary_logloss: 0.6817\n",
      "[2000]\tvalid_0's binary_logloss: 0.681433\n",
      "[2200]\tvalid_0's binary_logloss: 0.681291\n",
      "[2400]\tvalid_0's binary_logloss: 0.681185\n",
      "Early stopping, best iteration is:\n",
      "[2394]\tvalid_0's binary_logloss: 0.681162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  75%|#######5  | 15/20 [03:33<01:09, 13.85s/it]\n",
      "regularization_factors, val_score: 0.680887:  80%|########  | 16/20 [03:33<00:54, 13.67s/it][I 2020-09-17 04:55:38,117] Finished trial#55 with value: 0.6811621865565305 with parameters: {'lambda_l1': 5.026049379256462, 'lambda_l2': 9.98375085571108}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  80%|########  | 16/20 [03:33<00:54, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688529\n",
      "[400]\tvalid_0's binary_logloss: 0.686838\n",
      "[600]\tvalid_0's binary_logloss: 0.685523\n",
      "[800]\tvalid_0's binary_logloss: 0.684472\n",
      "[1000]\tvalid_0's binary_logloss: 0.683654\n",
      "[1200]\tvalid_0's binary_logloss: 0.683209\n",
      "[1400]\tvalid_0's binary_logloss: 0.682848\n",
      "[1600]\tvalid_0's binary_logloss: 0.682569\n",
      "[1800]\tvalid_0's binary_logloss: 0.682397\n",
      "[2000]\tvalid_0's binary_logloss: 0.682242\n",
      "Early stopping, best iteration is:\n",
      "[2009]\tvalid_0's binary_logloss: 0.682226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  80%|########  | 16/20 [03:45<00:54, 13.67s/it]\n",
      "regularization_factors, val_score: 0.680887:  85%|########5 | 17/20 [03:45<00:39, 13.17s/it][I 2020-09-17 04:55:50,123] Finished trial#56 with value: 0.6822256449614457 with parameters: {'lambda_l1': 0.029648826185768872, 'lambda_l2': 0.49671834175512697}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  85%|########5 | 17/20 [03:45<00:39, 13.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68853\n",
      "[400]\tvalid_0's binary_logloss: 0.68676\n",
      "[600]\tvalid_0's binary_logloss: 0.685385\n",
      "[800]\tvalid_0's binary_logloss: 0.684343\n",
      "[1000]\tvalid_0's binary_logloss: 0.68351\n",
      "[1200]\tvalid_0's binary_logloss: 0.683019\n",
      "[1400]\tvalid_0's binary_logloss: 0.682605\n",
      "[1600]\tvalid_0's binary_logloss: 0.682252\n",
      "[1800]\tvalid_0's binary_logloss: 0.682051\n",
      "[2000]\tvalid_0's binary_logloss: 0.681859\n",
      "Early stopping, best iteration is:\n",
      "[1989]\tvalid_0's binary_logloss: 0.681835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  85%|########5 | 17/20 [03:49<00:39, 13.17s/it]\n",
      "regularization_factors, val_score: 0.680887:  90%|######### | 18/20 [03:49<00:20, 10.25s/it][I 2020-09-17 04:55:53,569] Finished trial#57 with value: 0.6818354982662861 with parameters: {'lambda_l1': 1.1988923200652493, 'lambda_l2': 0.05904654837122798}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  90%|######### | 18/20 [03:49<00:20, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688572\n",
      "[400]\tvalid_0's binary_logloss: 0.686907\n",
      "[600]\tvalid_0's binary_logloss: 0.685636\n",
      "[800]\tvalid_0's binary_logloss: 0.684594\n",
      "[1000]\tvalid_0's binary_logloss: 0.68377\n",
      "[1200]\tvalid_0's binary_logloss: 0.683295\n",
      "[1400]\tvalid_0's binary_logloss: 0.682901\n",
      "[1600]\tvalid_0's binary_logloss: 0.682623\n",
      "[1800]\tvalid_0's binary_logloss: 0.682433\n",
      "[2000]\tvalid_0's binary_logloss: 0.68231\n",
      "[2200]\tvalid_0's binary_logloss: 0.682192\n",
      "Early stopping, best iteration is:\n",
      "[2283]\tvalid_0's binary_logloss: 0.682167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  90%|######### | 18/20 [04:02<00:20, 10.25s/it]\n",
      "regularization_factors, val_score: 0.680887:  95%|#########5| 19/20 [04:02<00:11, 11.15s/it][I 2020-09-17 04:56:06,819] Finished trial#58 with value: 0.6821666937283992 with parameters: {'lambda_l1': 4.065389074903491e-05, 'lambda_l2': 0.004320394035229654}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "regularization_factors, val_score: 0.680887:  95%|#########5| 19/20 [04:02<00:11, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688527\n",
      "[400]\tvalid_0's binary_logloss: 0.686764\n",
      "[600]\tvalid_0's binary_logloss: 0.685419\n",
      "[800]\tvalid_0's binary_logloss: 0.68437\n",
      "[1000]\tvalid_0's binary_logloss: 0.683578\n",
      "[1200]\tvalid_0's binary_logloss: 0.68314\n",
      "[1400]\tvalid_0's binary_logloss: 0.682785\n",
      "[1600]\tvalid_0's binary_logloss: 0.682472\n",
      "[1800]\tvalid_0's binary_logloss: 0.682318\n",
      "[2000]\tvalid_0's binary_logloss: 0.682164\n",
      "[2200]\tvalid_0's binary_logloss: 0.682052\n",
      "Early stopping, best iteration is:\n",
      "[2247]\tvalid_0's binary_logloss: 0.682008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680887:  95%|#########5| 19/20 [04:15<00:11, 11.15s/it]\n",
      "regularization_factors, val_score: 0.680887: 100%|##########| 20/20 [04:15<00:00, 11.70s/it][I 2020-09-17 04:56:19,807] Finished trial#59 with value: 0.6820083445216972 with parameters: {'lambda_l1': 0.015211141468217487, 'lambda_l2': 1.8575460772800905}. Best is trial#50 with value: 0.6808869289061315.\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.680887:   0%|          | 0/5 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688768\n",
      "[400]\tvalid_0's binary_logloss: 0.687118\n",
      "[600]\tvalid_0's binary_logloss: 0.685787\n",
      "[800]\tvalid_0's binary_logloss: 0.684666\n",
      "[1000]\tvalid_0's binary_logloss: 0.683734\n",
      "[1200]\tvalid_0's binary_logloss: 0.683028\n",
      "[1400]\tvalid_0's binary_logloss: 0.682405\n",
      "[1600]\tvalid_0's binary_logloss: 0.681938\n",
      "[1800]\tvalid_0's binary_logloss: 0.681554\n",
      "[2000]\tvalid_0's binary_logloss: 0.68131\n",
      "[2200]\tvalid_0's binary_logloss: 0.681171\n",
      "[2400]\tvalid_0's binary_logloss: 0.68107\n",
      "[2600]\tvalid_0's binary_logloss: 0.680957\n",
      "Early stopping, best iteration is:\n",
      "[2643]\tvalid_0's binary_logloss: 0.680899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.680887:   0%|          | 0/5 [00:14<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.680887:  20%|##        | 1/5 [00:14<00:59, 14.85s/it][I 2020-09-17 04:56:34,717] Finished trial#60 with value: 0.6808988253208966 with parameters: {'min_child_samples': 5}. Best is trial#60 with value: 0.6808988253208966.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.680887:  20%|##        | 1/5 [00:14<00:59, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688768\n",
      "[400]\tvalid_0's binary_logloss: 0.687097\n",
      "[600]\tvalid_0's binary_logloss: 0.685754\n",
      "[800]\tvalid_0's binary_logloss: 0.684601\n",
      "[1000]\tvalid_0's binary_logloss: 0.683671\n",
      "[1200]\tvalid_0's binary_logloss: 0.682976\n",
      "[1400]\tvalid_0's binary_logloss: 0.68234\n",
      "[1600]\tvalid_0's binary_logloss: 0.68187\n",
      "[1800]\tvalid_0's binary_logloss: 0.681469\n",
      "[2000]\tvalid_0's binary_logloss: 0.68121\n",
      "[2200]\tvalid_0's binary_logloss: 0.681067\n",
      "[2400]\tvalid_0's binary_logloss: 0.680955\n",
      "[2600]\tvalid_0's binary_logloss: 0.680834\n",
      "[2800]\tvalid_0's binary_logloss: 0.68076\n",
      "Early stopping, best iteration is:\n",
      "[2779]\tvalid_0's binary_logloss: 0.680756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.680756:  20%|##        | 1/5 [00:34<00:59, 14.85s/it]\n",
      "min_data_in_leaf, val_score: 0.680756:  40%|####      | 2/5 [00:34<00:48, 16.17s/it][I 2020-09-17 04:56:53,984] Finished trial#61 with value: 0.6807557376090089 with parameters: {'min_child_samples': 50}. Best is trial#61 with value: 0.6807557376090089.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.680756:  40%|####      | 2/5 [00:34<00:48, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688768\n",
      "[400]\tvalid_0's binary_logloss: 0.687118\n",
      "[600]\tvalid_0's binary_logloss: 0.685787\n",
      "[800]\tvalid_0's binary_logloss: 0.684666\n",
      "[1000]\tvalid_0's binary_logloss: 0.683734\n",
      "[1200]\tvalid_0's binary_logloss: 0.683028\n",
      "[1400]\tvalid_0's binary_logloss: 0.682405\n",
      "[1600]\tvalid_0's binary_logloss: 0.681938\n",
      "[1800]\tvalid_0's binary_logloss: 0.681554\n",
      "[2000]\tvalid_0's binary_logloss: 0.681307\n",
      "[2200]\tvalid_0's binary_logloss: 0.681174\n",
      "[2400]\tvalid_0's binary_logloss: 0.681073\n",
      "[2600]\tvalid_0's binary_logloss: 0.680957\n",
      "Early stopping, best iteration is:\n",
      "[2643]\tvalid_0's binary_logloss: 0.680899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.680756:  40%|####      | 2/5 [00:51<00:48, 16.17s/it]\n",
      "min_data_in_leaf, val_score: 0.680756:  60%|######    | 3/5 [00:51<00:33, 16.62s/it][I 2020-09-17 04:57:11,654] Finished trial#62 with value: 0.6808992868692182 with parameters: {'min_child_samples': 10}. Best is trial#61 with value: 0.6807557376090089.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.680756:  60%|######    | 3/5 [00:51<00:33, 16.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68877\n",
      "[400]\tvalid_0's binary_logloss: 0.687118\n",
      "[600]\tvalid_0's binary_logloss: 0.685785\n",
      "[800]\tvalid_0's binary_logloss: 0.684664\n",
      "[1000]\tvalid_0's binary_logloss: 0.683732\n",
      "[1200]\tvalid_0's binary_logloss: 0.683027\n",
      "[1400]\tvalid_0's binary_logloss: 0.682403\n",
      "[1600]\tvalid_0's binary_logloss: 0.681931\n",
      "[1800]\tvalid_0's binary_logloss: 0.681552\n",
      "[2000]\tvalid_0's binary_logloss: 0.681299\n",
      "[2200]\tvalid_0's binary_logloss: 0.681163\n",
      "[2400]\tvalid_0's binary_logloss: 0.681062\n",
      "[2600]\tvalid_0's binary_logloss: 0.68094\n",
      "Early stopping, best iteration is:\n",
      "[2674]\tvalid_0's binary_logloss: 0.680877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.680756:  60%|######    | 3/5 [01:14<00:33, 16.62s/it]\n",
      "min_data_in_leaf, val_score: 0.680756:  80%|########  | 4/5 [01:14<00:18, 18.43s/it][I 2020-09-17 04:57:34,314] Finished trial#63 with value: 0.6808771869176988 with parameters: {'min_child_samples': 25}. Best is trial#61 with value: 0.6807557376090089.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.680756:  80%|########  | 4/5 [01:14<00:18, 18.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688776\n",
      "[400]\tvalid_0's binary_logloss: 0.687091\n",
      "[600]\tvalid_0's binary_logloss: 0.685762\n",
      "[800]\tvalid_0's binary_logloss: 0.684596\n",
      "[1000]\tvalid_0's binary_logloss: 0.683667\n",
      "[1200]\tvalid_0's binary_logloss: 0.682959\n",
      "[1400]\tvalid_0's binary_logloss: 0.682361\n",
      "[1600]\tvalid_0's binary_logloss: 0.681898\n",
      "[1800]\tvalid_0's binary_logloss: 0.681521\n",
      "[2000]\tvalid_0's binary_logloss: 0.68122\n",
      "[2200]\tvalid_0's binary_logloss: 0.68107\n",
      "[2400]\tvalid_0's binary_logloss: 0.680935\n",
      "[2600]\tvalid_0's binary_logloss: 0.680813\n",
      "Early stopping, best iteration is:\n",
      "[2622]\tvalid_0's binary_logloss: 0.680756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.680756:  80%|########  | 4/5 [01:28<00:18, 18.43s/it]\n",
      "min_data_in_leaf, val_score: 0.680756: 100%|##########| 5/5 [01:28<00:00, 17.04s/it][I 2020-09-17 04:57:48,092] Finished trial#64 with value: 0.6807560384632376 with parameters: {'min_child_samples': 100}. Best is trial#61 with value: 0.6807557376090089.\n",
      "/home/tubotu/.local/lib/python3.6/site-packages/optuna/_experimental.py:90: ExperimentalWarning: train is experimental (supported from v0.18.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687838\n",
      "[400]\tvalid_0's binary_logloss: 0.68586\n",
      "[600]\tvalid_0's binary_logloss: 0.684731\n",
      "[800]\tvalid_0's binary_logloss: 0.683979\n",
      "[1000]\tvalid_0's binary_logloss: 0.683108\n",
      "[1200]\tvalid_0's binary_logloss: 0.682761\n",
      "Early stopping, best iteration is:\n",
      "[1279]\tvalid_0's binary_logloss: 0.682504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.682504:   0%|          | 0/7 [00:14<?, ?it/s]\n",
      "feature_fraction, val_score: 0.682504:  14%|#4        | 1/7 [00:14<01:24, 14.01s/it][I 2020-09-17 04:58:02,202] Finished trial#0 with value: 0.6825044891711226 with parameters: {'feature_fraction': 0.4}. Best is trial#0 with value: 0.6825044891711226.\n",
      "\n",
      "feature_fraction, val_score: 0.682504:  14%|#4        | 1/7 [00:14<01:24, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68836\n",
      "[400]\tvalid_0's binary_logloss: 0.686641\n",
      "[600]\tvalid_0's binary_logloss: 0.685617\n",
      "[800]\tvalid_0's binary_logloss: 0.684409\n",
      "[1000]\tvalid_0's binary_logloss: 0.683417\n",
      "Early stopping, best iteration is:\n",
      "[1031]\tvalid_0's binary_logloss: 0.683305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.682504:  14%|#4        | 1/7 [00:27<01:24, 14.01s/it]\n",
      "feature_fraction, val_score: 0.682504:  29%|##8       | 2/7 [00:27<01:09, 13.92s/it][I 2020-09-17 04:58:15,919] Finished trial#1 with value: 0.6833054118501269 with parameters: {'feature_fraction': 0.8999999999999999}. Best is trial#0 with value: 0.6825044891711226.\n",
      "\n",
      "feature_fraction, val_score: 0.682504:  29%|##8       | 2/7 [00:27<01:09, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688018\n",
      "[400]\tvalid_0's binary_logloss: 0.686624\n",
      "[600]\tvalid_0's binary_logloss: 0.685244\n",
      "[800]\tvalid_0's binary_logloss: 0.684202\n",
      "[1000]\tvalid_0's binary_logloss: 0.683452\n",
      "Early stopping, best iteration is:\n",
      "[964]\tvalid_0's binary_logloss: 0.6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.682504:  29%|##8       | 2/7 [00:41<01:09, 13.92s/it]\n",
      "feature_fraction, val_score: 0.682504:  43%|####2     | 3/7 [00:41<00:55, 13.76s/it][I 2020-09-17 04:58:29,289] Finished trial#2 with value: 0.6834004680919276 with parameters: {'feature_fraction': 0.8}. Best is trial#0 with value: 0.6825044891711226.\n",
      "\n",
      "feature_fraction, val_score: 0.682504:  43%|####2     | 3/7 [00:41<00:55, 13.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687389\n",
      "[400]\tvalid_0's binary_logloss: 0.685185\n",
      "[600]\tvalid_0's binary_logloss: 0.683499\n",
      "[800]\tvalid_0's binary_logloss: 0.682383\n",
      "[1000]\tvalid_0's binary_logloss: 0.681915\n",
      "Early stopping, best iteration is:\n",
      "[1004]\tvalid_0's binary_logloss: 0.681872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.681872:  43%|####2     | 3/7 [00:54<00:55, 13.76s/it]\n",
      "feature_fraction, val_score: 0.681872:  57%|#####7    | 4/7 [00:54<00:40, 13.59s/it][I 2020-09-17 04:58:42,472] Finished trial#3 with value: 0.6818717362295231 with parameters: {'feature_fraction': 0.5}. Best is trial#3 with value: 0.6818717362295231.\n",
      "\n",
      "feature_fraction, val_score: 0.681872:  57%|#####7    | 4/7 [00:54<00:40, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687211\n",
      "[400]\tvalid_0's binary_logloss: 0.685289\n",
      "[600]\tvalid_0's binary_logloss: 0.683946\n",
      "[800]\tvalid_0's binary_logloss: 0.683112\n",
      "[1000]\tvalid_0's binary_logloss: 0.682906\n",
      "[1200]\tvalid_0's binary_logloss: 0.682587\n",
      "Early stopping, best iteration is:\n",
      "[1174]\tvalid_0's binary_logloss: 0.682529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.681872:  57%|#####7    | 4/7 [01:12<00:40, 13.59s/it]\n",
      "feature_fraction, val_score: 0.681872:  71%|#######1  | 5/7 [01:12<00:30, 15.09s/it][I 2020-09-17 04:59:01,088] Finished trial#4 with value: 0.6825287569810384 with parameters: {'feature_fraction': 0.6}. Best is trial#3 with value: 0.6818717362295231.\n",
      "\n",
      "feature_fraction, val_score: 0.681872:  71%|#######1  | 5/7 [01:12<00:30, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687786\n",
      "[400]\tvalid_0's binary_logloss: 0.686741\n",
      "[600]\tvalid_0's binary_logloss: 0.685632\n",
      "[800]\tvalid_0's binary_logloss: 0.684909\n",
      "[1000]\tvalid_0's binary_logloss: 0.683397\n",
      "[1200]\tvalid_0's binary_logloss: 0.682143\n",
      "Early stopping, best iteration is:\n",
      "[1228]\tvalid_0's binary_logloss: 0.682053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.681872:  71%|#######1  | 5/7 [01:30<00:30, 15.09s/it]\n",
      "feature_fraction, val_score: 0.681872:  86%|########5 | 6/7 [01:30<00:15, 15.78s/it][I 2020-09-17 04:59:18,476] Finished trial#5 with value: 0.6820534722541546 with parameters: {'feature_fraction': 1.0}. Best is trial#3 with value: 0.6818717362295231.\n",
      "\n",
      "feature_fraction, val_score: 0.681872:  86%|########5 | 6/7 [01:30<00:15, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687672\n",
      "[400]\tvalid_0's binary_logloss: 0.685831\n",
      "[600]\tvalid_0's binary_logloss: 0.684183\n",
      "[800]\tvalid_0's binary_logloss: 0.683182\n",
      "[1000]\tvalid_0's binary_logloss: 0.682816\n",
      "[1200]\tvalid_0's binary_logloss: 0.682693\n",
      "Early stopping, best iteration is:\n",
      "[1186]\tvalid_0's binary_logloss: 0.682608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.681872:  86%|########5 | 6/7 [01:44<00:15, 15.78s/it]\n",
      "feature_fraction, val_score: 0.681872: 100%|##########| 7/7 [01:44<00:00, 15.24s/it][I 2020-09-17 04:59:32,433] Finished trial#6 with value: 0.6826076621930557 with parameters: {'feature_fraction': 0.7}. Best is trial#3 with value: 0.6818717362295231.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "num_leaves, val_score: 0.681872:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:   0%|          | 0/20 [00:14<?, ?it/s]\n",
      "num_leaves, val_score: 0.681872:   5%|5         | 1/20 [00:14<04:34, 14.45s/it][I 2020-09-17 04:59:46,941] Finished trial#7 with value: 0.6828486457363533 with parameters: {'num_leaves': 95}. Best is trial#7 with value: 0.6828486457363533.\n",
      "\n",
      "num_leaves, val_score: 0.681872:   5%|5         | 1/20 [00:14<04:34, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68742\n",
      "[400]\tvalid_0's binary_logloss: 0.685287\n",
      "[600]\tvalid_0's binary_logloss: 0.683659\n",
      "[800]\tvalid_0's binary_logloss: 0.682712\n",
      "Early stopping, best iteration is:\n",
      "[878]\tvalid_0's binary_logloss: 0.682501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:   5%|5         | 1/20 [00:45<04:34, 14.45s/it]\n",
      "num_leaves, val_score: 0.681872:  10%|#         | 2/20 [00:45<05:49, 19.40s/it][I 2020-09-17 05:00:17,899] Finished trial#8 with value: 0.6825014884512287 with parameters: {'num_leaves': 70}. Best is trial#8 with value: 0.6825014884512287.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  10%|#         | 2/20 [00:45<05:49, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  10%|#         | 2/20 [01:17<05:49, 19.40s/it]\n",
      "num_leaves, val_score: 0.681872:  15%|#5        | 3/20 [01:17<06:33, 23.15s/it][I 2020-09-17 05:00:49,786] Finished trial#9 with value: 0.6828486457363532 with parameters: {'num_leaves': 138}. Best is trial#8 with value: 0.6825014884512287.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  15%|#5        | 3/20 [01:17<06:33, 23.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68739\n",
      "[400]\tvalid_0's binary_logloss: 0.685108\n",
      "[600]\tvalid_0's binary_logloss: 0.68349\n",
      "[800]\tvalid_0's binary_logloss: 0.682543\n",
      "[1000]\tvalid_0's binary_logloss: 0.682221\n",
      "Early stopping, best iteration is:\n",
      "[973]\tvalid_0's binary_logloss: 0.682116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  15%|#5        | 3/20 [01:41<06:33, 23.15s/it]\n",
      "num_leaves, val_score: 0.681872:  20%|##        | 4/20 [01:41<06:13, 23.33s/it][I 2020-09-17 05:01:13,542] Finished trial#10 with value: 0.6821163778097432 with parameters: {'num_leaves': 47}. Best is trial#10 with value: 0.6821163778097432.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  20%|##        | 4/20 [01:41<06:13, 23.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  20%|##        | 4/20 [01:55<06:13, 23.33s/it]\n",
      "num_leaves, val_score: 0.681872:  25%|##5       | 5/20 [01:55<05:10, 20.69s/it][I 2020-09-17 05:01:28,060] Finished trial#11 with value: 0.6828486457363533 with parameters: {'num_leaves': 211}. Best is trial#10 with value: 0.6821163778097432.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  25%|##5       | 5/20 [01:55<05:10, 20.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  25%|##5       | 5/20 [02:08<05:10, 20.69s/it]\n",
      "num_leaves, val_score: 0.681872:  30%|###       | 6/20 [02:08<04:15, 18.25s/it][I 2020-09-17 05:01:40,629] Finished trial#12 with value: 0.6828486457363532 with parameters: {'num_leaves': 155}. Best is trial#10 with value: 0.6821163778097432.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  30%|###       | 6/20 [02:08<04:15, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  30%|###       | 6/20 [02:20<04:15, 18.25s/it]\n",
      "num_leaves, val_score: 0.681872:  35%|###5      | 7/20 [02:20<03:33, 16.43s/it][I 2020-09-17 05:01:52,803] Finished trial#13 with value: 0.6828486457363533 with parameters: {'num_leaves': 147}. Best is trial#10 with value: 0.6821163778097432.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  35%|###5      | 7/20 [02:20<03:33, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  35%|###5      | 7/20 [02:33<03:33, 16.43s/it]\n",
      "num_leaves, val_score: 0.681872:  40%|####      | 8/20 [02:33<03:05, 15.42s/it][I 2020-09-17 05:02:05,883] Finished trial#14 with value: 0.6828486457363532 with parameters: {'num_leaves': 151}. Best is trial#10 with value: 0.6821163778097432.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  40%|####      | 8/20 [02:33<03:05, 15.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687655\n",
      "[400]\tvalid_0's binary_logloss: 0.685799\n",
      "[600]\tvalid_0's binary_logloss: 0.68422\n",
      "[800]\tvalid_0's binary_logloss: 0.682969\n",
      "[1000]\tvalid_0's binary_logloss: 0.682562\n",
      "[1200]\tvalid_0's binary_logloss: 0.682159\n",
      "[1400]\tvalid_0's binary_logloss: 0.682234\n",
      "Early stopping, best iteration is:\n",
      "[1313]\tvalid_0's binary_logloss: 0.682015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  40%|####      | 8/20 [02:59<03:05, 15.42s/it]\n",
      "num_leaves, val_score: 0.681872:  45%|####5     | 9/20 [02:59<03:24, 18.62s/it][I 2020-09-17 05:02:31,968] Finished trial#15 with value: 0.682014511278877 with parameters: {'num_leaves': 20}. Best is trial#15 with value: 0.682014511278877.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  45%|####5     | 9/20 [02:59<03:24, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  45%|####5     | 9/20 [03:13<03:24, 18.62s/it]\n",
      "num_leaves, val_score: 0.681872:  50%|#####     | 10/20 [03:13<02:52, 17.23s/it][I 2020-09-17 05:02:45,942] Finished trial#16 with value: 0.6828486457363532 with parameters: {'num_leaves': 211}. Best is trial#15 with value: 0.682014511278877.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  50%|#####     | 10/20 [03:13<02:52, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689656\n",
      "[400]\tvalid_0's binary_logloss: 0.688788\n",
      "[600]\tvalid_0's binary_logloss: 0.68819\n",
      "[800]\tvalid_0's binary_logloss: 0.68776\n",
      "[1000]\tvalid_0's binary_logloss: 0.687509\n",
      "[1200]\tvalid_0's binary_logloss: 0.687311\n",
      "[1400]\tvalid_0's binary_logloss: 0.687183\n",
      "[1600]\tvalid_0's binary_logloss: 0.687111\n",
      "[1800]\tvalid_0's binary_logloss: 0.68707\n",
      "[2000]\tvalid_0's binary_logloss: 0.687042\n",
      "[2200]\tvalid_0's binary_logloss: 0.687034\n",
      "Early stopping, best iteration is:\n",
      "[2179]\tvalid_0's binary_logloss: 0.687023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  50%|#####     | 10/20 [03:25<02:52, 17.23s/it]\n",
      "num_leaves, val_score: 0.681872:  55%|#####5    | 11/20 [03:25<02:21, 15.74s/it][I 2020-09-17 05:02:58,217] Finished trial#17 with value: 0.6870231135820298 with parameters: {'num_leaves': 2}. Best is trial#15 with value: 0.682014511278877.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  55%|#####5    | 11/20 [03:25<02:21, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688197\n",
      "[400]\tvalid_0's binary_logloss: 0.686402\n",
      "[600]\tvalid_0's binary_logloss: 0.684941\n",
      "[800]\tvalid_0's binary_logloss: 0.683926\n",
      "[1000]\tvalid_0's binary_logloss: 0.683239\n",
      "[1200]\tvalid_0's binary_logloss: 0.683073\n",
      "Early stopping, best iteration is:\n",
      "[1181]\tvalid_0's binary_logloss: 0.683052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  55%|#####5    | 11/20 [03:36<02:21, 15.74s/it]\n",
      "num_leaves, val_score: 0.681872:  60%|######    | 12/20 [03:36<01:54, 14.28s/it][I 2020-09-17 05:03:09,070] Finished trial#18 with value: 0.6830518137853325 with parameters: {'num_leaves': 5}. Best is trial#15 with value: 0.682014511278877.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  60%|######    | 12/20 [03:36<01:54, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687322\n",
      "[400]\tvalid_0's binary_logloss: 0.685065\n",
      "[600]\tvalid_0's binary_logloss: 0.683375\n",
      "[800]\tvalid_0's binary_logloss: 0.682513\n",
      "[1000]\tvalid_0's binary_logloss: 0.682273\n",
      "Early stopping, best iteration is:\n",
      "[976]\tvalid_0's binary_logloss: 0.682194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  60%|######    | 12/20 [03:50<01:54, 14.28s/it]\n",
      "num_leaves, val_score: 0.681872:  65%|######5   | 13/20 [03:50<01:38, 14.06s/it][I 2020-09-17 05:03:22,620] Finished trial#19 with value: 0.6821937324178435 with parameters: {'num_leaves': 45}. Best is trial#15 with value: 0.682014511278877.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  65%|######5   | 13/20 [03:50<01:38, 14.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687363\n",
      "[400]\tvalid_0's binary_logloss: 0.685151\n",
      "[600]\tvalid_0's binary_logloss: 0.683505\n",
      "[800]\tvalid_0's binary_logloss: 0.682613\n",
      "[1000]\tvalid_0's binary_logloss: 0.682204\n",
      "Early stopping, best iteration is:\n",
      "[991]\tvalid_0's binary_logloss: 0.682146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681872:  65%|######5   | 13/20 [04:11<01:38, 14.06s/it]\n",
      "num_leaves, val_score: 0.681872:  70%|#######   | 14/20 [04:11<01:37, 16.31s/it][I 2020-09-17 05:03:44,185] Finished trial#20 with value: 0.6821455110887868 with parameters: {'num_leaves': 41}. Best is trial#15 with value: 0.682014511278877.\n",
      "\n",
      "num_leaves, val_score: 0.681872:  70%|#######   | 14/20 [04:11<01:37, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687506\n",
      "[400]\tvalid_0's binary_logloss: 0.68543\n",
      "[600]\tvalid_0's binary_logloss: 0.68389\n",
      "[800]\tvalid_0's binary_logloss: 0.682743\n",
      "[1000]\tvalid_0's binary_logloss: 0.682142\n",
      "[1200]\tvalid_0's binary_logloss: 0.681715\n",
      "[1400]\tvalid_0's binary_logloss: 0.681527\n",
      "Early stopping, best iteration is:\n",
      "[1411]\tvalid_0's binary_logloss: 0.681485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681485:  70%|#######   | 14/20 [04:27<01:37, 16.31s/it]\n",
      "num_leaves, val_score: 0.681485:  75%|#######5  | 15/20 [04:27<01:20, 16.09s/it][I 2020-09-17 05:03:59,749] Finished trial#21 with value: 0.6814854699032009 with parameters: {'num_leaves': 26}. Best is trial#21 with value: 0.6814854699032009.\n",
      "\n",
      "num_leaves, val_score: 0.681485:  75%|#######5  | 15/20 [04:27<01:20, 16.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688134\n",
      "[400]\tvalid_0's binary_logloss: 0.686319\n",
      "[600]\tvalid_0's binary_logloss: 0.684763\n",
      "[800]\tvalid_0's binary_logloss: 0.683704\n",
      "[1000]\tvalid_0's binary_logloss: 0.682932\n",
      "[1200]\tvalid_0's binary_logloss: 0.682718\n",
      "Early stopping, best iteration is:\n",
      "[1177]\tvalid_0's binary_logloss: 0.682689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681485:  75%|#######5  | 15/20 [04:30<01:20, 16.09s/it]\n",
      "num_leaves, val_score: 0.681485:  80%|########  | 16/20 [04:30<00:49, 12.27s/it][I 2020-09-17 05:04:03,117] Finished trial#22 with value: 0.6826887639704093 with parameters: {'num_leaves': 7}. Best is trial#21 with value: 0.6814854699032009.\n",
      "\n",
      "num_leaves, val_score: 0.681485:  80%|########  | 16/20 [04:30<00:49, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687408\n",
      "[400]\tvalid_0's binary_logloss: 0.685238\n",
      "[600]\tvalid_0's binary_logloss: 0.683546\n",
      "Early stopping, best iteration is:\n",
      "[696]\tvalid_0's binary_logloss: 0.682796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681485:  80%|########  | 16/20 [04:42<00:49, 12.27s/it]\n",
      "num_leaves, val_score: 0.681485:  85%|########5 | 17/20 [04:42<00:36, 12.02s/it][I 2020-09-17 05:04:14,560] Finished trial#23 with value: 0.682795775567991 with parameters: {'num_leaves': 83}. Best is trial#21 with value: 0.6814854699032009.\n",
      "\n",
      "num_leaves, val_score: 0.681485:  85%|########5 | 17/20 [04:42<00:36, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681485:  85%|########5 | 17/20 [05:00<00:36, 12.02s/it]\n",
      "num_leaves, val_score: 0.681485:  90%|######### | 18/20 [05:00<00:28, 14.07s/it][I 2020-09-17 05:04:33,407] Finished trial#24 with value: 0.6828486457363532 with parameters: {'num_leaves': 248}. Best is trial#21 with value: 0.6814854699032009.\n",
      "\n",
      "num_leaves, val_score: 0.681485:  90%|######### | 18/20 [05:00<00:28, 14.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687404\n",
      "[400]\tvalid_0's binary_logloss: 0.685234\n",
      "[600]\tvalid_0's binary_logloss: 0.683583\n",
      "[800]\tvalid_0's binary_logloss: 0.682906\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's binary_logloss: 0.682849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681485:  90%|######### | 18/20 [05:07<00:28, 14.07s/it]\n",
      "num_leaves, val_score: 0.681485:  95%|#########5| 19/20 [05:07<00:11, 11.95s/it][I 2020-09-17 05:04:40,396] Finished trial#25 with value: 0.6828486457363533 with parameters: {'num_leaves': 109}. Best is trial#21 with value: 0.6814854699032009.\n",
      "\n",
      "num_leaves, val_score: 0.681485:  95%|#########5| 19/20 [05:07<00:11, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687483\n",
      "[400]\tvalid_0's binary_logloss: 0.685434\n",
      "[600]\tvalid_0's binary_logloss: 0.683874\n",
      "[800]\tvalid_0's binary_logloss: 0.682907\n",
      "[1000]\tvalid_0's binary_logloss: 0.682357\n",
      "[1200]\tvalid_0's binary_logloss: 0.681879\n",
      "[1400]\tvalid_0's binary_logloss: 0.681675\n",
      "Early stopping, best iteration is:\n",
      "[1369]\tvalid_0's binary_logloss: 0.681656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.681485:  95%|#########5| 19/20 [05:20<00:11, 11.95s/it]\n",
      "num_leaves, val_score: 0.681485: 100%|##########| 20/20 [05:20<00:00, 12.20s/it][I 2020-09-17 05:04:53,187] Finished trial#26 with value: 0.6816562240641983 with parameters: {'num_leaves': 25}. Best is trial#21 with value: 0.6814854699032009.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "bagging, val_score: 0.681485:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687713\n",
      "[400]\tvalid_0's binary_logloss: 0.685557\n",
      "[600]\tvalid_0's binary_logloss: 0.684055\n",
      "[800]\tvalid_0's binary_logloss: 0.682966\n",
      "[1000]\tvalid_0's binary_logloss: 0.682505\n",
      "[1200]\tvalid_0's binary_logloss: 0.682046\n",
      "Early stopping, best iteration is:\n",
      "[1183]\tvalid_0's binary_logloss: 0.681976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681485:   0%|          | 0/10 [00:12<?, ?it/s]\n",
      "bagging, val_score: 0.681485:  10%|#         | 1/10 [00:12<01:50, 12.28s/it][I 2020-09-17 05:05:05,536] Finished trial#27 with value: 0.6819762025564786 with parameters: {'bagging_fraction': 0.8708593355061649, 'bagging_freq': 1}. Best is trial#27 with value: 0.6819762025564786.\n",
      "\n",
      "bagging, val_score: 0.681485:  10%|#         | 1/10 [00:12<01:50, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687754\n",
      "[400]\tvalid_0's binary_logloss: 0.685731\n",
      "[600]\tvalid_0's binary_logloss: 0.684141\n",
      "[800]\tvalid_0's binary_logloss: 0.683277\n",
      "[1000]\tvalid_0's binary_logloss: 0.682565\n",
      "[1200]\tvalid_0's binary_logloss: 0.682151\n",
      "[1400]\tvalid_0's binary_logloss: 0.681746\n",
      "[1600]\tvalid_0's binary_logloss: 0.681356\n",
      "Early stopping, best iteration is:\n",
      "[1669]\tvalid_0's binary_logloss: 0.681194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681194:  10%|#         | 1/10 [00:35<01:50, 12.28s/it]\n",
      "bagging, val_score: 0.681194:  20%|##        | 2/10 [00:35<02:04, 15.56s/it][I 2020-09-17 05:05:28,737] Finished trial#28 with value: 0.681194356738578 with parameters: {'bagging_fraction': 0.6723751885912492, 'bagging_freq': 3}. Best is trial#28 with value: 0.681194356738578.\n",
      "\n",
      "bagging, val_score: 0.681194:  20%|##        | 2/10 [00:35<02:04, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687609\n",
      "[400]\tvalid_0's binary_logloss: 0.685371\n",
      "[600]\tvalid_0's binary_logloss: 0.68348\n",
      "[800]\tvalid_0's binary_logloss: 0.682465\n",
      "[1000]\tvalid_0's binary_logloss: 0.6813\n",
      "Early stopping, best iteration is:\n",
      "[1005]\tvalid_0's binary_logloss: 0.68127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681194:  20%|##        | 2/10 [00:38<02:04, 15.56s/it]\n",
      "bagging, val_score: 0.681194:  30%|###       | 3/10 [00:38<01:23, 11.94s/it][I 2020-09-17 05:05:32,247] Finished trial#29 with value: 0.6812701377704273 with parameters: {'bagging_fraction': 0.7102000090585567, 'bagging_freq': 5}. Best is trial#28 with value: 0.681194356738578.\n",
      "\n",
      "bagging, val_score: 0.681194:  30%|###       | 3/10 [00:39<01:23, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687646\n",
      "[400]\tvalid_0's binary_logloss: 0.685235\n",
      "[600]\tvalid_0's binary_logloss: 0.683716\n",
      "[800]\tvalid_0's binary_logloss: 0.682428\n",
      "[1000]\tvalid_0's binary_logloss: 0.681606\n",
      "[1200]\tvalid_0's binary_logloss: 0.681336\n",
      "[1400]\tvalid_0's binary_logloss: 0.681103\n",
      "[1600]\tvalid_0's binary_logloss: 0.681058\n",
      "Early stopping, best iteration is:\n",
      "[1520]\tvalid_0's binary_logloss: 0.681004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681004:  30%|###       | 3/10 [00:51<01:23, 11.94s/it]\n",
      "bagging, val_score: 0.681004:  40%|####      | 4/10 [00:51<01:13, 12.17s/it][I 2020-09-17 05:05:44,948] Finished trial#30 with value: 0.6810036018057479 with parameters: {'bagging_fraction': 0.8525922961022108, 'bagging_freq': 1}. Best is trial#30 with value: 0.6810036018057479.\n",
      "\n",
      "bagging, val_score: 0.681004:  40%|####      | 4/10 [00:51<01:13, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687383\n",
      "[400]\tvalid_0's binary_logloss: 0.685524\n",
      "[600]\tvalid_0's binary_logloss: 0.684359\n",
      "[800]\tvalid_0's binary_logloss: 0.683389\n",
      "[1000]\tvalid_0's binary_logloss: 0.682774\n",
      "[1200]\tvalid_0's binary_logloss: 0.682436\n",
      "Early stopping, best iteration is:\n",
      "[1252]\tvalid_0's binary_logloss: 0.682279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681004:  40%|####      | 4/10 [01:03<01:13, 12.17s/it]\n",
      "bagging, val_score: 0.681004:  50%|#####     | 5/10 [01:03<01:00, 12.13s/it][I 2020-09-17 05:05:56,967] Finished trial#31 with value: 0.6822789453181454 with parameters: {'bagging_fraction': 0.6154790782495905, 'bagging_freq': 2}. Best is trial#30 with value: 0.6810036018057479.\n",
      "\n",
      "bagging, val_score: 0.681004:  50%|#####     | 5/10 [01:03<01:00, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688001\n",
      "[400]\tvalid_0's binary_logloss: 0.685884\n",
      "[600]\tvalid_0's binary_logloss: 0.684207\n",
      "[800]\tvalid_0's binary_logloss: 0.682987\n",
      "[1000]\tvalid_0's binary_logloss: 0.682083\n",
      "[1200]\tvalid_0's binary_logloss: 0.681626\n",
      "Early stopping, best iteration is:\n",
      "[1233]\tvalid_0's binary_logloss: 0.681429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681004:  50%|#####     | 5/10 [01:15<01:00, 12.13s/it]\n",
      "bagging, val_score: 0.681004:  60%|######    | 6/10 [01:15<00:48, 12.11s/it][I 2020-09-17 05:06:09,024] Finished trial#32 with value: 0.6814287952772208 with parameters: {'bagging_fraction': 0.5966980948991301, 'bagging_freq': 3}. Best is trial#30 with value: 0.6810036018057479.\n",
      "\n",
      "bagging, val_score: 0.681004:  60%|######    | 6/10 [01:15<00:48, 12.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688065\n",
      "[400]\tvalid_0's binary_logloss: 0.686171\n",
      "[600]\tvalid_0's binary_logloss: 0.684757\n",
      "[800]\tvalid_0's binary_logloss: 0.68382\n",
      "[1000]\tvalid_0's binary_logloss: 0.683353\n",
      "[1200]\tvalid_0's binary_logloss: 0.682902\n",
      "[1400]\tvalid_0's binary_logloss: 0.682764\n",
      "Early stopping, best iteration is:\n",
      "[1347]\tvalid_0's binary_logloss: 0.682549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681004:  60%|######    | 6/10 [01:28<00:48, 12.11s/it]\n",
      "bagging, val_score: 0.681004:  70%|#######   | 7/10 [01:28<00:36, 12.15s/it][I 2020-09-17 05:06:21,270] Finished trial#33 with value: 0.682548788604237 with parameters: {'bagging_fraction': 0.45609922958021565, 'bagging_freq': 7}. Best is trial#30 with value: 0.6810036018057479.\n",
      "\n",
      "bagging, val_score: 0.681004:  70%|#######   | 7/10 [01:28<00:36, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.6877\n",
      "[400]\tvalid_0's binary_logloss: 0.686031\n",
      "[600]\tvalid_0's binary_logloss: 0.684934\n",
      "[800]\tvalid_0's binary_logloss: 0.683837\n",
      "[1000]\tvalid_0's binary_logloss: 0.683633\n",
      "[1200]\tvalid_0's binary_logloss: 0.683174\n",
      "Early stopping, best iteration is:\n",
      "[1211]\tvalid_0's binary_logloss: 0.683075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681004:  70%|#######   | 7/10 [01:40<00:36, 12.15s/it]\n",
      "bagging, val_score: 0.681004:  80%|########  | 8/10 [01:40<00:24, 12.36s/it][I 2020-09-17 05:06:34,113] Finished trial#34 with value: 0.6830745212423973 with parameters: {'bagging_fraction': 0.5559188933225946, 'bagging_freq': 2}. Best is trial#30 with value: 0.6810036018057479.\n",
      "\n",
      "bagging, val_score: 0.681004:  80%|########  | 8/10 [01:40<00:24, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687364\n",
      "[400]\tvalid_0's binary_logloss: 0.685158\n",
      "[600]\tvalid_0's binary_logloss: 0.683852\n",
      "[800]\tvalid_0's binary_logloss: 0.682897\n",
      "[1000]\tvalid_0's binary_logloss: 0.682463\n",
      "[1200]\tvalid_0's binary_logloss: 0.682302\n",
      "Early stopping, best iteration is:\n",
      "[1186]\tvalid_0's binary_logloss: 0.6822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681004:  80%|########  | 8/10 [01:55<00:24, 12.36s/it]\n",
      "bagging, val_score: 0.681004:  90%|######### | 9/10 [01:55<00:12, 12.95s/it][I 2020-09-17 05:06:48,465] Finished trial#35 with value: 0.6821998178497182 with parameters: {'bagging_fraction': 0.6749932112002198, 'bagging_freq': 2}. Best is trial#30 with value: 0.6810036018057479.\n",
      "\n",
      "bagging, val_score: 0.681004:  90%|######### | 9/10 [01:55<00:12, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688463\n",
      "[400]\tvalid_0's binary_logloss: 0.6865\n",
      "[600]\tvalid_0's binary_logloss: 0.684971\n",
      "[800]\tvalid_0's binary_logloss: 0.684424\n",
      "[1000]\tvalid_0's binary_logloss: 0.683397\n",
      "Early stopping, best iteration is:\n",
      "[978]\tvalid_0's binary_logloss: 0.683252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.681004:  90%|######### | 9/10 [02:27<00:12, 12.95s/it]\n",
      "bagging, val_score: 0.681004: 100%|##########| 10/10 [02:27<00:00, 18.78s/it][I 2020-09-17 05:07:20,833] Finished trial#36 with value: 0.6832522366546518 with parameters: {'bagging_fraction': 0.46150082983423735, 'bagging_freq': 6}. Best is trial#30 with value: 0.6810036018057479.\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.681004:   0%|          | 0/6 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687205\n",
      "[400]\tvalid_0's binary_logloss: 0.685143\n",
      "[600]\tvalid_0's binary_logloss: 0.683747\n",
      "[800]\tvalid_0's binary_logloss: 0.68286\n",
      "[1000]\tvalid_0's binary_logloss: 0.682049\n",
      "[1200]\tvalid_0's binary_logloss: 0.681697\n",
      "[1400]\tvalid_0's binary_logloss: 0.68169\n",
      "[1600]\tvalid_0's binary_logloss: 0.681564\n",
      "Early stopping, best iteration is:\n",
      "[1521]\tvalid_0's binary_logloss: 0.681524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:   0%|          | 0/6 [00:42<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.681004:  17%|#6        | 1/6 [00:42<03:31, 42.27s/it][I 2020-09-17 05:08:03,170] Finished trial#37 with value: 0.6815242989530973 with parameters: {'feature_fraction': 0.58}. Best is trial#37 with value: 0.6815242989530973.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:  17%|#6        | 1/6 [00:42<03:31, 42.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687646\n",
      "[400]\tvalid_0's binary_logloss: 0.685235\n",
      "[600]\tvalid_0's binary_logloss: 0.683716\n",
      "[800]\tvalid_0's binary_logloss: 0.682428\n",
      "[1000]\tvalid_0's binary_logloss: 0.681606\n",
      "[1200]\tvalid_0's binary_logloss: 0.681336\n",
      "[1400]\tvalid_0's binary_logloss: 0.681103\n",
      "[1600]\tvalid_0's binary_logloss: 0.681058\n",
      "Early stopping, best iteration is:\n",
      "[1520]\tvalid_0's binary_logloss: 0.681004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:  17%|#6        | 1/6 [00:55<03:31, 42.27s/it]\n",
      "feature_fraction_stage2, val_score: 0.681004:  33%|###3      | 2/6 [00:55<02:14, 33.58s/it][I 2020-09-17 05:08:16,467] Finished trial#38 with value: 0.681003601805748 with parameters: {'feature_fraction': 0.484}. Best is trial#38 with value: 0.681003601805748.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:  33%|###3      | 2/6 [00:55<02:14, 33.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687717\n",
      "[400]\tvalid_0's binary_logloss: 0.685281\n",
      "[600]\tvalid_0's binary_logloss: 0.683945\n",
      "[800]\tvalid_0's binary_logloss: 0.682965\n",
      "[1000]\tvalid_0's binary_logloss: 0.681962\n",
      "[1200]\tvalid_0's binary_logloss: 0.68166\n",
      "[1400]\tvalid_0's binary_logloss: 0.681381\n",
      "[1600]\tvalid_0's binary_logloss: 0.681144\n",
      "Early stopping, best iteration is:\n",
      "[1532]\tvalid_0's binary_logloss: 0.681104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:  33%|###3      | 2/6 [01:08<02:14, 33.58s/it]\n",
      "feature_fraction_stage2, val_score: 0.681004:  50%|#####     | 3/6 [01:08<01:22, 27.38s/it][I 2020-09-17 05:08:29,383] Finished trial#39 with value: 0.6811039759894665 with parameters: {'feature_fraction': 0.42}. Best is trial#38 with value: 0.681003601805748.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:  50%|#####     | 3/6 [01:08<01:22, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687646\n",
      "[400]\tvalid_0's binary_logloss: 0.685235\n",
      "[600]\tvalid_0's binary_logloss: 0.683716\n",
      "[800]\tvalid_0's binary_logloss: 0.682428\n",
      "[1000]\tvalid_0's binary_logloss: 0.681606\n",
      "[1200]\tvalid_0's binary_logloss: 0.681336\n",
      "[1400]\tvalid_0's binary_logloss: 0.681103\n",
      "[1600]\tvalid_0's binary_logloss: 0.681058\n",
      "Early stopping, best iteration is:\n",
      "[1520]\tvalid_0's binary_logloss: 0.681004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:  50%|#####     | 3/6 [01:21<01:22, 27.38s/it]\n",
      "feature_fraction_stage2, val_score: 0.681004:  67%|######6   | 4/6 [01:21<00:46, 23.15s/it][I 2020-09-17 05:08:42,657] Finished trial#40 with value: 0.681003601805748 with parameters: {'feature_fraction': 0.516}. Best is trial#38 with value: 0.681003601805748.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.681004:  67%|######6   | 4/6 [01:21<00:46, 23.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687314\n",
      "[400]\tvalid_0's binary_logloss: 0.685015\n",
      "[600]\tvalid_0's binary_logloss: 0.683706\n",
      "[800]\tvalid_0's binary_logloss: 0.682476\n",
      "[1000]\tvalid_0's binary_logloss: 0.681682\n",
      "[1200]\tvalid_0's binary_logloss: 0.68116\n",
      "[1400]\tvalid_0's binary_logloss: 0.681029\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.680885:  67%|######6   | 4/6 [01:33<00:46, 23.15s/it]\n",
      "feature_fraction_stage2, val_score: 0.680885:  83%|########3 | 5/6 [01:33<00:19, 19.79s/it][I 2020-09-17 05:08:54,614] Finished trial#41 with value: 0.6808845324638358 with parameters: {'feature_fraction': 0.5479999999999999}. Best is trial#41 with value: 0.6808845324638358.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.680885:  83%|########3 | 5/6 [01:33<00:19, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687789\n",
      "[400]\tvalid_0's binary_logloss: 0.685426\n",
      "[600]\tvalid_0's binary_logloss: 0.683959\n",
      "[800]\tvalid_0's binary_logloss: 0.682794\n",
      "[1000]\tvalid_0's binary_logloss: 0.681961\n",
      "[1200]\tvalid_0's binary_logloss: 0.68186\n",
      "[1400]\tvalid_0's binary_logloss: 0.681644\n",
      "Early stopping, best iteration is:\n",
      "[1418]\tvalid_0's binary_logloss: 0.681535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.680885:  83%|########3 | 5/6 [01:50<00:19, 19.79s/it]\n",
      "feature_fraction_stage2, val_score: 0.680885: 100%|##########| 6/6 [01:50<00:00, 18.86s/it][I 2020-09-17 05:09:11,289] Finished trial#42 with value: 0.68153495790899 with parameters: {'feature_fraction': 0.45199999999999996}. Best is trial#41 with value: 0.6808845324638358.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "regularization_factors, val_score: 0.680885:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687328\n",
      "[400]\tvalid_0's binary_logloss: 0.685016\n",
      "[600]\tvalid_0's binary_logloss: 0.683695\n",
      "[800]\tvalid_0's binary_logloss: 0.682478\n",
      "[1000]\tvalid_0's binary_logloss: 0.681687\n",
      "[1200]\tvalid_0's binary_logloss: 0.681177\n",
      "[1400]\tvalid_0's binary_logloss: 0.681058\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680885:   0%|          | 0/20 [00:12<?, ?it/s]\n",
      "regularization_factors, val_score: 0.680885:   5%|5         | 1/20 [00:12<04:03, 12.80s/it][I 2020-09-17 05:09:24,149] Finished trial#43 with value: 0.6809241767124443 with parameters: {'lambda_l1': 0.00434600414194554, 'lambda_l2': 1.1127318951846695e-05}. Best is trial#43 with value: 0.6809241767124443.\n",
      "\n",
      "regularization_factors, val_score: 0.680885:   5%|5         | 1/20 [00:12<04:03, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687469\n",
      "[400]\tvalid_0's binary_logloss: 0.685113\n",
      "[600]\tvalid_0's binary_logloss: 0.683668\n",
      "[800]\tvalid_0's binary_logloss: 0.682532\n",
      "[1000]\tvalid_0's binary_logloss: 0.681649\n",
      "[1200]\tvalid_0's binary_logloss: 0.681139\n",
      "Early stopping, best iteration is:\n",
      "[1192]\tvalid_0's binary_logloss: 0.681112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680885:   5%|5         | 1/20 [00:25<04:03, 12.80s/it]\n",
      "regularization_factors, val_score: 0.680885:  10%|#         | 2/20 [00:25<03:50, 12.81s/it][I 2020-09-17 05:09:36,984] Finished trial#44 with value: 0.6811115197559727 with parameters: {'lambda_l1': 7.125242922430935e-08, 'lambda_l2': 0.8222925134557629}. Best is trial#43 with value: 0.6809241767124443.\n",
      "\n",
      "regularization_factors, val_score: 0.680885:  10%|#         | 2/20 [00:25<03:50, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687425\n",
      "[400]\tvalid_0's binary_logloss: 0.685046\n",
      "[600]\tvalid_0's binary_logloss: 0.68354\n",
      "[800]\tvalid_0's binary_logloss: 0.682344\n",
      "[1000]\tvalid_0's binary_logloss: 0.681501\n",
      "[1200]\tvalid_0's binary_logloss: 0.681155\n",
      "[1400]\tvalid_0's binary_logloss: 0.68096\n",
      "Early stopping, best iteration is:\n",
      "[1420]\tvalid_0's binary_logloss: 0.680832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  10%|#         | 2/20 [00:48<03:50, 12.81s/it]\n",
      "regularization_factors, val_score: 0.680832:  15%|#5        | 3/20 [00:48<04:27, 15.76s/it][I 2020-09-17 05:09:59,630] Finished trial#45 with value: 0.680832277873093 with parameters: {'lambda_l1': 0.5699491535575475, 'lambda_l2': 2.3058100709835316e-05}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  15%|#5        | 3/20 [00:48<04:27, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687482\n",
      "[400]\tvalid_0's binary_logloss: 0.68512\n",
      "[600]\tvalid_0's binary_logloss: 0.683707\n",
      "[800]\tvalid_0's binary_logloss: 0.682513\n",
      "[1000]\tvalid_0's binary_logloss: 0.681606\n",
      "[1200]\tvalid_0's binary_logloss: 0.681085\n",
      "Early stopping, best iteration is:\n",
      "[1192]\tvalid_0's binary_logloss: 0.681059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  15%|#5        | 3/20 [01:01<04:27, 15.76s/it]\n",
      "regularization_factors, val_score: 0.680832:  20%|##        | 4/20 [01:01<03:58, 14.88s/it][I 2020-09-17 05:10:12,449] Finished trial#46 with value: 0.6810593614711862 with parameters: {'lambda_l1': 0.0009999631454700544, 'lambda_l2': 0.7909379393993461}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  20%|##        | 4/20 [01:01<03:58, 14.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687314\n",
      "[400]\tvalid_0's binary_logloss: 0.685015\n",
      "[600]\tvalid_0's binary_logloss: 0.683706\n",
      "[800]\tvalid_0's binary_logloss: 0.682476\n",
      "[1000]\tvalid_0's binary_logloss: 0.681682\n",
      "[1200]\tvalid_0's binary_logloss: 0.68116\n",
      "[1400]\tvalid_0's binary_logloss: 0.681029\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  20%|##        | 4/20 [01:14<03:58, 14.88s/it]\n",
      "regularization_factors, val_score: 0.680832:  25%|##5       | 5/20 [01:14<03:37, 14.48s/it][I 2020-09-17 05:10:25,990] Finished trial#47 with value: 0.680887504638443 with parameters: {'lambda_l1': 9.585506756367786e-08, 'lambda_l2': 0.0001613300885057052}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  25%|##5       | 5/20 [01:14<03:37, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687314\n",
      "[400]\tvalid_0's binary_logloss: 0.685015\n",
      "[600]\tvalid_0's binary_logloss: 0.683706\n",
      "[800]\tvalid_0's binary_logloss: 0.682476\n",
      "[1000]\tvalid_0's binary_logloss: 0.681682\n",
      "[1200]\tvalid_0's binary_logloss: 0.68116\n",
      "[1400]\tvalid_0's binary_logloss: 0.681029\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  25%|##5       | 5/20 [01:27<03:37, 14.48s/it]\n",
      "regularization_factors, val_score: 0.680832:  30%|###       | 6/20 [01:27<03:15, 13.94s/it][I 2020-09-17 05:10:38,690] Finished trial#48 with value: 0.6808845325080635 with parameters: {'lambda_l1': 6.107696230831325e-07, 'lambda_l2': 7.288879201535211e-08}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  30%|###       | 6/20 [01:27<03:15, 13.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687314\n",
      "[400]\tvalid_0's binary_logloss: 0.685015\n",
      "[600]\tvalid_0's binary_logloss: 0.683706\n",
      "[800]\tvalid_0's binary_logloss: 0.682476\n",
      "[1000]\tvalid_0's binary_logloss: 0.681682\n",
      "[1200]\tvalid_0's binary_logloss: 0.68116\n",
      "[1400]\tvalid_0's binary_logloss: 0.681029\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  30%|###       | 6/20 [01:40<03:15, 13.94s/it]\n",
      "regularization_factors, val_score: 0.680832:  35%|###5      | 7/20 [01:40<02:57, 13.66s/it][I 2020-09-17 05:10:51,679] Finished trial#49 with value: 0.68088453434185 with parameters: {'lambda_l1': 2.420691892588743e-05, 'lambda_l2': 4.1048089693841883e-07}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  35%|###5      | 7/20 [01:40<02:57, 13.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68744\n",
      "[400]\tvalid_0's binary_logloss: 0.685221\n",
      "[600]\tvalid_0's binary_logloss: 0.683666\n",
      "[800]\tvalid_0's binary_logloss: 0.682473\n",
      "[1000]\tvalid_0's binary_logloss: 0.681643\n",
      "[1200]\tvalid_0's binary_logloss: 0.681298\n",
      "[1400]\tvalid_0's binary_logloss: 0.681103\n",
      "Early stopping, best iteration is:\n",
      "[1420]\tvalid_0's binary_logloss: 0.680975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  35%|###5      | 7/20 [01:53<02:57, 13.66s/it]\n",
      "regularization_factors, val_score: 0.680832:  40%|####      | 8/20 [01:53<02:42, 13.51s/it][I 2020-09-17 05:11:04,842] Finished trial#50 with value: 0.6809751593743202 with parameters: {'lambda_l1': 0.6766194676443029, 'lambda_l2': 0.014623887851339234}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  40%|####      | 8/20 [01:53<02:42, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68747\n",
      "[400]\tvalid_0's binary_logloss: 0.685085\n",
      "[600]\tvalid_0's binary_logloss: 0.683635\n",
      "[800]\tvalid_0's binary_logloss: 0.682451\n",
      "[1000]\tvalid_0's binary_logloss: 0.681523\n",
      "[1200]\tvalid_0's binary_logloss: 0.681008\n",
      "Early stopping, best iteration is:\n",
      "[1192]\tvalid_0's binary_logloss: 0.680983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  40%|####      | 8/20 [02:07<02:42, 13.51s/it]\n",
      "regularization_factors, val_score: 0.680832:  45%|####5     | 9/20 [02:07<02:30, 13.70s/it][I 2020-09-17 05:11:18,979] Finished trial#51 with value: 0.6809825659046302 with parameters: {'lambda_l1': 1.3441606346416646e-05, 'lambda_l2': 0.8020326184540503}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  45%|####5     | 9/20 [02:07<02:30, 13.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687314\n",
      "[400]\tvalid_0's binary_logloss: 0.685015\n",
      "[600]\tvalid_0's binary_logloss: 0.683706\n",
      "[800]\tvalid_0's binary_logloss: 0.682476\n",
      "[1000]\tvalid_0's binary_logloss: 0.681682\n",
      "[1200]\tvalid_0's binary_logloss: 0.68116\n",
      "[1400]\tvalid_0's binary_logloss: 0.681029\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680832:  45%|####5     | 9/20 [02:27<02:30, 13.70s/it]\n",
      "regularization_factors, val_score: 0.680832:  50%|#####     | 10/20 [02:27<02:35, 15.58s/it][I 2020-09-17 05:11:38,954] Finished trial#52 with value: 0.6808845324694985 with parameters: {'lambda_l1': 4.9081327178954904e-08, 'lambda_l2': 3.2423872727672226e-08}. Best is trial#45 with value: 0.680832277873093.\n",
      "\n",
      "regularization_factors, val_score: 0.680832:  50%|#####     | 10/20 [02:27<02:35, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687719\n",
      "[400]\tvalid_0's binary_logloss: 0.685362\n",
      "[600]\tvalid_0's binary_logloss: 0.683798\n",
      "[800]\tvalid_0's binary_logloss: 0.682453\n",
      "[1000]\tvalid_0's binary_logloss: 0.681557\n",
      "[1200]\tvalid_0's binary_logloss: 0.680624\n",
      "[1400]\tvalid_0's binary_logloss: 0.680392\n",
      "[1600]\tvalid_0's binary_logloss: 0.680236\n",
      "Early stopping, best iteration is:\n",
      "[1536]\tvalid_0's binary_logloss: 0.68018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.680180:  50%|#####     | 10/20 [02:43<02:35, 15.58s/it]\n",
      "regularization_factors, val_score: 0.680180:  55%|#####5    | 11/20 [02:43<02:22, 15.78s/it][I 2020-09-17 05:11:55,197] Finished trial#53 with value: 0.6801796158441621 with parameters: {'lambda_l1': 3.7574870026515863, 'lambda_l2': 0.0026387155572900707}. Best is trial#53 with value: 0.6801796158441621.\n",
      "\n",
      "regularization_factors, val_score: 0.680180:  55%|#####5    | 11/20 [02:43<02:22, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688061\n",
      "[400]\tvalid_0's binary_logloss: 0.685936\n",
      "[600]\tvalid_0's binary_logloss: 0.684231\n",
      "[800]\tvalid_0's binary_logloss: 0.683008\n",
      "[1000]\tvalid_0's binary_logloss: 0.682063\n",
      "[1200]\tvalid_0's binary_logloss: 0.681314\n",
      "[1400]\tvalid_0's binary_logloss: 0.681014\n",
      "[1600]\tvalid_0's binary_logloss: 0.680785\n",
      "[1800]\tvalid_0's binary_logloss: 0.680441\n",
      "[2000]\tvalid_0's binary_logloss: 0.680151\n",
      "[2200]\tvalid_0's binary_logloss: 0.680082\n",
      "[2400]\tvalid_0's binary_logloss: 0.680073\n",
      "Early stopping, best iteration is:\n",
      "[2309]\tvalid_0's binary_logloss: 0.679979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  55%|#####5    | 11/20 [02:59<02:22, 15.78s/it]\n",
      "regularization_factors, val_score: 0.679979:  60%|######    | 12/20 [02:59<02:05, 15.66s/it][I 2020-09-17 05:12:10,578] Finished trial#54 with value: 0.6799789339857202 with parameters: {'lambda_l1': 6.348648012781223, 'lambda_l2': 0.004384233880028371}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  60%|######    | 12/20 [02:59<02:05, 15.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688389\n",
      "[400]\tvalid_0's binary_logloss: 0.686495\n",
      "[600]\tvalid_0's binary_logloss: 0.685043\n",
      "[800]\tvalid_0's binary_logloss: 0.683992\n",
      "[1000]\tvalid_0's binary_logloss: 0.683113\n",
      "[1200]\tvalid_0's binary_logloss: 0.68244\n",
      "[1400]\tvalid_0's binary_logloss: 0.682103\n",
      "[1600]\tvalid_0's binary_logloss: 0.681755\n",
      "[1800]\tvalid_0's binary_logloss: 0.681437\n",
      "[2000]\tvalid_0's binary_logloss: 0.681181\n",
      "[2200]\tvalid_0's binary_logloss: 0.680953\n",
      "[2400]\tvalid_0's binary_logloss: 0.680773\n",
      "[2600]\tvalid_0's binary_logloss: 0.680527\n",
      "[2800]\tvalid_0's binary_logloss: 0.680464\n",
      "[3000]\tvalid_0's binary_logloss: 0.680422\n",
      "Early stopping, best iteration is:\n",
      "[2988]\tvalid_0's binary_logloss: 0.680401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  60%|######    | 12/20 [03:24<02:05, 15.66s/it]\n",
      "regularization_factors, val_score: 0.679979:  65%|######5   | 13/20 [03:24<02:09, 18.46s/it][I 2020-09-17 05:12:35,559] Finished trial#55 with value: 0.6804005296224911 with parameters: {'lambda_l1': 9.541253450461658, 'lambda_l2': 0.011270274954367696}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  65%|######5   | 13/20 [03:24<02:09, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687353\n",
      "[400]\tvalid_0's binary_logloss: 0.685075\n",
      "[600]\tvalid_0's binary_logloss: 0.683768\n",
      "[800]\tvalid_0's binary_logloss: 0.682515\n",
      "[1000]\tvalid_0's binary_logloss: 0.681729\n",
      "[1200]\tvalid_0's binary_logloss: 0.681234\n",
      "[1400]\tvalid_0's binary_logloss: 0.681101\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  65%|######5   | 13/20 [03:38<02:09, 18.46s/it]\n",
      "regularization_factors, val_score: 0.679979:  70%|#######   | 14/20 [03:38<01:43, 17.27s/it][I 2020-09-17 05:12:50,051] Finished trial#56 with value: 0.6809693747925524 with parameters: {'lambda_l1': 0.0486427695827361, 'lambda_l2': 0.005700276212938537}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  70%|#######   | 14/20 [03:38<01:43, 17.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688344\n",
      "[400]\tvalid_0's binary_logloss: 0.686413\n",
      "[600]\tvalid_0's binary_logloss: 0.684915\n",
      "[800]\tvalid_0's binary_logloss: 0.683874\n",
      "[1000]\tvalid_0's binary_logloss: 0.683003\n",
      "[1200]\tvalid_0's binary_logloss: 0.682323\n",
      "[1400]\tvalid_0's binary_logloss: 0.681981\n",
      "[1600]\tvalid_0's binary_logloss: 0.681644\n",
      "[1800]\tvalid_0's binary_logloss: 0.681312\n",
      "[2000]\tvalid_0's binary_logloss: 0.681101\n",
      "[2200]\tvalid_0's binary_logloss: 0.680943\n",
      "[2400]\tvalid_0's binary_logloss: 0.680719\n",
      "[2600]\tvalid_0's binary_logloss: 0.680491\n",
      "Early stopping, best iteration is:\n",
      "[2614]\tvalid_0's binary_logloss: 0.680475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  70%|#######   | 14/20 [04:03<01:43, 17.27s/it]\n",
      "regularization_factors, val_score: 0.679979:  75%|#######5  | 15/20 [04:03<01:37, 19.45s/it][I 2020-09-17 05:13:14,596] Finished trial#57 with value: 0.680474666946117 with parameters: {'lambda_l1': 8.87197207043605, 'lambda_l2': 0.0007799223716938442}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  75%|#######5  | 15/20 [04:03<01:37, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687432\n",
      "[400]\tvalid_0's binary_logloss: 0.685083\n",
      "[600]\tvalid_0's binary_logloss: 0.683668\n",
      "[800]\tvalid_0's binary_logloss: 0.682483\n",
      "[1000]\tvalid_0's binary_logloss: 0.681612\n",
      "[1200]\tvalid_0's binary_logloss: 0.681288\n",
      "Early stopping, best iteration is:\n",
      "[1192]\tvalid_0's binary_logloss: 0.681259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  75%|#######5  | 15/20 [04:15<01:37, 19.45s/it]\n",
      "regularization_factors, val_score: 0.679979:  80%|########  | 16/20 [04:15<01:08, 17.19s/it][I 2020-09-17 05:13:26,500] Finished trial#58 with value: 0.6812594303796592 with parameters: {'lambda_l1': 0.4749383620430354, 'lambda_l2': 0.054683757673112066}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  80%|########  | 16/20 [04:15<01:08, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687336\n",
      "[400]\tvalid_0's binary_logloss: 0.685065\n",
      "[600]\tvalid_0's binary_logloss: 0.683715\n",
      "[800]\tvalid_0's binary_logloss: 0.682448\n",
      "[1000]\tvalid_0's binary_logloss: 0.681687\n",
      "[1200]\tvalid_0's binary_logloss: 0.681195\n",
      "[1400]\tvalid_0's binary_logloss: 0.681035\n",
      "Early stopping, best iteration is:\n",
      "[1443]\tvalid_0's binary_logloss: 0.680899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  80%|########  | 16/20 [04:28<01:08, 17.19s/it]\n",
      "regularization_factors, val_score: 0.679979:  85%|########5 | 17/20 [04:28<00:47, 15.94s/it][I 2020-09-17 05:13:39,533] Finished trial#59 with value: 0.680899035038425 with parameters: {'lambda_l1': 0.030420645945953968, 'lambda_l2': 0.0005246772858980064}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  85%|########5 | 17/20 [04:28<00:47, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688196\n",
      "[400]\tvalid_0's binary_logloss: 0.686174\n",
      "[600]\tvalid_0's binary_logloss: 0.684611\n",
      "[800]\tvalid_0's binary_logloss: 0.683461\n",
      "[1000]\tvalid_0's binary_logloss: 0.682452\n",
      "[1200]\tvalid_0's binary_logloss: 0.681742\n",
      "[1400]\tvalid_0's binary_logloss: 0.681393\n",
      "[1600]\tvalid_0's binary_logloss: 0.681184\n",
      "[1800]\tvalid_0's binary_logloss: 0.680808\n",
      "[2000]\tvalid_0's binary_logloss: 0.680533\n",
      "[2200]\tvalid_0's binary_logloss: 0.680463\n",
      "[2400]\tvalid_0's binary_logloss: 0.680328\n",
      "[2600]\tvalid_0's binary_logloss: 0.680191\n",
      "[2800]\tvalid_0's binary_logloss: 0.680112\n",
      "[3000]\tvalid_0's binary_logloss: 0.680068\n",
      "Early stopping, best iteration is:\n",
      "[3019]\tvalid_0's binary_logloss: 0.680037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  85%|########5 | 17/20 [05:06<00:47, 15.94s/it]\n",
      "regularization_factors, val_score: 0.679979:  90%|######### | 18/20 [05:06<00:45, 22.78s/it][I 2020-09-17 05:14:18,285] Finished trial#60 with value: 0.680036885042385 with parameters: {'lambda_l1': 7.42833414049432, 'lambda_l2': 0.12913317261489252}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  90%|######### | 18/20 [05:06<00:45, 22.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687406\n",
      "[400]\tvalid_0's binary_logloss: 0.685084\n",
      "[600]\tvalid_0's binary_logloss: 0.683427\n",
      "[800]\tvalid_0's binary_logloss: 0.682331\n",
      "[1000]\tvalid_0's binary_logloss: 0.681561\n",
      "[1200]\tvalid_0's binary_logloss: 0.680944\n",
      "[1400]\tvalid_0's binary_logloss: 0.680616\n",
      "Early stopping, best iteration is:\n",
      "[1497]\tvalid_0's binary_logloss: 0.680458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  90%|######### | 18/20 [05:12<00:45, 22.78s/it]\n",
      "regularization_factors, val_score: 0.679979:  95%|#########5| 19/20 [05:12<00:17, 17.72s/it][I 2020-09-17 05:14:24,179] Finished trial#61 with value: 0.6804575463262327 with parameters: {'lambda_l1': 0.04646550411178458, 'lambda_l2': 4.479951718242561}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "regularization_factors, val_score: 0.679979:  95%|#########5| 19/20 [05:12<00:17, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688459\n",
      "[400]\tvalid_0's binary_logloss: 0.68657\n",
      "[600]\tvalid_0's binary_logloss: 0.685141\n",
      "[800]\tvalid_0's binary_logloss: 0.684069\n",
      "[1000]\tvalid_0's binary_logloss: 0.683187\n",
      "[1200]\tvalid_0's binary_logloss: 0.682561\n",
      "[1400]\tvalid_0's binary_logloss: 0.682222\n",
      "[1600]\tvalid_0's binary_logloss: 0.681848\n",
      "[1800]\tvalid_0's binary_logloss: 0.681541\n",
      "[2000]\tvalid_0's binary_logloss: 0.681292\n",
      "[2200]\tvalid_0's binary_logloss: 0.68108\n",
      "[2400]\tvalid_0's binary_logloss: 0.680917\n",
      "[2600]\tvalid_0's binary_logloss: 0.68064\n",
      "Early stopping, best iteration is:\n",
      "[2614]\tvalid_0's binary_logloss: 0.680628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.679979:  95%|#########5| 19/20 [05:23<00:17, 17.72s/it]\n",
      "regularization_factors, val_score: 0.679979: 100%|##########| 20/20 [05:23<00:00, 15.59s/it][I 2020-09-17 05:14:34,794] Finished trial#62 with value: 0.6806277085336239 with parameters: {'lambda_l1': 9.854810644646053, 'lambda_l2': 0.1792560382602303}. Best is trial#54 with value: 0.6799789339857202.\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.679979:   0%|          | 0/5 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688421\n",
      "[400]\tvalid_0's binary_logloss: 0.686686\n",
      "[600]\tvalid_0's binary_logloss: 0.685292\n",
      "[800]\tvalid_0's binary_logloss: 0.684305\n",
      "[1000]\tvalid_0's binary_logloss: 0.683483\n",
      "[1200]\tvalid_0's binary_logloss: 0.682918\n",
      "[1400]\tvalid_0's binary_logloss: 0.682595\n",
      "[1600]\tvalid_0's binary_logloss: 0.682445\n",
      "[1800]\tvalid_0's binary_logloss: 0.682133\n",
      "[2000]\tvalid_0's binary_logloss: 0.681978\n",
      "[2200]\tvalid_0's binary_logloss: 0.681895\n",
      "Early stopping, best iteration is:\n",
      "[2193]\tvalid_0's binary_logloss: 0.68187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.679979:   0%|          | 0/5 [00:09<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.679979:  20%|##        | 1/5 [00:09<00:38,  9.73s/it][I 2020-09-17 05:14:44,588] Finished trial#63 with value: 0.6818698645094122 with parameters: {'min_child_samples': 50}. Best is trial#63 with value: 0.6818698645094122.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  20%|##        | 1/5 [00:09<00:38,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687943\n",
      "[400]\tvalid_0's binary_logloss: 0.685749\n",
      "[600]\tvalid_0's binary_logloss: 0.684056\n",
      "[800]\tvalid_0's binary_logloss: 0.682876\n",
      "[1000]\tvalid_0's binary_logloss: 0.681981\n",
      "[1200]\tvalid_0's binary_logloss: 0.681214\n",
      "[1400]\tvalid_0's binary_logloss: 0.680841\n",
      "[1600]\tvalid_0's binary_logloss: 0.680703\n",
      "[1800]\tvalid_0's binary_logloss: 0.680464\n",
      "[2000]\tvalid_0's binary_logloss: 0.680181\n",
      "Early stopping, best iteration is:\n",
      "[2076]\tvalid_0's binary_logloss: 0.680093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  20%|##        | 1/5 [00:21<00:38,  9.73s/it]\n",
      "min_data_in_leaf, val_score: 0.679979:  40%|####      | 2/5 [00:21<00:30, 10.28s/it][I 2020-09-17 05:14:56,130] Finished trial#64 with value: 0.6800930392648886 with parameters: {'min_child_samples': 5}. Best is trial#64 with value: 0.6800930392648886.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  40%|####      | 2/5 [00:21<00:30, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688108\n",
      "[400]\tvalid_0's binary_logloss: 0.68607\n",
      "[600]\tvalid_0's binary_logloss: 0.684488\n",
      "[800]\tvalid_0's binary_logloss: 0.683337\n",
      "[1000]\tvalid_0's binary_logloss: 0.68242\n",
      "[1200]\tvalid_0's binary_logloss: 0.681784\n",
      "[1400]\tvalid_0's binary_logloss: 0.68143\n",
      "[1600]\tvalid_0's binary_logloss: 0.681221\n",
      "[1800]\tvalid_0's binary_logloss: 0.680894\n",
      "[2000]\tvalid_0's binary_logloss: 0.68068\n",
      "Early stopping, best iteration is:\n",
      "[1977]\tvalid_0's binary_logloss: 0.680637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  40%|####      | 2/5 [00:34<00:30, 10.28s/it]\n",
      "min_data_in_leaf, val_score: 0.679979:  60%|######    | 3/5 [00:34<00:22, 11.06s/it][I 2020-09-17 05:15:09,028] Finished trial#65 with value: 0.6806372212940234 with parameters: {'min_child_samples': 25}. Best is trial#64 with value: 0.6800930392648886.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  60%|######    | 3/5 [00:34<00:22, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687989\n",
      "[400]\tvalid_0's binary_logloss: 0.685801\n",
      "[600]\tvalid_0's binary_logloss: 0.684084\n",
      "[800]\tvalid_0's binary_logloss: 0.682886\n",
      "[1000]\tvalid_0's binary_logloss: 0.681975\n",
      "[1200]\tvalid_0's binary_logloss: 0.681248\n",
      "[1400]\tvalid_0's binary_logloss: 0.680937\n",
      "[1600]\tvalid_0's binary_logloss: 0.680852\n",
      "[1800]\tvalid_0's binary_logloss: 0.680507\n",
      "[2000]\tvalid_0's binary_logloss: 0.680184\n",
      "Early stopping, best iteration is:\n",
      "[2090]\tvalid_0's binary_logloss: 0.680098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  60%|######    | 3/5 [00:47<00:22, 11.06s/it]\n",
      "min_data_in_leaf, val_score: 0.679979:  80%|########  | 4/5 [00:47<00:11, 11.72s/it][I 2020-09-17 05:15:22,285] Finished trial#66 with value: 0.680098461305827 with parameters: {'min_child_samples': 10}. Best is trial#64 with value: 0.6800930392648886.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  80%|########  | 4/5 [00:47<00:11, 11.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68878\n",
      "[400]\tvalid_0's binary_logloss: 0.687201\n",
      "[600]\tvalid_0's binary_logloss: 0.686057\n",
      "[800]\tvalid_0's binary_logloss: 0.685337\n",
      "[1000]\tvalid_0's binary_logloss: 0.684687\n",
      "[1200]\tvalid_0's binary_logloss: 0.684237\n",
      "[1400]\tvalid_0's binary_logloss: 0.684026\n",
      "[1600]\tvalid_0's binary_logloss: 0.683795\n",
      "[1800]\tvalid_0's binary_logloss: 0.683538\n",
      "[2000]\tvalid_0's binary_logloss: 0.683434\n",
      "[2200]\tvalid_0's binary_logloss: 0.683388\n",
      "Early stopping, best iteration is:\n",
      "[2232]\tvalid_0's binary_logloss: 0.683361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.679979:  80%|########  | 4/5 [01:12<00:11, 11.72s/it]\n",
      "min_data_in_leaf, val_score: 0.679979: 100%|##########| 5/5 [01:12<00:00, 15.85s/it][I 2020-09-17 05:15:47,788] Finished trial#67 with value: 0.6833605010772068 with parameters: {'min_child_samples': 100}. Best is trial#64 with value: 0.6800930392648886.\n",
      "/home/tubotu/.local/lib/python3.6/site-packages/optuna/_experimental.py:90: ExperimentalWarning: train is experimental (supported from v0.18.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685556\n",
      "[400]\tvalid_0's binary_logloss: 0.682445\n",
      "[600]\tvalid_0's binary_logloss: 0.680833\n",
      "[800]\tvalid_0's binary_logloss: 0.680182\n",
      "[1000]\tvalid_0's binary_logloss: 0.679862\n",
      "Early stopping, best iteration is:\n",
      "[1065]\tvalid_0's binary_logloss: 0.679587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.679587:   0%|          | 0/7 [00:39<?, ?it/s]\n",
      "feature_fraction, val_score: 0.679587:  14%|#4        | 1/7 [00:39<03:58, 39.74s/it][I 2020-09-17 05:16:27,652] Finished trial#0 with value: 0.6795869194448889 with parameters: {'feature_fraction': 0.7}. Best is trial#0 with value: 0.6795869194448889.\n",
      "\n",
      "feature_fraction, val_score: 0.679587:  14%|#4        | 1/7 [00:39<03:58, 39.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68576\n",
      "[400]\tvalid_0's binary_logloss: 0.682813\n",
      "[600]\tvalid_0's binary_logloss: 0.680884\n",
      "[800]\tvalid_0's binary_logloss: 0.680011\n",
      "[1000]\tvalid_0's binary_logloss: 0.679607\n",
      "[1200]\tvalid_0's binary_logloss: 0.679234\n",
      "Early stopping, best iteration is:\n",
      "[1253]\tvalid_0's binary_logloss: 0.679167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.679167:  14%|#4        | 1/7 [01:13<03:58, 39.74s/it]\n",
      "feature_fraction, val_score: 0.679167:  29%|##8       | 2/7 [01:13<03:09, 37.82s/it][I 2020-09-17 05:17:00,981] Finished trial#1 with value: 0.67916674043927 with parameters: {'feature_fraction': 0.6}. Best is trial#1 with value: 0.67916674043927.\n",
      "\n",
      "feature_fraction, val_score: 0.679167:  29%|##8       | 2/7 [01:13<03:09, 37.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685476\n",
      "[400]\tvalid_0's binary_logloss: 0.682024\n",
      "[600]\tvalid_0's binary_logloss: 0.680173\n",
      "[800]\tvalid_0's binary_logloss: 0.679549\n",
      "[1000]\tvalid_0's binary_logloss: 0.678982\n",
      "[1200]\tvalid_0's binary_logloss: 0.678944\n",
      "Early stopping, best iteration is:\n",
      "[1123]\tvalid_0's binary_logloss: 0.678724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678724:  29%|##8       | 2/7 [01:20<03:09, 37.82s/it]\n",
      "feature_fraction, val_score: 0.678724:  43%|####2     | 3/7 [01:20<01:54, 28.66s/it][I 2020-09-17 05:17:08,266] Finished trial#2 with value: 0.6787242421037718 with parameters: {'feature_fraction': 0.8999999999999999}. Best is trial#2 with value: 0.6787242421037718.\n",
      "\n",
      "feature_fraction, val_score: 0.678724:  43%|####2     | 3/7 [01:20<01:54, 28.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685562\n",
      "[400]\tvalid_0's binary_logloss: 0.682226\n",
      "[600]\tvalid_0's binary_logloss: 0.680374\n",
      "[800]\tvalid_0's binary_logloss: 0.679919\n",
      "[1000]\tvalid_0's binary_logloss: 0.679247\n",
      "[1200]\tvalid_0's binary_logloss: 0.678597\n",
      "[1400]\tvalid_0's binary_logloss: 0.678404\n",
      "Early stopping, best iteration is:\n",
      "[1377]\tvalid_0's binary_logloss: 0.678338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678338:  43%|####2     | 3/7 [01:31<01:54, 28.66s/it]\n",
      "feature_fraction, val_score: 0.678338:  57%|#####7    | 4/7 [01:31<01:09, 23.31s/it][I 2020-09-17 05:17:19,091] Finished trial#3 with value: 0.678338331154298 with parameters: {'feature_fraction': 0.8}. Best is trial#3 with value: 0.678338331154298.\n",
      "\n",
      "feature_fraction, val_score: 0.678338:  57%|#####7    | 4/7 [01:31<01:09, 23.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68662\n",
      "[400]\tvalid_0's binary_logloss: 0.683955\n",
      "[600]\tvalid_0's binary_logloss: 0.681619\n",
      "[800]\tvalid_0's binary_logloss: 0.680445\n",
      "[1000]\tvalid_0's binary_logloss: 0.679426\n",
      "Early stopping, best iteration is:\n",
      "[997]\tvalid_0's binary_logloss: 0.67941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678338:  57%|#####7    | 4/7 [01:35<01:09, 23.31s/it]\n",
      "feature_fraction, val_score: 0.678338:  71%|#######1  | 5/7 [01:35<00:35, 17.73s/it][I 2020-09-17 05:17:23,818] Finished trial#4 with value: 0.6794096679651961 with parameters: {'feature_fraction': 0.5}. Best is trial#3 with value: 0.678338331154298.\n",
      "\n",
      "feature_fraction, val_score: 0.678338:  71%|#######1  | 5/7 [01:35<00:35, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68481\n",
      "[400]\tvalid_0's binary_logloss: 0.682394\n",
      "[600]\tvalid_0's binary_logloss: 0.680452\n",
      "[800]\tvalid_0's binary_logloss: 0.679765\n",
      "[1000]\tvalid_0's binary_logloss: 0.679283\n",
      "[1200]\tvalid_0's binary_logloss: 0.678254\n",
      "[1400]\tvalid_0's binary_logloss: 0.678127\n",
      "Early stopping, best iteration is:\n",
      "[1403]\tvalid_0's binary_logloss: 0.67809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678090:  71%|#######1  | 5/7 [01:42<00:35, 17.73s/it]\n",
      "feature_fraction, val_score: 0.678090:  86%|########5 | 6/7 [01:42<00:14, 14.42s/it][I 2020-09-17 05:17:30,510] Finished trial#5 with value: 0.6780899517189316 with parameters: {'feature_fraction': 1.0}. Best is trial#5 with value: 0.6780899517189316.\n",
      "\n",
      "feature_fraction, val_score: 0.678090:  86%|########5 | 6/7 [01:42<00:14, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687285\n",
      "[400]\tvalid_0's binary_logloss: 0.684634\n",
      "[600]\tvalid_0's binary_logloss: 0.683123\n",
      "[800]\tvalid_0's binary_logloss: 0.681684\n",
      "[1000]\tvalid_0's binary_logloss: 0.680853\n",
      "[1200]\tvalid_0's binary_logloss: 0.680361\n",
      "[1400]\tvalid_0's binary_logloss: 0.680313\n",
      "Early stopping, best iteration is:\n",
      "[1317]\tvalid_0's binary_logloss: 0.680183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction, val_score: 0.678090:  86%|########5 | 6/7 [01:47<00:14, 14.42s/it]\n",
      "feature_fraction, val_score: 0.678090: 100%|##########| 7/7 [01:47<00:00, 11.59s/it][I 2020-09-17 05:17:35,511] Finished trial#6 with value: 0.6801825076526521 with parameters: {'feature_fraction': 0.4}. Best is trial#5 with value: 0.6780899517189316.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "num_leaves, val_score: 0.678090:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:   0%|          | 0/20 [00:04<?, ?it/s]\n",
      "num_leaves, val_score: 0.678090:   5%|5         | 1/20 [00:04<01:21,  4.27s/it][I 2020-09-17 05:17:39,837] Finished trial#7 with value: 0.6812338108038722 with parameters: {'num_leaves': 226}. Best is trial#7 with value: 0.6812338108038722.\n",
      "\n",
      "num_leaves, val_score: 0.678090:   5%|5         | 1/20 [00:04<01:21,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:   5%|5         | 1/20 [00:08<01:21,  4.27s/it]\n",
      "num_leaves, val_score: 0.678090:  10%|#         | 2/20 [00:08<01:16,  4.24s/it][I 2020-09-17 05:17:44,011] Finished trial#8 with value: 0.6812338108038722 with parameters: {'num_leaves': 87}. Best is trial#7 with value: 0.6812338108038722.\n",
      "\n",
      "num_leaves, val_score: 0.678090:  10%|#         | 2/20 [00:08<01:16,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:  10%|#         | 2/20 [00:12<01:16,  4.24s/it]\n",
      "num_leaves, val_score: 0.678090:  15%|#5        | 3/20 [00:12<01:13,  4.31s/it][I 2020-09-17 05:17:48,475] Finished trial#9 with value: 0.6812338108038722 with parameters: {'num_leaves': 127}. Best is trial#7 with value: 0.6812338108038722.\n",
      "\n",
      "num_leaves, val_score: 0.678090:  15%|#5        | 3/20 [00:12<01:13,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:  15%|#5        | 3/20 [00:17<01:13,  4.31s/it]\n",
      "num_leaves, val_score: 0.678090:  20%|##        | 4/20 [00:17<01:09,  4.32s/it][I 2020-09-17 05:17:52,824] Finished trial#10 with value: 0.6812338108038722 with parameters: {'num_leaves': 106}. Best is trial#7 with value: 0.6812338108038722.\n",
      "\n",
      "num_leaves, val_score: 0.678090:  20%|##        | 4/20 [00:17<01:09,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:  20%|##        | 4/20 [00:21<01:09,  4.32s/it]\n",
      "num_leaves, val_score: 0.678090:  25%|##5       | 5/20 [00:21<01:04,  4.30s/it][I 2020-09-17 05:17:57,084] Finished trial#11 with value: 0.6812338108038721 with parameters: {'num_leaves': 163}. Best is trial#11 with value: 0.6812338108038721.\n",
      "\n",
      "num_leaves, val_score: 0.678090:  25%|##5       | 5/20 [00:21<01:04,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:  25%|##5       | 5/20 [00:25<01:04,  4.30s/it]\n",
      "num_leaves, val_score: 0.678090:  30%|###       | 6/20 [00:25<00:59,  4.28s/it][I 2020-09-17 05:18:01,331] Finished trial#12 with value: 0.6812338108038722 with parameters: {'num_leaves': 246}. Best is trial#11 with value: 0.6812338108038721.\n",
      "\n",
      "num_leaves, val_score: 0.678090:  30%|###       | 6/20 [00:25<00:59,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:  30%|###       | 6/20 [00:29<00:59,  4.28s/it]\n",
      "num_leaves, val_score: 0.678090:  35%|###5      | 7/20 [00:29<00:55,  4.24s/it][I 2020-09-17 05:18:05,474] Finished trial#13 with value: 0.6812338108038721 with parameters: {'num_leaves': 103}. Best is trial#11 with value: 0.6812338108038721.\n",
      "\n",
      "num_leaves, val_score: 0.678090:  35%|###5      | 7/20 [00:29<00:55,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.678090:  35%|###5      | 7/20 [00:34<00:55,  4.24s/it]\n",
      "num_leaves, val_score: 0.678090:  40%|####      | 8/20 [00:34<00:52,  4.40s/it][I 2020-09-17 05:18:10,251] Finished trial#14 with value: 0.6812338108038722 with parameters: {'num_leaves': 222}. Best is trial#11 with value: 0.6812338108038721.\n",
      "\n",
      "num_leaves, val_score: 0.678090:  40%|####      | 8/20 [00:34<00:52,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684581\n",
      "[400]\tvalid_0's binary_logloss: 0.680947\n",
      "[600]\tvalid_0's binary_logloss: 0.679929\n",
      "[800]\tvalid_0's binary_logloss: 0.679109\n",
      "[1000]\tvalid_0's binary_logloss: 0.678836\n",
      "[1200]\tvalid_0's binary_logloss: 0.677656\n",
      "Early stopping, best iteration is:\n",
      "[1219]\tvalid_0's binary_logloss: 0.677491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.677491:  40%|####      | 8/20 [00:40<00:52,  4.40s/it]\n",
      "num_leaves, val_score: 0.677491:  45%|####5     | 9/20 [00:40<00:54,  4.97s/it][I 2020-09-17 05:18:16,551] Finished trial#15 with value: 0.6774906928820349 with parameters: {'num_leaves': 49}. Best is trial#15 with value: 0.6774906928820349.\n",
      "\n",
      "num_leaves, val_score: 0.677491:  45%|####5     | 9/20 [00:41<00:54,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684533\n",
      "[400]\tvalid_0's binary_logloss: 0.681741\n",
      "[600]\tvalid_0's binary_logloss: 0.681517\n",
      "Early stopping, best iteration is:\n",
      "[544]\tvalid_0's binary_logloss: 0.681234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.677491:  45%|####5     | 9/20 [00:45<00:54,  4.97s/it]\n",
      "num_leaves, val_score: 0.677491:  50%|#####     | 10/20 [00:45<00:48,  4.86s/it][I 2020-09-17 05:18:21,163] Finished trial#16 with value: 0.6812338108038722 with parameters: {'num_leaves': 165}. Best is trial#15 with value: 0.6774906928820349.\n",
      "\n",
      "num_leaves, val_score: 0.677491:  50%|#####     | 10/20 [00:45<00:48,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.688779\n",
      "[400]\tvalid_0's binary_logloss: 0.687026\n",
      "[600]\tvalid_0's binary_logloss: 0.685427\n",
      "[800]\tvalid_0's binary_logloss: 0.684166\n",
      "[1000]\tvalid_0's binary_logloss: 0.68315\n",
      "[1200]\tvalid_0's binary_logloss: 0.682346\n",
      "[1400]\tvalid_0's binary_logloss: 0.681565\n",
      "[1600]\tvalid_0's binary_logloss: 0.681011\n",
      "[1800]\tvalid_0's binary_logloss: 0.680556\n",
      "[2000]\tvalid_0's binary_logloss: 0.68012\n",
      "[2200]\tvalid_0's binary_logloss: 0.679722\n",
      "[2400]\tvalid_0's binary_logloss: 0.679307\n",
      "[2600]\tvalid_0's binary_logloss: 0.678965\n",
      "[2800]\tvalid_0's binary_logloss: 0.678755\n",
      "[3000]\tvalid_0's binary_logloss: 0.678689\n",
      "Early stopping, best iteration is:\n",
      "[3003]\tvalid_0's binary_logloss: 0.678678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.677491:  50%|#####     | 10/20 [00:52<00:48,  4.86s/it]\n",
      "num_leaves, val_score: 0.677491:  55%|#####5    | 11/20 [00:52<00:48,  5.38s/it][I 2020-09-17 05:18:27,744] Finished trial#17 with value: 0.6786777693040912 with parameters: {'num_leaves': 3}. Best is trial#15 with value: 0.6774906928820349.\n",
      "\n",
      "num_leaves, val_score: 0.677491:  55%|#####5    | 11/20 [00:52<00:48,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68759\n",
      "[400]\tvalid_0's binary_logloss: 0.685246\n",
      "[600]\tvalid_0's binary_logloss: 0.683484\n",
      "[800]\tvalid_0's binary_logloss: 0.682166\n",
      "[1000]\tvalid_0's binary_logloss: 0.681066\n",
      "[1200]\tvalid_0's binary_logloss: 0.680236\n",
      "[1400]\tvalid_0's binary_logloss: 0.679486\n",
      "[1600]\tvalid_0's binary_logloss: 0.678845\n",
      "[1800]\tvalid_0's binary_logloss: 0.67828\n",
      "[2000]\tvalid_0's binary_logloss: 0.677742\n",
      "[2200]\tvalid_0's binary_logloss: 0.677343\n",
      "[2400]\tvalid_0's binary_logloss: 0.677044\n",
      "[2600]\tvalid_0's binary_logloss: 0.676879\n",
      "Early stopping, best iteration is:\n",
      "[2686]\tvalid_0's binary_logloss: 0.676858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.676858:  55%|#####5    | 11/20 [00:58<00:48,  5.38s/it]\n",
      "num_leaves, val_score: 0.676858:  60%|######    | 12/20 [00:58<00:45,  5.68s/it][I 2020-09-17 05:18:34,128] Finished trial#18 with value: 0.6768580902114057 with parameters: {'num_leaves': 4}. Best is trial#18 with value: 0.6768580902114057.\n",
      "\n",
      "num_leaves, val_score: 0.676858:  60%|######    | 12/20 [00:58<00:45,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689904\n",
      "[400]\tvalid_0's binary_logloss: 0.68881\n",
      "[600]\tvalid_0's binary_logloss: 0.68785\n",
      "[800]\tvalid_0's binary_logloss: 0.68696\n",
      "[1000]\tvalid_0's binary_logloss: 0.686243\n",
      "[1200]\tvalid_0's binary_logloss: 0.685595\n",
      "[1400]\tvalid_0's binary_logloss: 0.684978\n",
      "[1600]\tvalid_0's binary_logloss: 0.68442\n",
      "[1800]\tvalid_0's binary_logloss: 0.683913\n",
      "[2000]\tvalid_0's binary_logloss: 0.683451\n",
      "[2200]\tvalid_0's binary_logloss: 0.683181\n",
      "[2400]\tvalid_0's binary_logloss: 0.68299\n",
      "[2600]\tvalid_0's binary_logloss: 0.68282\n",
      "[2800]\tvalid_0's binary_logloss: 0.68264\n",
      "[3000]\tvalid_0's binary_logloss: 0.682444\n",
      "[3200]\tvalid_0's binary_logloss: 0.682266\n",
      "[3400]\tvalid_0's binary_logloss: 0.682101\n",
      "[3600]\tvalid_0's binary_logloss: 0.681947\n",
      "[3800]\tvalid_0's binary_logloss: 0.681875\n",
      "Early stopping, best iteration is:\n",
      "[3871]\tvalid_0's binary_logloss: 0.681865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.676858:  60%|######    | 12/20 [01:06<00:45,  5.68s/it]\n",
      "num_leaves, val_score: 0.676858:  65%|######5   | 13/20 [01:06<00:44,  6.36s/it][I 2020-09-17 05:18:42,083] Finished trial#19 with value: 0.6818645334656162 with parameters: {'num_leaves': 2}. Best is trial#18 with value: 0.6768580902114057.\n",
      "\n",
      "num_leaves, val_score: 0.676858:  65%|######5   | 13/20 [01:06<00:44,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684663\n",
      "[400]\tvalid_0's binary_logloss: 0.68043\n",
      "[600]\tvalid_0's binary_logloss: 0.679585\n",
      "[800]\tvalid_0's binary_logloss: 0.679074\n",
      "[1000]\tvalid_0's binary_logloss: 0.678748\n",
      "[1200]\tvalid_0's binary_logloss: 0.677372\n",
      "Early stopping, best iteration is:\n",
      "[1231]\tvalid_0's binary_logloss: 0.677204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.676858:  65%|######5   | 13/20 [01:12<00:44,  6.36s/it]\n",
      "num_leaves, val_score: 0.676858:  70%|#######   | 14/20 [01:12<00:37,  6.32s/it][I 2020-09-17 05:18:48,286] Finished trial#20 with value: 0.6772042071134127 with parameters: {'num_leaves': 46}. Best is trial#18 with value: 0.6768580902114057.\n",
      "\n",
      "num_leaves, val_score: 0.676858:  70%|#######   | 14/20 [01:12<00:37,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684785\n",
      "[400]\tvalid_0's binary_logloss: 0.681158\n",
      "[600]\tvalid_0's binary_logloss: 0.67946\n",
      "[800]\tvalid_0's binary_logloss: 0.67861\n",
      "Early stopping, best iteration is:\n",
      "[859]\tvalid_0's binary_logloss: 0.678603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.676858:  70%|#######   | 14/20 [01:17<00:37,  6.32s/it]\n",
      "num_leaves, val_score: 0.676858:  75%|#######5  | 15/20 [01:17<00:28,  5.76s/it][I 2020-09-17 05:18:52,754] Finished trial#21 with value: 0.6786025792615176 with parameters: {'num_leaves': 41}. Best is trial#18 with value: 0.6768580902114057.\n",
      "\n",
      "num_leaves, val_score: 0.676858:  75%|#######5  | 15/20 [01:17<00:28,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684686\n",
      "[400]\tvalid_0's binary_logloss: 0.68205\n",
      "[600]\tvalid_0's binary_logloss: 0.679687\n",
      "[800]\tvalid_0's binary_logloss: 0.67907\n",
      "Early stopping, best iteration is:\n",
      "[738]\tvalid_0's binary_logloss: 0.678925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.676858:  75%|#######5  | 15/20 [01:20<00:28,  5.76s/it]\n",
      "num_leaves, val_score: 0.676858:  80%|########  | 16/20 [01:20<00:20,  5.11s/it][I 2020-09-17 05:18:56,338] Finished trial#22 with value: 0.6789252304786852 with parameters: {'num_leaves': 33}. Best is trial#18 with value: 0.6768580902114057.\n",
      "\n",
      "num_leaves, val_score: 0.676858:  80%|########  | 16/20 [01:20<00:20,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684655\n",
      "[400]\tvalid_0's binary_logloss: 0.681844\n",
      "Early stopping, best iteration is:\n",
      "[479]\tvalid_0's binary_logloss: 0.681346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.676858:  80%|########  | 16/20 [01:24<00:20,  5.11s/it]\n",
      "num_leaves, val_score: 0.676858:  85%|########5 | 17/20 [01:24<00:13,  4.55s/it][I 2020-09-17 05:18:59,596] Finished trial#23 with value: 0.6813459228652599 with parameters: {'num_leaves': 69}. Best is trial#18 with value: 0.6768580902114057.\n",
      "\n",
      "num_leaves, val_score: 0.676858:  85%|########5 | 17/20 [01:24<00:13,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686029\n",
      "[400]\tvalid_0's binary_logloss: 0.683062\n",
      "[600]\tvalid_0's binary_logloss: 0.68133\n",
      "[800]\tvalid_0's binary_logloss: 0.679555\n",
      "[1000]\tvalid_0's binary_logloss: 0.678255\n",
      "[1200]\tvalid_0's binary_logloss: 0.677302\n",
      "[1400]\tvalid_0's binary_logloss: 0.676891\n",
      "[1600]\tvalid_0's binary_logloss: 0.676485\n",
      "[1800]\tvalid_0's binary_logloss: 0.676108\n",
      "[2000]\tvalid_0's binary_logloss: 0.675897\n",
      "[2200]\tvalid_0's binary_logloss: 0.675719\n",
      "Early stopping, best iteration is:\n",
      "[2201]\tvalid_0's binary_logloss: 0.675706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675706:  85%|########5 | 17/20 [01:29<00:13,  4.55s/it]\n",
      "num_leaves, val_score: 0.675706:  90%|######### | 18/20 [01:29<00:09,  4.94s/it][I 2020-09-17 05:19:05,445] Finished trial#24 with value: 0.6757064439527763 with parameters: {'num_leaves': 8}. Best is trial#24 with value: 0.6757064439527763.\n",
      "\n",
      "num_leaves, val_score: 0.675706:  90%|######### | 18/20 [01:29<00:09,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.689904\n",
      "[400]\tvalid_0's binary_logloss: 0.68881\n",
      "[600]\tvalid_0's binary_logloss: 0.68785\n",
      "[800]\tvalid_0's binary_logloss: 0.68696\n",
      "[1000]\tvalid_0's binary_logloss: 0.686243\n",
      "[1200]\tvalid_0's binary_logloss: 0.685595\n",
      "[1400]\tvalid_0's binary_logloss: 0.684978\n",
      "[1600]\tvalid_0's binary_logloss: 0.68442\n",
      "[1800]\tvalid_0's binary_logloss: 0.683913\n",
      "[2000]\tvalid_0's binary_logloss: 0.683451\n",
      "[2200]\tvalid_0's binary_logloss: 0.683181\n",
      "[2400]\tvalid_0's binary_logloss: 0.68299\n",
      "[2600]\tvalid_0's binary_logloss: 0.68282\n",
      "[2800]\tvalid_0's binary_logloss: 0.68264\n",
      "[3000]\tvalid_0's binary_logloss: 0.682444\n",
      "[3200]\tvalid_0's binary_logloss: 0.682266\n",
      "[3400]\tvalid_0's binary_logloss: 0.682101\n",
      "[3600]\tvalid_0's binary_logloss: 0.681947\n",
      "[3800]\tvalid_0's binary_logloss: 0.681875\n",
      "Early stopping, best iteration is:\n",
      "[3871]\tvalid_0's binary_logloss: 0.681865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.675706:  90%|######### | 18/20 [01:37<00:09,  4.94s/it]\n",
      "num_leaves, val_score: 0.675706:  95%|#########5| 19/20 [01:37<00:05,  5.83s/it][I 2020-09-17 05:19:13,350] Finished trial#25 with value: 0.6818645334656162 with parameters: {'num_leaves': 2}. Best is trial#24 with value: 0.6757064439527763.\n",
      "\n",
      "num_leaves, val_score: 0.675706:  95%|#########5| 19/20 [01:37<00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679783\n",
      "[800]\tvalid_0's binary_logloss: 0.678562\n",
      "[1000]\tvalid_0's binary_logloss: 0.676981\n",
      "[1200]\tvalid_0's binary_logloss: 0.675748\n",
      "[1400]\tvalid_0's binary_logloss: 0.674691\n",
      "[1600]\tvalid_0's binary_logloss: 0.674524\n",
      "Early stopping, best iteration is:\n",
      "[1542]\tvalid_0's binary_logloss: 0.674334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "num_leaves, val_score: 0.674334:  95%|#########5| 19/20 [01:43<00:05,  5.83s/it]\n",
      "num_leaves, val_score: 0.674334: 100%|##########| 20/20 [01:43<00:00,  5.79s/it][I 2020-09-17 05:19:19,045] Finished trial#26 with value: 0.6743339445533021 with parameters: {'num_leaves': 24}. Best is trial#26 with value: 0.6743339445533021.\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      "bagging, val_score: 0.674334:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687688\n",
      "[400]\tvalid_0's binary_logloss: 0.685886\n",
      "[600]\tvalid_0's binary_logloss: 0.684992\n",
      "[800]\tvalid_0's binary_logloss: 0.683901\n",
      "Early stopping, best iteration is:\n",
      "[791]\tvalid_0's binary_logloss: 0.683825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:   0%|          | 0/10 [00:03<?, ?it/s]\n",
      "bagging, val_score: 0.674334:  10%|#         | 1/10 [00:03<00:30,  3.35s/it][I 2020-09-17 05:19:22,451] Finished trial#27 with value: 0.6838247309373521 with parameters: {'bagging_fraction': 0.5586555300060838, 'bagging_freq': 7}. Best is trial#27 with value: 0.6838247309373521.\n",
      "\n",
      "bagging, val_score: 0.674334:  10%|#         | 1/10 [00:03<00:30,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686447\n",
      "[400]\tvalid_0's binary_logloss: 0.684569\n",
      "[600]\tvalid_0's binary_logloss: 0.683313\n",
      "[800]\tvalid_0's binary_logloss: 0.682117\n",
      "Early stopping, best iteration is:\n",
      "[805]\tvalid_0's binary_logloss: 0.682056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  10%|#         | 1/10 [00:07<00:30,  3.35s/it]\n",
      "bagging, val_score: 0.674334:  20%|##        | 2/10 [00:07<00:27,  3.45s/it][I 2020-09-17 05:19:26,131] Finished trial#28 with value: 0.6820563226394306 with parameters: {'bagging_fraction': 0.7503773630896782, 'bagging_freq': 7}. Best is trial#28 with value: 0.6820563226394306.\n",
      "\n",
      "bagging, val_score: 0.674334:  20%|##        | 2/10 [00:07<00:27,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686409\n",
      "[400]\tvalid_0's binary_logloss: 0.683223\n",
      "[600]\tvalid_0's binary_logloss: 0.681463\n",
      "[800]\tvalid_0's binary_logloss: 0.680522\n",
      "[1000]\tvalid_0's binary_logloss: 0.680102\n",
      "Early stopping, best iteration is:\n",
      "[1086]\tvalid_0's binary_logloss: 0.679802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  20%|##        | 2/10 [00:11<00:27,  3.45s/it]\n",
      "bagging, val_score: 0.674334:  30%|###       | 3/10 [00:11<00:26,  3.75s/it][I 2020-09-17 05:19:30,583] Finished trial#29 with value: 0.6798015228069362 with parameters: {'bagging_fraction': 0.8518948114311072, 'bagging_freq': 2}. Best is trial#29 with value: 0.6798015228069362.\n",
      "\n",
      "bagging, val_score: 0.674334:  30%|###       | 3/10 [00:11<00:26,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686189\n",
      "[400]\tvalid_0's binary_logloss: 0.683494\n",
      "[600]\tvalid_0's binary_logloss: 0.682171\n",
      "[800]\tvalid_0's binary_logloss: 0.680925\n",
      "Early stopping, best iteration is:\n",
      "[864]\tvalid_0's binary_logloss: 0.680552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  30%|###       | 3/10 [00:14<00:26,  3.75s/it]\n",
      "bagging, val_score: 0.674334:  40%|####      | 4/10 [00:14<00:21,  3.60s/it][I 2020-09-17 05:19:33,837] Finished trial#30 with value: 0.6805523761173274 with parameters: {'bagging_fraction': 0.8139847714791495, 'bagging_freq': 4}. Best is trial#29 with value: 0.6798015228069362.\n",
      "\n",
      "bagging, val_score: 0.674334:  40%|####      | 4/10 [00:14<00:21,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685529\n",
      "[400]\tvalid_0's binary_logloss: 0.68227\n",
      "[600]\tvalid_0's binary_logloss: 0.680294\n",
      "[800]\tvalid_0's binary_logloss: 0.678942\n",
      "[1000]\tvalid_0's binary_logloss: 0.678048\n",
      "[1200]\tvalid_0's binary_logloss: 0.677396\n",
      "[1400]\tvalid_0's binary_logloss: 0.677301\n",
      "Early stopping, best iteration is:\n",
      "[1331]\tvalid_0's binary_logloss: 0.677219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  40%|####      | 4/10 [00:18<00:21,  3.60s/it]\n",
      "bagging, val_score: 0.674334:  50%|#####     | 5/10 [00:18<00:18,  3.64s/it][I 2020-09-17 05:19:37,555] Finished trial#31 with value: 0.6772192952452374 with parameters: {'bagging_fraction': 0.9564232906224956, 'bagging_freq': 1}. Best is trial#31 with value: 0.6772192952452374.\n",
      "\n",
      "bagging, val_score: 0.674334:  50%|#####     | 5/10 [00:18<00:18,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686458\n",
      "[400]\tvalid_0's binary_logloss: 0.683602\n",
      "[600]\tvalid_0's binary_logloss: 0.681913\n",
      "[800]\tvalid_0's binary_logloss: 0.680618\n",
      "[1000]\tvalid_0's binary_logloss: 0.680057\n",
      "Early stopping, best iteration is:\n",
      "[1024]\tvalid_0's binary_logloss: 0.679903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  50%|#####     | 5/10 [00:23<00:18,  3.64s/it]\n",
      "bagging, val_score: 0.674334:  60%|######    | 6/10 [00:23<00:16,  4.12s/it][I 2020-09-17 05:19:42,816] Finished trial#32 with value: 0.6799025985014259 with parameters: {'bagging_fraction': 0.8273413740962225, 'bagging_freq': 1}. Best is trial#31 with value: 0.6772192952452374.\n",
      "\n",
      "bagging, val_score: 0.674334:  60%|######    | 6/10 [00:23<00:16,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685772\n",
      "[400]\tvalid_0's binary_logloss: 0.682266\n",
      "[600]\tvalid_0's binary_logloss: 0.680233\n",
      "[800]\tvalid_0's binary_logloss: 0.679189\n",
      "[1000]\tvalid_0's binary_logloss: 0.67853\n",
      "Early stopping, best iteration is:\n",
      "[1076]\tvalid_0's binary_logloss: 0.678285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  60%|######    | 6/10 [00:27<00:16,  4.12s/it]\n",
      "bagging, val_score: 0.674334:  70%|#######   | 7/10 [00:27<00:12,  4.12s/it][I 2020-09-17 05:19:46,925] Finished trial#33 with value: 0.6782848009035262 with parameters: {'bagging_fraction': 0.7950848621212628, 'bagging_freq': 4}. Best is trial#31 with value: 0.6772192952452374.\n",
      "\n",
      "bagging, val_score: 0.674334:  70%|#######   | 7/10 [00:27<00:12,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686494\n",
      "[400]\tvalid_0's binary_logloss: 0.684037\n",
      "[600]\tvalid_0's binary_logloss: 0.682019\n",
      "[800]\tvalid_0's binary_logloss: 0.680926\n",
      "[1000]\tvalid_0's binary_logloss: 0.679976\n",
      "[1200]\tvalid_0's binary_logloss: 0.679647\n",
      "Early stopping, best iteration is:\n",
      "[1190]\tvalid_0's binary_logloss: 0.679575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  70%|#######   | 7/10 [00:35<00:12,  4.12s/it]\n",
      "bagging, val_score: 0.674334:  80%|########  | 8/10 [00:35<00:10,  5.10s/it][I 2020-09-17 05:19:54,323] Finished trial#34 with value: 0.6795746891285687 with parameters: {'bagging_fraction': 0.9054092791744749, 'bagging_freq': 7}. Best is trial#31 with value: 0.6772192952452374.\n",
      "\n",
      "bagging, val_score: 0.674334:  80%|########  | 8/10 [00:35<00:10,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686584\n",
      "[400]\tvalid_0's binary_logloss: 0.684319\n",
      "[600]\tvalid_0's binary_logloss: 0.682684\n",
      "[800]\tvalid_0's binary_logloss: 0.681937\n",
      "[1000]\tvalid_0's binary_logloss: 0.681605\n",
      "Early stopping, best iteration is:\n",
      "[916]\tvalid_0's binary_logloss: 0.681286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  80%|########  | 8/10 [00:39<00:10,  5.10s/it]\n",
      "bagging, val_score: 0.674334:  90%|######### | 9/10 [00:39<00:04,  4.89s/it][I 2020-09-17 05:19:58,736] Finished trial#35 with value: 0.6812856826243012 with parameters: {'bagging_fraction': 0.7193645996514588, 'bagging_freq': 2}. Best is trial#31 with value: 0.6772192952452374.\n",
      "\n",
      "bagging, val_score: 0.674334:  90%|######### | 9/10 [00:39<00:04,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.687263\n",
      "[400]\tvalid_0's binary_logloss: 0.685625\n",
      "[600]\tvalid_0's binary_logloss: 0.684443\n",
      "[800]\tvalid_0's binary_logloss: 0.683771\n",
      "Early stopping, best iteration is:\n",
      "[828]\tvalid_0's binary_logloss: 0.683564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bagging, val_score: 0.674334:  90%|######### | 9/10 [00:44<00:04,  4.89s/it]\n",
      "bagging, val_score: 0.674334: 100%|##########| 10/10 [00:44<00:00,  4.88s/it][I 2020-09-17 05:20:03,573] Finished trial#36 with value: 0.6835636583171857 with parameters: {'bagging_fraction': 0.4720865422342758, 'bagging_freq': 2}. Best is trial#31 with value: 0.6772192952452374.\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.674334:   0%|          | 0/3 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685289\n",
      "[400]\tvalid_0's binary_logloss: 0.682408\n",
      "[600]\tvalid_0's binary_logloss: 0.680153\n",
      "[800]\tvalid_0's binary_logloss: 0.678952\n",
      "[1000]\tvalid_0's binary_logloss: 0.677897\n",
      "[1200]\tvalid_0's binary_logloss: 0.676805\n",
      "[1400]\tvalid_0's binary_logloss: 0.67628\n",
      "Early stopping, best iteration is:\n",
      "[1474]\tvalid_0's binary_logloss: 0.676086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.674334:   0%|          | 0/3 [00:05<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: 0.674334:  33%|###3      | 1/3 [00:05<00:11,  5.58s/it][I 2020-09-17 05:20:09,206] Finished trial#37 with value: 0.6760860855401418 with parameters: {'feature_fraction': 0.9840000000000001}. Best is trial#37 with value: 0.6760860855401418.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.674334:  33%|###3      | 1/3 [00:05<00:11,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685598\n",
      "[400]\tvalid_0's binary_logloss: 0.682443\n",
      "[600]\tvalid_0's binary_logloss: 0.68009\n",
      "[800]\tvalid_0's binary_logloss: 0.678901\n",
      "[1000]\tvalid_0's binary_logloss: 0.678058\n",
      "[1200]\tvalid_0's binary_logloss: 0.676871\n",
      "[1400]\tvalid_0's binary_logloss: 0.676273\n",
      "Early stopping, best iteration is:\n",
      "[1465]\tvalid_0's binary_logloss: 0.676131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.674334:  33%|###3      | 1/3 [00:10<00:11,  5.58s/it]\n",
      "feature_fraction_stage2, val_score: 0.674334:  67%|######6   | 2/3 [00:10<00:05,  5.41s/it][I 2020-09-17 05:20:14,228] Finished trial#38 with value: 0.6761307765208817 with parameters: {'feature_fraction': 0.92}. Best is trial#37 with value: 0.6760860855401418.\n",
      "\n",
      "feature_fraction_stage2, val_score: 0.674334:  67%|######6   | 2/3 [00:10<00:05,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685289\n",
      "[400]\tvalid_0's binary_logloss: 0.682408\n",
      "[600]\tvalid_0's binary_logloss: 0.680153\n",
      "[800]\tvalid_0's binary_logloss: 0.678952\n",
      "[1000]\tvalid_0's binary_logloss: 0.677897\n",
      "[1200]\tvalid_0's binary_logloss: 0.676805\n",
      "[1400]\tvalid_0's binary_logloss: 0.67628\n",
      "Early stopping, best iteration is:\n",
      "[1474]\tvalid_0's binary_logloss: 0.676086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_fraction_stage2, val_score: 0.674334:  67%|######6   | 2/3 [00:16<00:05,  5.41s/it]\n",
      "feature_fraction_stage2, val_score: 0.674334: 100%|##########| 3/3 [00:16<00:00,  5.62s/it][I 2020-09-17 05:20:20,337] Finished trial#39 with value: 0.6760860855401418 with parameters: {'feature_fraction': 0.9520000000000001}. Best is trial#37 with value: 0.6760860855401418.\n",
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "regularization_factors, val_score: 0.674334:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685093\n",
      "[400]\tvalid_0's binary_logloss: 0.68178\n",
      "[600]\tvalid_0's binary_logloss: 0.679689\n",
      "[800]\tvalid_0's binary_logloss: 0.678615\n",
      "Early stopping, best iteration is:\n",
      "[775]\tvalid_0's binary_logloss: 0.678584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674334:   0%|          | 0/20 [00:03<?, ?it/s]\n",
      "regularization_factors, val_score: 0.674334:   5%|5         | 1/20 [00:03<01:04,  3.41s/it][I 2020-09-17 05:20:23,805] Finished trial#40 with value: 0.678583708703349 with parameters: {'lambda_l1': 1.6187956697650503, 'lambda_l2': 0.00938919992834392}. Best is trial#40 with value: 0.678583708703349.\n",
      "\n",
      "regularization_factors, val_score: 0.674334:   5%|5         | 1/20 [00:03<01:04,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685072\n",
      "[400]\tvalid_0's binary_logloss: 0.682289\n",
      "[600]\tvalid_0's binary_logloss: 0.679886\n",
      "[800]\tvalid_0's binary_logloss: 0.678683\n",
      "[1000]\tvalid_0's binary_logloss: 0.677317\n",
      "[1200]\tvalid_0's binary_logloss: 0.675917\n",
      "[1400]\tvalid_0's binary_logloss: 0.674989\n",
      "Early stopping, best iteration is:\n",
      "[1453]\tvalid_0's binary_logloss: 0.674766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674334:   5%|5         | 1/20 [00:08<01:04,  3.41s/it]\n",
      "regularization_factors, val_score: 0.674334:  10%|#         | 2/20 [00:08<01:11,  3.95s/it][I 2020-09-17 05:20:29,028] Finished trial#41 with value: 0.6747658264934683 with parameters: {'lambda_l1': 3.906285785783723e-08, 'lambda_l2': 0.01181302552105977}. Best is trial#41 with value: 0.6747658264934683.\n",
      "\n",
      "regularization_factors, val_score: 0.674334:  10%|#         | 2/20 [00:08<01:11,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679783\n",
      "[800]\tvalid_0's binary_logloss: 0.678562\n",
      "[1000]\tvalid_0's binary_logloss: 0.676981\n",
      "[1200]\tvalid_0's binary_logloss: 0.675748\n",
      "[1400]\tvalid_0's binary_logloss: 0.674654\n",
      "[1600]\tvalid_0's binary_logloss: 0.674516\n",
      "Early stopping, best iteration is:\n",
      "[1546]\tvalid_0's binary_logloss: 0.674302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  10%|#         | 2/20 [00:14<01:11,  3.95s/it]\n",
      "regularization_factors, val_score: 0.674302:  15%|#5        | 3/20 [00:14<01:15,  4.42s/it][I 2020-09-17 05:20:34,547] Finished trial#42 with value: 0.674302226570683 with parameters: {'lambda_l1': 7.687376607811107e-06, 'lambda_l2': 1.1661028763529614e-07}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  15%|#5        | 3/20 [00:14<01:15,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679783\n",
      "[800]\tvalid_0's binary_logloss: 0.678562\n",
      "[1000]\tvalid_0's binary_logloss: 0.676981\n",
      "[1200]\tvalid_0's binary_logloss: 0.675748\n",
      "[1400]\tvalid_0's binary_logloss: 0.674656\n",
      "[1600]\tvalid_0's binary_logloss: 0.674518\n",
      "Early stopping, best iteration is:\n",
      "[1546]\tvalid_0's binary_logloss: 0.674304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  15%|#5        | 3/20 [00:19<01:15,  4.42s/it]\n",
      "regularization_factors, val_score: 0.674302:  20%|##        | 4/20 [00:19<01:15,  4.74s/it][I 2020-09-17 05:20:40,025] Finished trial#43 with value: 0.6743036618109162 with parameters: {'lambda_l1': 5.0883869346838805e-06, 'lambda_l2': 3.140022926102802e-07}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  20%|##        | 4/20 [00:19<01:15,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.67979\n",
      "[800]\tvalid_0's binary_logloss: 0.67852\n",
      "[1000]\tvalid_0's binary_logloss: 0.677107\n",
      "[1200]\tvalid_0's binary_logloss: 0.675913\n",
      "[1400]\tvalid_0's binary_logloss: 0.674737\n",
      "[1600]\tvalid_0's binary_logloss: 0.674845\n",
      "Early stopping, best iteration is:\n",
      "[1503]\tvalid_0's binary_logloss: 0.674498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  20%|##        | 4/20 [00:30<01:15,  4.74s/it]\n",
      "regularization_factors, val_score: 0.674302:  25%|##5       | 5/20 [00:30<01:38,  6.57s/it][I 2020-09-17 05:20:50,855] Finished trial#44 with value: 0.674498031225138 with parameters: {'lambda_l1': 7.723086780032446e-05, 'lambda_l2': 5.224340068010163e-06}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  25%|##5       | 5/20 [00:30<01:38,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.684815\n",
      "[400]\tvalid_0's binary_logloss: 0.680676\n",
      "[600]\tvalid_0's binary_logloss: 0.67884\n",
      "[800]\tvalid_0's binary_logloss: 0.6779\n",
      "[1000]\tvalid_0's binary_logloss: 0.677744\n",
      "Early stopping, best iteration is:\n",
      "[997]\tvalid_0's binary_logloss: 0.677733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  25%|##5       | 5/20 [00:40<01:38,  6.57s/it]\n",
      "regularization_factors, val_score: 0.674302:  30%|###       | 6/20 [00:40<01:48,  7.72s/it][I 2020-09-17 05:21:01,266] Finished trial#45 with value: 0.6777331985456417 with parameters: {'lambda_l1': 3.419978689885679, 'lambda_l2': 3.872178090056219e-06}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  30%|###       | 6/20 [00:40<01:48,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682241\n",
      "[600]\tvalid_0's binary_logloss: 0.679836\n",
      "[800]\tvalid_0's binary_logloss: 0.678614\n",
      "[1000]\tvalid_0's binary_logloss: 0.67709\n",
      "[1200]\tvalid_0's binary_logloss: 0.675652\n",
      "[1400]\tvalid_0's binary_logloss: 0.674732\n",
      "Early stopping, best iteration is:\n",
      "[1467]\tvalid_0's binary_logloss: 0.674524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  30%|###       | 6/20 [00:48<01:48,  7.72s/it]\n",
      "regularization_factors, val_score: 0.674302:  35%|###5      | 7/20 [00:48<01:39,  7.68s/it][I 2020-09-17 05:21:08,859] Finished trial#46 with value: 0.6745235460067202 with parameters: {'lambda_l1': 0.00019957821921762222, 'lambda_l2': 6.156858993760464e-07}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  35%|###5      | 7/20 [00:48<01:39,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682219\n",
      "[600]\tvalid_0's binary_logloss: 0.679757\n",
      "[800]\tvalid_0's binary_logloss: 0.678629\n",
      "[1000]\tvalid_0's binary_logloss: 0.677136\n",
      "[1200]\tvalid_0's binary_logloss: 0.67589\n",
      "[1400]\tvalid_0's binary_logloss: 0.674771\n",
      "Early stopping, best iteration is:\n",
      "[1460]\tvalid_0's binary_logloss: 0.674544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  35%|###5      | 7/20 [00:56<01:39,  7.68s/it]\n",
      "regularization_factors, val_score: 0.674302:  40%|####      | 8/20 [00:56<01:32,  7.68s/it][I 2020-09-17 05:21:16,521] Finished trial#47 with value: 0.6745439621756965 with parameters: {'lambda_l1': 5.979767743570013e-06, 'lambda_l2': 0.00033676034259612467}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  40%|####      | 8/20 [00:56<01:32,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679783\n",
      "[800]\tvalid_0's binary_logloss: 0.678562\n",
      "[1000]\tvalid_0's binary_logloss: 0.676981\n",
      "[1200]\tvalid_0's binary_logloss: 0.675748\n",
      "[1400]\tvalid_0's binary_logloss: 0.674691\n",
      "[1600]\tvalid_0's binary_logloss: 0.674524\n",
      "Early stopping, best iteration is:\n",
      "[1542]\tvalid_0's binary_logloss: 0.674334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  40%|####      | 8/20 [01:03<01:32,  7.68s/it]\n",
      "regularization_factors, val_score: 0.674302:  45%|####5     | 9/20 [01:03<01:21,  7.45s/it][I 2020-09-17 05:21:23,438] Finished trial#48 with value: 0.6743339406482105 with parameters: {'lambda_l1': 1.95282295432328e-06, 'lambda_l2': 8.01357055406393e-08}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  45%|####5     | 9/20 [01:03<01:21,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682267\n",
      "[600]\tvalid_0's binary_logloss: 0.67983\n",
      "[800]\tvalid_0's binary_logloss: 0.678505\n",
      "[1000]\tvalid_0's binary_logloss: 0.676966\n",
      "[1200]\tvalid_0's binary_logloss: 0.675809\n",
      "[1400]\tvalid_0's binary_logloss: 0.674845\n",
      "[1600]\tvalid_0's binary_logloss: 0.674887\n",
      "Early stopping, best iteration is:\n",
      "[1510]\tvalid_0's binary_logloss: 0.674539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  45%|####5     | 9/20 [01:10<01:21,  7.45s/it]\n",
      "regularization_factors, val_score: 0.674302:  50%|#####     | 10/20 [01:10<01:13,  7.32s/it][I 2020-09-17 05:21:30,445] Finished trial#49 with value: 0.6745391738682398 with parameters: {'lambda_l1': 0.00013655069864118322, 'lambda_l2': 5.63501702024687e-05}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  50%|#####     | 10/20 [01:10<01:13,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685517\n",
      "[400]\tvalid_0's binary_logloss: 0.682981\n",
      "[600]\tvalid_0's binary_logloss: 0.680471\n",
      "[800]\tvalid_0's binary_logloss: 0.678939\n",
      "[1000]\tvalid_0's binary_logloss: 0.677393\n",
      "[1200]\tvalid_0's binary_logloss: 0.67624\n",
      "[1400]\tvalid_0's binary_logloss: 0.676178\n",
      "Early stopping, best iteration is:\n",
      "[1306]\tvalid_0's binary_logloss: 0.675942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  50%|#####     | 10/20 [01:16<01:13,  7.32s/it]\n",
      "regularization_factors, val_score: 0.674302:  55%|#####5    | 11/20 [01:16<01:02,  6.97s/it][I 2020-09-17 05:21:36,624] Finished trial#50 with value: 0.6759423520682606 with parameters: {'lambda_l1': 0.019518363595584023, 'lambda_l2': 1.1084783484879852}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  55%|#####5    | 11/20 [01:16<01:02,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679783\n",
      "[800]\tvalid_0's binary_logloss: 0.678562\n",
      "[1000]\tvalid_0's binary_logloss: 0.676981\n",
      "[1200]\tvalid_0's binary_logloss: 0.675748\n",
      "[1400]\tvalid_0's binary_logloss: 0.674691\n",
      "[1600]\tvalid_0's binary_logloss: 0.674523\n",
      "Early stopping, best iteration is:\n",
      "[1542]\tvalid_0's binary_logloss: 0.674333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  55%|#####5    | 11/20 [01:22<01:02,  6.97s/it]\n",
      "regularization_factors, val_score: 0.674302:  60%|######    | 12/20 [01:22<00:52,  6.62s/it][I 2020-09-17 05:21:42,408] Finished trial#51 with value: 0.6743325356840355 with parameters: {'lambda_l1': 2.1602093382925917e-08, 'lambda_l2': 1.2350737288587804e-08}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  60%|######    | 12/20 [01:22<00:52,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679783\n",
      "[800]\tvalid_0's binary_logloss: 0.678562\n",
      "[1000]\tvalid_0's binary_logloss: 0.676981\n",
      "[1200]\tvalid_0's binary_logloss: 0.675748\n",
      "[1400]\tvalid_0's binary_logloss: 0.67469\n",
      "[1600]\tvalid_0's binary_logloss: 0.674523\n",
      "Early stopping, best iteration is:\n",
      "[1542]\tvalid_0's binary_logloss: 0.674333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  60%|######    | 12/20 [01:28<00:52,  6.62s/it]\n",
      "regularization_factors, val_score: 0.674302:  65%|######5   | 13/20 [01:28<00:45,  6.56s/it][I 2020-09-17 05:21:48,845] Finished trial#52 with value: 0.6743325321199949 with parameters: {'lambda_l1': 6.165585850877185e-07, 'lambda_l2': 1.0242971513987307e-08}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  65%|######5   | 13/20 [01:28<00:45,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685109\n",
      "[400]\tvalid_0's binary_logloss: 0.682369\n",
      "[600]\tvalid_0's binary_logloss: 0.680121\n",
      "[800]\tvalid_0's binary_logloss: 0.678714\n",
      "[1000]\tvalid_0's binary_logloss: 0.677241\n",
      "[1200]\tvalid_0's binary_logloss: 0.676198\n",
      "[1400]\tvalid_0's binary_logloss: 0.675311\n",
      "Early stopping, best iteration is:\n",
      "[1483]\tvalid_0's binary_logloss: 0.675172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  65%|######5   | 13/20 [01:34<00:45,  6.56s/it]\n",
      "regularization_factors, val_score: 0.674302:  70%|#######   | 14/20 [01:34<00:38,  6.45s/it][I 2020-09-17 05:21:55,008] Finished trial#53 with value: 0.6751723699161832 with parameters: {'lambda_l1': 0.004825743685432284, 'lambda_l2': 1.0477594941813792e-07}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  70%|#######   | 14/20 [01:34<00:38,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679783\n",
      "[800]\tvalid_0's binary_logloss: 0.678562\n",
      "[1000]\tvalid_0's binary_logloss: 0.676981\n",
      "[1200]\tvalid_0's binary_logloss: 0.675748\n",
      "[1400]\tvalid_0's binary_logloss: 0.67469\n",
      "[1600]\tvalid_0's binary_logloss: 0.674521\n",
      "Early stopping, best iteration is:\n",
      "[1542]\tvalid_0's binary_logloss: 0.674333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  70%|#######   | 14/20 [01:40<00:38,  6.45s/it]\n",
      "regularization_factors, val_score: 0.674302:  75%|#######5  | 15/20 [01:40<00:30,  6.13s/it][I 2020-09-17 05:22:00,416] Finished trial#54 with value: 0.6743325144349235 with parameters: {'lambda_l1': 1.7299737340484636e-07, 'lambda_l2': 9.347055959387547e-07}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  75%|#######5  | 15/20 [01:40<00:30,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.679739\n",
      "[800]\tvalid_0's binary_logloss: 0.678601\n",
      "[1000]\tvalid_0's binary_logloss: 0.677093\n",
      "[1200]\tvalid_0's binary_logloss: 0.676014\n",
      "[1400]\tvalid_0's binary_logloss: 0.674915\n",
      "Early stopping, best iteration is:\n",
      "[1472]\tvalid_0's binary_logloss: 0.674659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  75%|#######5  | 15/20 [01:46<00:30,  6.13s/it]\n",
      "regularization_factors, val_score: 0.674302:  80%|########  | 16/20 [01:46<00:24,  6.23s/it][I 2020-09-17 05:22:06,870] Finished trial#55 with value: 0.6746589245748235 with parameters: {'lambda_l1': 1.2271117586011566e-05, 'lambda_l2': 0.00010555787205587289}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  80%|########  | 16/20 [01:46<00:24,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685075\n",
      "[400]\tvalid_0's binary_logloss: 0.682271\n",
      "[600]\tvalid_0's binary_logloss: 0.679924\n",
      "[800]\tvalid_0's binary_logloss: 0.678543\n",
      "[1000]\tvalid_0's binary_logloss: 0.677074\n",
      "[1200]\tvalid_0's binary_logloss: 0.675893\n",
      "[1400]\tvalid_0's binary_logloss: 0.675164\n",
      "[1600]\tvalid_0's binary_logloss: 0.67498\n",
      "Early stopping, best iteration is:\n",
      "[1532]\tvalid_0's binary_logloss: 0.674718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  80%|########  | 16/20 [01:54<00:24,  6.23s/it]\n",
      "regularization_factors, val_score: 0.674302:  85%|########5 | 17/20 [01:54<00:20,  6.70s/it][I 2020-09-17 05:22:14,658] Finished trial#56 with value: 0.6747183358212099 with parameters: {'lambda_l1': 0.007289910603898965, 'lambda_l2': 4.570181377255113e-08}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  85%|########5 | 17/20 [01:54<00:20,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685095\n",
      "[400]\tvalid_0's binary_logloss: 0.682176\n",
      "[600]\tvalid_0's binary_logloss: 0.67979\n",
      "[800]\tvalid_0's binary_logloss: 0.678502\n",
      "[1000]\tvalid_0's binary_logloss: 0.677158\n",
      "[1200]\tvalid_0's binary_logloss: 0.675863\n",
      "[1400]\tvalid_0's binary_logloss: 0.674872\n",
      "Early stopping, best iteration is:\n",
      "[1465]\tvalid_0's binary_logloss: 0.674656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  85%|########5 | 17/20 [02:04<00:20,  6.70s/it]\n",
      "regularization_factors, val_score: 0.674302:  90%|######### | 18/20 [02:04<00:15,  7.62s/it][I 2020-09-17 05:22:24,421] Finished trial#57 with value: 0.6746559966600509 with parameters: {'lambda_l1': 2.507098034678341e-05, 'lambda_l2': 7.049544050618289e-06}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  90%|######### | 18/20 [02:04<00:15,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685098\n",
      "[400]\tvalid_0's binary_logloss: 0.682195\n",
      "[600]\tvalid_0's binary_logloss: 0.679823\n",
      "[800]\tvalid_0's binary_logloss: 0.678572\n",
      "[1000]\tvalid_0's binary_logloss: 0.677098\n",
      "[1200]\tvalid_0's binary_logloss: 0.675911\n",
      "[1400]\tvalid_0's binary_logloss: 0.675041\n",
      "Early stopping, best iteration is:\n",
      "[1474]\tvalid_0's binary_logloss: 0.674746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  90%|######### | 18/20 [02:52<00:15,  7.62s/it]\n",
      "regularization_factors, val_score: 0.674302:  95%|#########5| 19/20 [02:52<00:20, 20.01s/it][I 2020-09-17 05:23:13,349] Finished trial#58 with value: 0.6747463486135032 with parameters: {'lambda_l1': 0.0014064391299371517, 'lambda_l2': 3.563867353371214e-07}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "regularization_factors, val_score: 0.674302:  95%|#########5| 19/20 [02:53<00:20, 20.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685087\n",
      "[400]\tvalid_0's binary_logloss: 0.682263\n",
      "[600]\tvalid_0's binary_logloss: 0.680033\n",
      "[800]\tvalid_0's binary_logloss: 0.678584\n",
      "[1000]\tvalid_0's binary_logloss: 0.677129\n",
      "[1200]\tvalid_0's binary_logloss: 0.675971\n",
      "[1400]\tvalid_0's binary_logloss: 0.674836\n",
      "Early stopping, best iteration is:\n",
      "[1465]\tvalid_0's binary_logloss: 0.674621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "regularization_factors, val_score: 0.674302:  95%|#########5| 19/20 [03:18<00:20, 20.01s/it]\n",
      "regularization_factors, val_score: 0.674302: 100%|##########| 20/20 [03:18<00:00, 21.64s/it][I 2020-09-17 05:23:38,785] Finished trial#59 with value: 0.6746206113317177 with parameters: {'lambda_l1': 2.9775298190508434e-07, 'lambda_l2': 0.0013823222623475986}. Best is trial#42 with value: 0.674302226570683.\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.674302:   0%|          | 0/5 [00:00<?, ?it/s]/home/tubotu/.local/lib/python3.6/site-packages/lightgbm/engine.py:118: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686161\n",
      "[400]\tvalid_0's binary_logloss: 0.682975\n",
      "[600]\tvalid_0's binary_logloss: 0.680567\n",
      "[800]\tvalid_0's binary_logloss: 0.680066\n",
      "Early stopping, best iteration is:\n",
      "[870]\tvalid_0's binary_logloss: 0.67982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674302:   0%|          | 0/5 [00:03<?, ?it/s]\n",
      "min_data_in_leaf, val_score: 0.674302:  20%|##        | 1/5 [00:03<00:14,  3.69s/it][I 2020-09-17 05:23:42,533] Finished trial#60 with value: 0.6798198840478991 with parameters: {'min_child_samples': 5}. Best is trial#60 with value: 0.6798198840478991.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.674302:  20%|##        | 1/5 [00:03<00:14,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.68535\n",
      "[400]\tvalid_0's binary_logloss: 0.682432\n",
      "[600]\tvalid_0's binary_logloss: 0.679297\n",
      "[800]\tvalid_0's binary_logloss: 0.677288\n",
      "[1000]\tvalid_0's binary_logloss: 0.675706\n",
      "[1200]\tvalid_0's binary_logloss: 0.674542\n",
      "[1400]\tvalid_0's binary_logloss: 0.674551\n",
      "Early stopping, best iteration is:\n",
      "[1305]\tvalid_0's binary_logloss: 0.674246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674246:  20%|##        | 1/5 [00:09<00:14,  3.69s/it]\n",
      "min_data_in_leaf, val_score: 0.674246:  40%|####      | 2/5 [00:09<00:12,  4.28s/it][I 2020-09-17 05:23:48,193] Finished trial#61 with value: 0.6742461026246716 with parameters: {'min_child_samples': 25}. Best is trial#61 with value: 0.6742461026246716.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.674246:  40%|####      | 2/5 [00:09<00:12,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.685371\n",
      "[400]\tvalid_0's binary_logloss: 0.682419\n",
      "[600]\tvalid_0's binary_logloss: 0.680348\n",
      "[800]\tvalid_0's binary_logloss: 0.678696\n",
      "[1000]\tvalid_0's binary_logloss: 0.677693\n",
      "[1200]\tvalid_0's binary_logloss: 0.676966\n",
      "Early stopping, best iteration is:\n",
      "[1216]\tvalid_0's binary_logloss: 0.676957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674246:  40%|####      | 2/5 [00:14<00:12,  4.28s/it]\n",
      "min_data_in_leaf, val_score: 0.674246:  60%|######    | 3/5 [00:14<00:08,  4.39s/it][I 2020-09-17 05:23:52,852] Finished trial#62 with value: 0.6769574657867401 with parameters: {'min_child_samples': 100}. Best is trial#61 with value: 0.6742461026246716.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.674246:  60%|######    | 3/5 [00:14<00:08,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686231\n",
      "[400]\tvalid_0's binary_logloss: 0.683318\n",
      "[600]\tvalid_0's binary_logloss: 0.681481\n",
      "Early stopping, best iteration is:\n",
      "[640]\tvalid_0's binary_logloss: 0.681313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674246:  60%|######    | 3/5 [00:25<00:08,  4.39s/it]\n",
      "min_data_in_leaf, val_score: 0.674246:  80%|########  | 4/5 [00:25<00:06,  6.50s/it][I 2020-09-17 05:24:04,250] Finished trial#63 with value: 0.6813126651788887 with parameters: {'min_child_samples': 50}. Best is trial#61 with value: 0.6742461026246716.\n",
      "\n",
      "min_data_in_leaf, val_score: 0.674246:  80%|########  | 4/5 [00:25<00:06,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\tvalid_0's binary_logloss: 0.686032\n",
      "[400]\tvalid_0's binary_logloss: 0.682531\n",
      "[600]\tvalid_0's binary_logloss: 0.680291\n",
      "[800]\tvalid_0's binary_logloss: 0.678749\n",
      "[1000]\tvalid_0's binary_logloss: 0.677959\n",
      "[1200]\tvalid_0's binary_logloss: 0.676938\n",
      "[1400]\tvalid_0's binary_logloss: 0.676239\n",
      "Early stopping, best iteration is:\n",
      "[1361]\tvalid_0's binary_logloss: 0.676085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "min_data_in_leaf, val_score: 0.674246:  80%|########  | 4/5 [00:40<00:06,  6.50s/it]\n",
      "min_data_in_leaf, val_score: 0.674246: 100%|##########| 5/5 [00:40<00:00,  8.96s/it][I 2020-09-17 05:24:18,952] Finished trial#64 with value: 0.6760847319861714 with parameters: {'min_child_samples': 10}. Best is trial#61 with value: 0.6742461026246716.\n"
     ]
    }
   ],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "K = 5\n",
    "kf =  StratifiedKFold(n_splits=K, shuffle=True, random_state=114)\n",
    "\n",
    "# ハイパーパラメータサーチ&モデル構築\n",
    "params = {'objective': 'binary',\n",
    "          'metric': 'binary_logloss',\n",
    "          'n_estimators': 10000,\n",
    "          'learning_rate': 0.001,\n",
    "          'random_seed':42} \n",
    "\n",
    "oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "oof_pred_ans = np.zeros_like(y, dtype=np.float)\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    # LightGBM用のデータセットに変換\n",
    "    train_lgb = lgb.Dataset(x_train, y_train)\n",
    "    val_lgb = lgb.Dataset(x_test, y_test)\n",
    "    clf = lgb_o.train(params,\n",
    "                    train_lgb,\n",
    "                    valid_sets=val_lgb,\n",
    "                    early_stopping_rounds=100,\n",
    "                    verbose_eval=200,)\n",
    "    pred_i = clf.predict(x_test)\n",
    "    oof_pred[test_index] = pred_i\n",
    "    oof_pred_ans[test_index] =(clf.predict(x_test) > 0.5).astype(int)\n",
    "    models.append(clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'metric': 'binary_logloss',\n",
       " 'learning_rate': 0.001,\n",
       " 'random_seed': 42,\n",
       " 'lambda_l1': 6.348648012781223,\n",
       " 'lambda_l2': 0.004384233880028371,\n",
       " 'num_leaves': 26,\n",
       " 'feature_fraction': 0.5479999999999999,\n",
       " 'bagging_fraction': 0.8525922961022108,\n",
       " 'bagging_freq': 1,\n",
       " 'min_child_samples': 20}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[8].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[30]\ttraining's binary_logloss: 0.670579\tvalid_1's binary_logloss: 0.685484\n[60]\ttraining's binary_logloss: 0.652375\tvalid_1's binary_logloss: 0.681446\n[90]\ttraining's binary_logloss: 0.635918\tvalid_1's binary_logloss: 0.678441\n[120]\ttraining's binary_logloss: 0.620697\tvalid_1's binary_logloss: 0.676732\n[150]\ttraining's binary_logloss: 0.605923\tvalid_1's binary_logloss: 0.6754\n[30]\ttraining's binary_logloss: 0.669885\tvalid_1's binary_logloss: 0.689746\n[60]\ttraining's binary_logloss: 0.65113\tvalid_1's binary_logloss: 0.689507\n[90]\ttraining's binary_logloss: 0.634391\tvalid_1's binary_logloss: 0.689657\n[120]\ttraining's binary_logloss: 0.6191\tvalid_1's binary_logloss: 0.690387\n[150]\ttraining's binary_logloss: 0.604281\tvalid_1's binary_logloss: 0.691427\n[30]\ttraining's binary_logloss: 0.669906\tvalid_1's binary_logloss: 0.68862\n[60]\ttraining's binary_logloss: 0.651321\tvalid_1's binary_logloss: 0.687084\n[90]\ttraining's binary_logloss: 0.634404\tvalid_1's binary_logloss: 0.686056\n[120]\ttraining's binary_logloss: 0.619059\tvalid_1's binary_logloss: 0.685955\n[150]\ttraining's binary_logloss: 0.604171\tvalid_1's binary_logloss: 0.685833\n[30]\ttraining's binary_logloss: 0.66996\tvalid_1's binary_logloss: 0.691009\n[60]\ttraining's binary_logloss: 0.65128\tvalid_1's binary_logloss: 0.690416\n[90]\ttraining's binary_logloss: 0.634217\tvalid_1's binary_logloss: 0.69042\n[120]\ttraining's binary_logloss: 0.618332\tvalid_1's binary_logloss: 0.68938\n[150]\ttraining's binary_logloss: 0.603667\tvalid_1's binary_logloss: 0.688006\n[30]\ttraining's binary_logloss: 0.669803\tvalid_1's binary_logloss: 0.686972\n[60]\ttraining's binary_logloss: 0.651067\tvalid_1's binary_logloss: 0.683749\n[90]\ttraining's binary_logloss: 0.634202\tvalid_1's binary_logloss: 0.680962\n[120]\ttraining's binary_logloss: 0.618169\tvalid_1's binary_logloss: 0.678734\n[150]\ttraining's binary_logloss: 0.603252\tvalid_1's binary_logloss: 0.67735\n[30]\ttraining's binary_logloss: 0.669782\tvalid_1's binary_logloss: 0.688483\n[60]\ttraining's binary_logloss: 0.650947\tvalid_1's binary_logloss: 0.68735\n[90]\ttraining's binary_logloss: 0.633816\tvalid_1's binary_logloss: 0.685793\n[120]\ttraining's binary_logloss: 0.618209\tvalid_1's binary_logloss: 0.683843\n[150]\ttraining's binary_logloss: 0.60331\tvalid_1's binary_logloss: 0.68264\n[30]\ttraining's binary_logloss: 0.67078\tvalid_1's binary_logloss: 0.69038\n[60]\ttraining's binary_logloss: 0.652685\tvalid_1's binary_logloss: 0.690617\n[90]\ttraining's binary_logloss: 0.635983\tvalid_1's binary_logloss: 0.691459\n[120]\ttraining's binary_logloss: 0.620475\tvalid_1's binary_logloss: 0.693294\n[150]\ttraining's binary_logloss: 0.605932\tvalid_1's binary_logloss: 0.69502\n[30]\ttraining's binary_logloss: 0.670327\tvalid_1's binary_logloss: 0.687464\n[60]\ttraining's binary_logloss: 0.651775\tvalid_1's binary_logloss: 0.684472\n[90]\ttraining's binary_logloss: 0.634715\tvalid_1's binary_logloss: 0.681563\n[120]\ttraining's binary_logloss: 0.619061\tvalid_1's binary_logloss: 0.678533\n[150]\ttraining's binary_logloss: 0.604568\tvalid_1's binary_logloss: 0.676991\n[30]\ttraining's binary_logloss: 0.669807\tvalid_1's binary_logloss: 0.688669\n[60]\ttraining's binary_logloss: 0.650879\tvalid_1's binary_logloss: 0.687664\n[90]\ttraining's binary_logloss: 0.633466\tvalid_1's binary_logloss: 0.688487\n[120]\ttraining's binary_logloss: 0.617375\tvalid_1's binary_logloss: 0.690178\n[150]\ttraining's binary_logloss: 0.602285\tvalid_1's binary_logloss: 0.691048\n[30]\ttraining's binary_logloss: 0.670203\tvalid_1's binary_logloss: 0.688502\n[60]\ttraining's binary_logloss: 0.651398\tvalid_1's binary_logloss: 0.686911\n[90]\ttraining's binary_logloss: 0.63449\tvalid_1's binary_logloss: 0.68616\n[120]\ttraining's binary_logloss: 0.618788\tvalid_1's binary_logloss: 0.685341\n[150]\ttraining's binary_logloss: 0.604116\tvalid_1's binary_logloss: 0.684507\n"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "K = 10\n",
    "kf =  StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "models = []\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.01,\n",
    "    'metric': 'binary_logloss',\n",
    "    'n_estimators': 150,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.0001,\n",
    "    'min_child_samples': 20,\n",
    "    'num_leaves': 31,\n",
    "    'colsample_bytree': 0.2,\n",
    "    'subsample': 1.0,\n",
    "    'verbosity': -1,\n",
    "    \"seed\":42,\n",
    "    'max_depth': -1\n",
    "}\n",
    "oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "oof_pred_ans = np.zeros_like(y, dtype=np.float)\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf = lgb.LGBMClassifier(**params)\n",
    "    clf.fit(x_train, y_train\n",
    "            ,eval_set=[(x_train,y_train),(x_test, y_test)]\n",
    "            ,verbose=30)\n",
    "    pred_i = clf.predict_proba(x_test)[:, 1]\n",
    "    oof_pred[test_index] = pred_i\n",
    "    oof_pred_ans[test_index] = clf.predict(x_test)\n",
    "    models.append(clf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5644614569791516\n[1. 0. 1. ... 0. 1. 1.]\n"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.roc_auc_score(y, oof_pred)\n",
    "print(score)\n",
    "print(oof_pred_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.28444272445820434"
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "source": [
    "np.sum(oof_pred_ans == 0) / np.sum(oof_pred_ans >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5491486068111455\n0.5340557275541795\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y, oof_pred_ans, normalize=True, sample_weight=None))\n",
    "print(accuracy_score(y, np.ones_like(y), normalize=True, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(oof_pred, columns=['pred']).to_csv(\"game_result_predict_20200910.csv\", mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_victory</th>\n",
       "      <th>last_week_home_win_rate_ratio</th>\n",
       "      <th>last_year_home_st_ERA</th>\n",
       "      <th>last_3_week_home_st_ERA</th>\n",
       "      <th>last_week_home_re_ERA</th>\n",
       "      <th>last_week_home_runs_ave</th>\n",
       "      <th>last_year_away_st_ERA</th>\n",
       "      <th>last_3_week_away_st_ERA</th>\n",
       "      <th>last_week_away_re_ERA</th>\n",
       "      <th>last_week_away_runs_ave</th>\n",
       "      <th>last_week_home_team_OPS</th>\n",
       "      <th>last_week_away_team_OPS</th>\n",
       "      <th>last_year_home_win_rate_ratio</th>\n",
       "      <th>prediction for win</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.91</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.11</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.494845</td>\n",
       "      <td>0.628540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.97</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.52</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526539</td>\n",
       "      <td>0.638487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.85</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.27</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.530108</td>\n",
       "      <td>0.524378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.91</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.386148</td>\n",
       "      <td>0.634059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.84</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535906</td>\n",
       "      <td>0.514458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.39</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.24</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.478585</td>\n",
       "      <td>0.450585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>3.31</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.594156</td>\n",
       "      <td>0.522689</td>\n",
       "      <td>0.494845</td>\n",
       "      <td>0.690814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>3.72</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>3.92</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>0.373412</td>\n",
       "      <td>0.818876</td>\n",
       "      <td>0.526539</td>\n",
       "      <td>0.612623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>4.12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>0.339216</td>\n",
       "      <td>0.427609</td>\n",
       "      <td>0.530108</td>\n",
       "      <td>0.572305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>3.55</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.649156</td>\n",
       "      <td>0.535906</td>\n",
       "      <td>0.573040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>3.17</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>3.51</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.478585</td>\n",
       "      <td>0.495181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>2.71</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.494845</td>\n",
       "      <td>0.523090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.81</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.649306</td>\n",
       "      <td>0.724149</td>\n",
       "      <td>0.526539</td>\n",
       "      <td>0.468971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.89</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.928571</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.548352</td>\n",
       "      <td>0.580796</td>\n",
       "      <td>0.530108</td>\n",
       "      <td>0.623186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>6.75</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.17</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.575808</td>\n",
       "      <td>0.546456</td>\n",
       "      <td>0.386148</td>\n",
       "      <td>0.573892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>2.48</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>12.150000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.729040</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.535906</td>\n",
       "      <td>0.555498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.79</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.724994</td>\n",
       "      <td>0.498429</td>\n",
       "      <td>0.478585</td>\n",
       "      <td>0.649650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.166668</td>\n",
       "      <td>2.89</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.125000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.928571</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>0.644709</td>\n",
       "      <td>0.750276</td>\n",
       "      <td>0.520773</td>\n",
       "      <td>0.498951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.363636</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.428571</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>0.660802</td>\n",
       "      <td>0.862962</td>\n",
       "      <td>0.452850</td>\n",
       "      <td>0.433435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>2.12</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.227273</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.682549</td>\n",
       "      <td>0.474973</td>\n",
       "      <td>0.542029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.84</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>2.38</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.310345</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>0.477204</td>\n",
       "      <td>0.660273</td>\n",
       "      <td>0.563948</td>\n",
       "      <td>0.509040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>3.27</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>4.58</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>0.631907</td>\n",
       "      <td>0.547465</td>\n",
       "      <td>0.558089</td>\n",
       "      <td>0.617207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.02</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.076923</td>\n",
       "      <td>5.375000</td>\n",
       "      <td>0.601265</td>\n",
       "      <td>0.747669</td>\n",
       "      <td>0.564080</td>\n",
       "      <td>0.463941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.22</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.862069</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.638343</td>\n",
       "      <td>0.725310</td>\n",
       "      <td>0.520773</td>\n",
       "      <td>0.416537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.19</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.629363</td>\n",
       "      <td>0.875284</td>\n",
       "      <td>0.452850</td>\n",
       "      <td>0.478875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.47</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.038462</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.743018</td>\n",
       "      <td>0.673063</td>\n",
       "      <td>0.474973</td>\n",
       "      <td>0.512749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.666666</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.35</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.236842</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.659954</td>\n",
       "      <td>0.643220</td>\n",
       "      <td>0.563948</td>\n",
       "      <td>0.586867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.666666</td>\n",
       "      <td>1.80</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>3.44</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.529412</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.777708</td>\n",
       "      <td>0.530147</td>\n",
       "      <td>0.558089</td>\n",
       "      <td>0.571558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.076923</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.22</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.797323</td>\n",
       "      <td>0.564080</td>\n",
       "      <td>0.531918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333334</td>\n",
       "      <td>3.20</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.352941</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.394737</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.654464</td>\n",
       "      <td>0.824318</td>\n",
       "      <td>0.452850</td>\n",
       "      <td>0.396122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>7.71</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.438356</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>0.577213</td>\n",
       "      <td>0.590856</td>\n",
       "      <td>0.537095</td>\n",
       "      <td>0.660240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>1</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>2.61</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.719700</td>\n",
       "      <td>0.612445</td>\n",
       "      <td>0.598223</td>\n",
       "      <td>0.679081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>8.470588</td>\n",
       "      <td>4.021277</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>0.595982</td>\n",
       "      <td>0.532753</td>\n",
       "      <td>0.478824</td>\n",
       "      <td>0.612620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.91</td>\n",
       "      <td>1.956522</td>\n",
       "      <td>5.062500</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>2.95</td>\n",
       "      <td>8.437500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.580309</td>\n",
       "      <td>0.571107</td>\n",
       "      <td>0.422307</td>\n",
       "      <td>0.563053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>0</td>\n",
       "      <td>0.432433</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.045455</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>2.94</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>0.599755</td>\n",
       "      <td>0.846418</td>\n",
       "      <td>0.416353</td>\n",
       "      <td>0.484307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>1</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>6.230769</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.544608</td>\n",
       "      <td>0.474842</td>\n",
       "      <td>0.558089</td>\n",
       "      <td>0.638497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>1</td>\n",
       "      <td>0.833332</td>\n",
       "      <td>3.01</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.647917</td>\n",
       "      <td>0.485641</td>\n",
       "      <td>0.524684</td>\n",
       "      <td>0.604552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.49</td>\n",
       "      <td>3.919355</td>\n",
       "      <td>2.872340</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>0.590054</td>\n",
       "      <td>0.602950</td>\n",
       "      <td>0.462905</td>\n",
       "      <td>0.562950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>1</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.928571</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>3.50</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.867480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.481053</td>\n",
       "      <td>0.622219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>1</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.724615</td>\n",
       "      <td>0.507778</td>\n",
       "      <td>0.511558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>0</td>\n",
       "      <td>0.666666</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.173913</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.294624</td>\n",
       "      <td>0.481053</td>\n",
       "      <td>0.471539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>0</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>6.31</td>\n",
       "      <td>3.176471</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.013289</td>\n",
       "      <td>0.592435</td>\n",
       "      <td>0.507778</td>\n",
       "      <td>0.568465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.842105</td>\n",
       "      <td>2.204082</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.764133</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.507778</td>\n",
       "      <td>0.596269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>0</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.773783</td>\n",
       "      <td>0.745679</td>\n",
       "      <td>0.481053</td>\n",
       "      <td>0.539052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>1</td>\n",
       "      <td>0.444445</td>\n",
       "      <td>2.61</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.738843</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>0.586208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>0</td>\n",
       "      <td>0.444445</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.89</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.218750</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.617154</td>\n",
       "      <td>0.577693</td>\n",
       "      <td>0.648681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>0</td>\n",
       "      <td>0.555555</td>\n",
       "      <td>2.71</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.689168</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>0.652180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333334</td>\n",
       "      <td>2.61</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>3.88</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.951220</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.605847</td>\n",
       "      <td>0.630479</td>\n",
       "      <td>0.577693</td>\n",
       "      <td>0.549332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>0</td>\n",
       "      <td>0.454546</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.500352</td>\n",
       "      <td>0.721605</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>0.534407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>1</td>\n",
       "      <td>0.238096</td>\n",
       "      <td>3.04</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.91</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.115385</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.590133</td>\n",
       "      <td>0.618986</td>\n",
       "      <td>0.577693</td>\n",
       "      <td>0.553274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>1</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>3.84</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.927273</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>0.700810</td>\n",
       "      <td>0.642275</td>\n",
       "      <td>0.577693</td>\n",
       "      <td>0.466231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>1</td>\n",
       "      <td>0.428572</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.30</td>\n",
       "      <td>3.780000</td>\n",
       "      <td>3.634615</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.751169</td>\n",
       "      <td>0.673610</td>\n",
       "      <td>0.577693</td>\n",
       "      <td>0.641048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333334</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.529304</td>\n",
       "      <td>0.654316</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>0.527501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>0</td>\n",
       "      <td>0.238096</td>\n",
       "      <td>2.71</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.12</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.547305</td>\n",
       "      <td>0.682745</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>0.487085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.883266</td>\n",
       "      <td>0.995597</td>\n",
       "      <td>0.551410</td>\n",
       "      <td>0.649311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>1</td>\n",
       "      <td>0.545454</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.764706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.93</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.297872</td>\n",
       "      <td>4.375000</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.814397</td>\n",
       "      <td>0.551410</td>\n",
       "      <td>0.587555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333334</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.292683</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588235</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.763029</td>\n",
       "      <td>0.777581</td>\n",
       "      <td>0.448590</td>\n",
       "      <td>0.495034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>1</td>\n",
       "      <td>0.125001</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>3.04</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.741935</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>0.576555</td>\n",
       "      <td>0.702899</td>\n",
       "      <td>0.448590</td>\n",
       "      <td>0.448037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.12</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.84</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.676370</td>\n",
       "      <td>0.602040</td>\n",
       "      <td>0.448590</td>\n",
       "      <td>0.479869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.218750</td>\n",
       "      <td>3.937500</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.454545</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.636036</td>\n",
       "      <td>0.663875</td>\n",
       "      <td>0.551410</td>\n",
       "      <td>0.554814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1727 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0.238096                   3.04   \n",
       "1717             1                       0.360000                   3.84   \n",
       "1718             1                       0.428572                   2.95   \n",
       "1719             0                       0.333334                   2.61   \n",
       "1720             0                       0.238096                   2.71   \n",
       "1721             1                       0.500000                   2.61   \n",
       "1722             1                       0.545454                   3.00   \n",
       "1723             0                       0.333334                   4.00   \n",
       "1724             1                       0.125001                   4.00   \n",
       "1725             1                       0.250000                   3.12   \n",
       "1726             1                       0.600000                   3.00   \n",
       "\n",
       "      last_3_week_home_st_ERA  last_week_home_re_ERA  last_week_home_runs_ave  \\\n",
       "0                    4.000000               4.000000                 3.500000   \n",
       "1                    4.000000               4.000000                 3.500000   \n",
       "2                    4.000000               4.000000                 3.500000   \n",
       "3                    4.000000               4.000000                 3.500000   \n",
       "4                    4.000000               4.000000                 3.500000   \n",
       "5                    4.000000               4.000000                 3.500000   \n",
       "6                    4.000000               4.500000                 3.375000   \n",
       "7                    4.000000               3.857143                 3.125000   \n",
       "8                    4.000000               0.000000                 2.875000   \n",
       "9                    4.000000               6.000000                 3.875000   \n",
       "10                   4.000000               0.000000                 3.375000   \n",
       "11                   4.000000               9.000000                 5.000000   \n",
       "12                   4.000000               3.375000                 4.000000   \n",
       "13                   4.000000               0.000000                 2.750000   \n",
       "14                   4.000000               0.000000                 4.250000   \n",
       "15                   4.000000               2.000000                 5.250000   \n",
       "16                   4.000000               0.000000                 4.000000   \n",
       "17                   4.000000              10.125000                 2.875000   \n",
       "18                   4.000000               7.363636                 2.375000   \n",
       "19                   4.000000               1.227273                 3.875000   \n",
       "20                   4.000000               5.400000                 2.625000   \n",
       "21                   4.000000               0.000000                 3.125000   \n",
       "22                   4.000000               0.900000                 3.250000   \n",
       "23                   4.000000               7.500000                 2.500000   \n",
       "24                   4.000000               7.714286                 2.000000   \n",
       "25                   4.000000               0.870968                 4.000000   \n",
       "26                   4.000000               3.240000                 5.000000   \n",
       "27                   4.000000               0.000000                 5.250000   \n",
       "28                   4.000000               2.076923                 3.000000   \n",
       "29                   4.000000               6.352941                 2.800000   \n",
       "...                       ...                    ...                      ...   \n",
       "1697                 1.285714               1.500000                 2.500000   \n",
       "1698                 6.000000               6.750000                 4.000000   \n",
       "1699                 8.470588               4.021277                 3.400000   \n",
       "1700                 1.956522               5.062500                 2.428571   \n",
       "1701                 2.045455               2.812500                 2.428571   \n",
       "1702                 1.125000               1.500000                 2.750000   \n",
       "1703                 4.000000               0.000000                 3.125000   \n",
       "1704                 3.919355               2.872340                 2.285714   \n",
       "1705                 0.000000               1.928571                 4.125000   \n",
       "1706                 1.125000               4.000000                 3.500000   \n",
       "1707                 4.500000               1.173913                 3.750000   \n",
       "1708                 3.176471               4.000000                 5.125000   \n",
       "1709                 0.000000               3.375000                 4.500000   \n",
       "1710                 1.000000               8.100000                 4.375000   \n",
       "1711                 4.000000               4.000000                 3.500000   \n",
       "1712                 4.000000               4.000000                 3.500000   \n",
       "1713                 4.000000               4.000000                 3.375000   \n",
       "1714                 4.500000               0.000000                 3.125000   \n",
       "1715                 1.800000               2.250000                 3.000000   \n",
       "1716                 4.000000               0.000000                 2.500000   \n",
       "1717                 4.500000               0.000000                 3.375000   \n",
       "1718                 3.600000               0.000000                 3.500000   \n",
       "1719                 0.000000               1.125000                 2.125000   \n",
       "1720                 7.200000               0.692308                 2.000000   \n",
       "1721                 1.500000               0.000000                 4.500000   \n",
       "1722                 4.764706               0.000000                 6.000000   \n",
       "1723                 3.000000               3.292683                 4.125000   \n",
       "1724                 2.571429               3.857143                 2.375000   \n",
       "1725                 7.500000               3.375000                 3.000000   \n",
       "1726                 4.218750               3.937500                 4.200000   \n",
       "\n",
       "      last_year_away_st_ERA  last_3_week_away_st_ERA  last_week_away_re_ERA  \\\n",
       "0                      3.11                 4.000000               4.000000   \n",
       "1                      2.52                 4.000000               4.000000   \n",
       "2                      3.27                 4.000000               4.000000   \n",
       "3                      3.22                 4.000000               4.000000   \n",
       "4                      3.19                 4.000000               4.000000   \n",
       "5                      2.24                 4.000000               4.000000   \n",
       "6                      3.31                 4.000000               9.000000   \n",
       "7                      3.92                 4.000000               0.000000   \n",
       "8                      4.12                 4.000000               4.500000   \n",
       "9                      4.00                 4.000000              10.800000   \n",
       "10                     3.51                 4.000000               0.000000   \n",
       "11                     4.00                 4.000000              10.500000   \n",
       "12                     3.81                 4.000000               0.000000   \n",
       "13                     2.89                 4.000000               1.928571   \n",
       "14                     3.17                 4.000000               1.000000   \n",
       "15                     2.48                 4.000000              12.150000   \n",
       "16                     4.79                 4.000000               6.750000   \n",
       "17                     2.40                 4.000000               1.928571   \n",
       "18                     2.94                 4.000000               6.428571   \n",
       "19                     4.64                 4.000000               0.000000   \n",
       "20                     2.38                 4.000000               9.310345   \n",
       "21                     4.58                 4.000000               7.000000   \n",
       "22                     3.02                 4.000000               2.076923   \n",
       "23                     3.22                 4.000000               1.862069   \n",
       "24                     4.00                 4.000000               7.000000   \n",
       "25                     4.00                 4.000000               1.038462   \n",
       "26                     3.35                 4.000000               9.236842   \n",
       "27                     3.44                 4.000000               9.529412   \n",
       "28                     4.22                 4.000000               3.000000   \n",
       "29                     1.91                 0.000000               6.394737   \n",
       "...                     ...                      ...                    ...   \n",
       "1697                   7.71                 4.000000               4.438356   \n",
       "1698                   2.66                 0.000000               3.857143   \n",
       "1699                   4.00                 3.000000               5.625000   \n",
       "1700                   2.95                 8.437500               0.000000   \n",
       "1701                   2.94                 3.000000               0.000000   \n",
       "1702                   4.00                 3.600000               6.230769   \n",
       "1703                   4.24                 4.000000               0.000000   \n",
       "1704                   4.00                 6.000000               1.125000   \n",
       "1705                   3.50                15.000000               4.000000   \n",
       "1706                   2.91                 0.900000               0.000000   \n",
       "1707                   2.93                 0.729730               0.000000   \n",
       "1708                   2.49                 1.800000               1.620000   \n",
       "1709                   4.30                 2.842105               2.204082   \n",
       "1710                   4.00                10.800000               3.375000   \n",
       "1711                   3.12                 0.000000               2.700000   \n",
       "1712                   3.89                 4.500000               4.218750   \n",
       "1713                   4.00                 1.285714               2.700000   \n",
       "1714                   3.88                 1.500000               3.951220   \n",
       "1715                   3.50                 3.000000               2.250000   \n",
       "1716                   2.91                 4.500000               3.115385   \n",
       "1717                   2.49                 0.600000               3.927273   \n",
       "1718                   4.30                 3.780000               3.634615   \n",
       "1719                   4.00                 1.285714               0.000000   \n",
       "1720                   3.12                 5.400000               0.000000   \n",
       "1721                   3.50                 1.636364               0.750000   \n",
       "1722                   2.93                 9.000000               2.297872   \n",
       "1723                   2.95                 0.000000               1.588235   \n",
       "1724                   3.04                 9.000000               1.741935   \n",
       "1725                   3.84                 3.857143               4.050000   \n",
       "1726                   2.93                 4.000000               2.454545   \n",
       "\n",
       "      last_week_away_runs_ave  last_week_home_team_OPS  \\\n",
       "0                    3.500000                      NaN   \n",
       "1                    3.500000                      NaN   \n",
       "2                    3.500000                      NaN   \n",
       "3                    3.500000                      NaN   \n",
       "4                    3.500000                      NaN   \n",
       "5                    3.500000                      NaN   \n",
       "6                    2.875000                 0.594156   \n",
       "7                    3.875000                 0.373412   \n",
       "8                    3.125000                 0.339216   \n",
       "9                    3.625000                 0.692308   \n",
       "10                   3.125000                 0.535897   \n",
       "11                   3.250000                 0.861111   \n",
       "12                   3.750000                 0.649306   \n",
       "13                   2.500000                 0.548352   \n",
       "14                   3.250000                 0.575808   \n",
       "15                   4.000000                 0.729040   \n",
       "16                   3.250000                 0.724994   \n",
       "17                   4.375000                 0.644709   \n",
       "18                   4.875000                 0.660802   \n",
       "19                   3.375000                 0.742424   \n",
       "20                   4.375000                 0.477204   \n",
       "21                   3.375000                 0.631907   \n",
       "22                   5.375000                 0.601265   \n",
       "23                   5.000000                 0.638343   \n",
       "24                   5.500000                 0.629363   \n",
       "25                   3.000000                 0.743018   \n",
       "26                   4.250000                 0.659954   \n",
       "27                   3.000000                 0.777708   \n",
       "28                   6.000000                 0.585737   \n",
       "29                   5.000000                 0.654464   \n",
       "...                       ...                      ...   \n",
       "1697                 2.428571                 0.577213   \n",
       "1698                 3.600000                 0.719700   \n",
       "1699                 2.166667                 0.595982   \n",
       "1700                 3.250000                 0.580309   \n",
       "1701                 6.125000                 0.599755   \n",
       "1702                 2.000000                 0.544608   \n",
       "1703                 1.625000                 0.647917   \n",
       "1704                 3.375000                 0.590054   \n",
       "1705                 3.500000                 0.867480   \n",
       "1706                 3.750000                      NaN   \n",
       "1707                 2.625000                 0.761905   \n",
       "1708                 2.000000                 1.013289   \n",
       "1709                 2.500000                 0.764133   \n",
       "1710                 5.000000                 0.773783   \n",
       "1711                 5.625000                      NaN   \n",
       "1712                 3.125000                      NaN   \n",
       "1713                 4.750000                 0.473214   \n",
       "1714                 3.000000                 0.605847   \n",
       "1715                 5.000000                 0.500352   \n",
       "1716                 2.800000                 0.590133   \n",
       "1717                 3.166667                 0.700810   \n",
       "1718                 3.666667                 0.751169   \n",
       "1719                 3.250000                 0.529304   \n",
       "1720                 3.400000                 0.547305   \n",
       "1721                 5.000000                 0.883266   \n",
       "1722                 4.375000                 0.873333   \n",
       "1723                 5.250000                 0.763029   \n",
       "1724                 5.125000                 0.576555   \n",
       "1725                 4.250000                 0.676370   \n",
       "1726                 3.400000                 0.636036   \n",
       "\n",
       "      last_week_away_team_OPS  last_year_home_win_rate_ratio  \\\n",
       "0                         NaN                       0.494845   \n",
       "1                         NaN                       0.526539   \n",
       "2                         NaN                       0.530108   \n",
       "3                         NaN                       0.386148   \n",
       "4                         NaN                       0.535906   \n",
       "5                         NaN                       0.478585   \n",
       "6                    0.522689                       0.494845   \n",
       "7                    0.818876                       0.526539   \n",
       "8                    0.427609                       0.530108   \n",
       "9                    0.649156                       0.535906   \n",
       "10                   0.455882                       0.478585   \n",
       "11                   0.654545                       0.494845   \n",
       "12                   0.724149                       0.526539   \n",
       "13                   0.580796                       0.530108   \n",
       "14                   0.546456                       0.386148   \n",
       "15                   0.666667                       0.535906   \n",
       "16                   0.498429                       0.478585   \n",
       "17                   0.750276                       0.520773   \n",
       "18                   0.862962                       0.452850   \n",
       "19                   0.682549                       0.474973   \n",
       "20                   0.660273                       0.563948   \n",
       "21                   0.547465                       0.558089   \n",
       "22                   0.747669                       0.564080   \n",
       "23                   0.725310                       0.520773   \n",
       "24                   0.875284                       0.452850   \n",
       "25                   0.673063                       0.474973   \n",
       "26                   0.643220                       0.563948   \n",
       "27                   0.530147                       0.558089   \n",
       "28                   0.797323                       0.564080   \n",
       "29                   0.824318                       0.452850   \n",
       "...                       ...                            ...   \n",
       "1697                 0.590856                       0.537095   \n",
       "1698                 0.612445                       0.598223   \n",
       "1699                 0.532753                       0.478824   \n",
       "1700                 0.571107                       0.422307   \n",
       "1701                 0.846418                       0.416353   \n",
       "1702                 0.474842                       0.558089   \n",
       "1703                 0.485641                       0.524684   \n",
       "1704                 0.602950                       0.462905   \n",
       "1705                      NaN                       0.481053   \n",
       "1706                 0.724615                       0.507778   \n",
       "1707                 0.294624                       0.481053   \n",
       "1708                 0.592435                       0.507778   \n",
       "1709                 0.617021                       0.507778   \n",
       "1710                 0.745679                       0.481053   \n",
       "1711                 0.738843                       0.561388   \n",
       "1712                 0.617154                       0.577693   \n",
       "1713                 0.689168                       0.561388   \n",
       "1714                 0.630479                       0.577693   \n",
       "1715                 0.721605                       0.561388   \n",
       "1716                 0.618986                       0.577693   \n",
       "1717                 0.642275                       0.577693   \n",
       "1718                 0.673610                       0.577693   \n",
       "1719                 0.654316                       0.561388   \n",
       "1720                 0.682745                       0.561388   \n",
       "1721                 0.995597                       0.551410   \n",
       "1722                 0.814397                       0.551410   \n",
       "1723                 0.777581                       0.448590   \n",
       "1724                 0.702899                       0.448590   \n",
       "1725                 0.602040                       0.448590   \n",
       "1726                 0.663875                       0.551410   \n",
       "\n",
       "      prediction for win  \n",
       "0               0.628540  \n",
       "1               0.638487  \n",
       "2               0.524378  \n",
       "3               0.634059  \n",
       "4               0.514458  \n",
       "5               0.450585  \n",
       "6               0.690814  \n",
       "7               0.612623  \n",
       "8               0.572305  \n",
       "9               0.573040  \n",
       "10              0.495181  \n",
       "11              0.523090  \n",
       "12              0.468971  \n",
       "13              0.623186  \n",
       "14              0.573892  \n",
       "15              0.555498  \n",
       "16              0.649650  \n",
       "17              0.498951  \n",
       "18              0.433435  \n",
       "19              0.542029  \n",
       "20              0.509040  \n",
       "21              0.617207  \n",
       "22              0.463941  \n",
       "23              0.416537  \n",
       "24              0.478875  \n",
       "25              0.512749  \n",
       "26              0.586867  \n",
       "27              0.571558  \n",
       "28              0.531918  \n",
       "29              0.396122  \n",
       "...                  ...  \n",
       "1697            0.660240  \n",
       "1698            0.679081  \n",
       "1699            0.612620  \n",
       "1700            0.563053  \n",
       "1701            0.484307  \n",
       "1702            0.638497  \n",
       "1703            0.604552  \n",
       "1704            0.562950  \n",
       "1705            0.622219  \n",
       "1706            0.511558  \n",
       "1707            0.471539  \n",
       "1708            0.568465  \n",
       "1709            0.596269  \n",
       "1710            0.539052  \n",
       "1711            0.586208  \n",
       "1712            0.648681  \n",
       "1713            0.652180  \n",
       "1714            0.549332  \n",
       "1715            0.534407  \n",
       "1716            0.553274  \n",
       "1717            0.466231  \n",
       "1718            0.641048  \n",
       "1719            0.527501  \n",
       "1720            0.487085  \n",
       "1721            0.649311  \n",
       "1722            0.587555  \n",
       "1723            0.495034  \n",
       "1724            0.448037  \n",
       "1725            0.479869  \n",
       "1726            0.554814  \n",
       "\n",
       "[1727 rows x 14 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([game_result_train.reset_index(drop=True),pd.DataFrame(oof_pred, columns=['prediction for win'])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 385.78125 277.314375 \nL 385.78125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \nL 378.58125 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m44df6a2878\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m44df6a2878\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(51.047869 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.872159\" xlink:href=\"#m44df6a2878\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(111.920597 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.744886\" xlink:href=\"#m44df6a2878\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(172.793324 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.617614\" xlink:href=\"#m44df6a2878\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(233.666051 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.490341\" xlink:href=\"#m44df6a2878\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(294.538778 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m44df6a2878\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(355.411506 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- False Positive Rate -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n     </defs>\n     <g transform=\"translate(163.975781 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"57.378906\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"118.658203\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"146.441406\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"198.541016\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"260.064453\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"291.851562\" xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"352.107422\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"413.289062\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"465.388672\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"493.171875\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"532.380859\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"560.164062\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"619.34375\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"680.867188\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"712.654297\" xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"782.105469\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"843.384766\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"882.59375\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"meb0a0fa73f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#meb0a0fa73f\" y=\"229.874489\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 233.673707)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#meb0a0fa73f\" y=\"190.339943\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 194.139162)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#meb0a0fa73f\" y=\"150.805398\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 154.604616)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#meb0a0fa73f\" y=\"111.270852\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 115.070071)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#meb0a0fa73f\" y=\"71.736307\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 75.535526)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#meb0a0fa73f\" y=\"32.201761\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.0 -->\n      <g transform=\"translate(20.878125 36.00098)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- True Positive Rate -->\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     </defs>\n     <g transform=\"translate(14.798438 176.584219)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"165.357422\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"226.880859\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"258.667969\" xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"318.923828\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"380.105469\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"432.205078\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"459.988281\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"499.197266\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"526.980469\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"586.160156\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"647.683594\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"679.470703\" xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"748.921875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"810.201172\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"849.410156\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p07148f5b24)\" d=\"M 58.999432 229.874489 \nL 58.999432 228.298836 \nL 59.505019 228.298836 \nL 59.505019 227.725872 \nL 59.757813 227.725872 \nL 59.757813 226.866425 \nL 60.516194 226.866425 \nL 60.516194 226.293461 \nL 61.021782 226.293461 \nL 61.021782 226.006979 \nL 61.527369 226.006979 \nL 61.527369 225.720497 \nL 61.780163 225.720497 \nL 61.780163 225.290773 \nL 62.791338 225.290773 \nL 62.791338 224.288085 \nL 63.802512 224.288085 \nL 63.802512 224.001603 \nL 64.055306 224.001603 \nL 64.055306 222.998916 \nL 64.3081 222.998916 \nL 64.3081 222.855674 \nL 64.813687 222.855674 \nL 64.813687 222.28271 \nL 65.066481 222.28271 \nL 65.066481 222.139469 \nL 65.319275 222.139469 \nL 65.319275 221.136781 \nL 65.572068 221.136781 \nL 65.572068 220.850299 \nL 65.824862 220.850299 \nL 65.824862 220.563817 \nL 66.077656 220.563817 \nL 66.077656 220.420576 \nL 66.33045 220.420576 \nL 66.33045 220.277334 \nL 67.088831 220.277334 \nL 67.088831 219.847611 \nL 67.341625 219.847611 \nL 67.341625 219.561129 \nL 67.594418 219.561129 \nL 67.594418 218.844923 \nL 68.352799 218.844923 \nL 68.352799 218.4152 \nL 68.858387 218.4152 \nL 68.858387 217.842236 \nL 69.111181 217.842236 \nL 69.111181 217.555753 \nL 69.363974 217.555753 \nL 69.363974 217.12603 \nL 70.122355 217.12603 \nL 70.122355 216.266583 \nL 70.375149 216.266583 \nL 70.375149 215.980101 \nL 71.386324 215.980101 \nL 71.386324 215.693619 \nL 71.639118 215.693619 \nL 71.639118 215.550378 \nL 72.397499 215.550378 \nL 72.397499 215.120655 \nL 72.650293 215.120655 \nL 72.650293 214.977414 \nL 72.903086 214.977414 \nL 72.903086 214.834172 \nL 73.661467 214.834172 \nL 73.661467 214.690931 \nL 74.167055 214.690931 \nL 74.167055 214.261208 \nL 74.672642 214.261208 \nL 74.672642 213.401761 \nL 74.925436 213.401761 \nL 74.925436 212.685556 \nL 75.17823 212.685556 \nL 75.17823 212.542315 \nL 75.936611 212.542315 \nL 75.936611 211.96935 \nL 76.189405 211.96935 \nL 76.189405 211.539627 \nL 76.442198 211.539627 \nL 76.442198 211.253145 \nL 76.694992 211.253145 \nL 76.694992 210.107216 \nL 77.706167 210.107216 \nL 77.706167 209.677493 \nL 77.958961 209.677493 \nL 77.958961 209.39101 \nL 78.717342 209.39101 \nL 78.717342 209.247769 \nL 79.222929 209.247769 \nL 79.222929 208.818046 \nL 79.475723 208.818046 \nL 79.475723 208.674805 \nL 79.728517 208.674805 \nL 79.728517 208.245082 \nL 79.98131 208.245082 \nL 79.98131 208.10184 \nL 80.234104 208.10184 \nL 80.234104 207.958599 \nL 80.739692 207.958599 \nL 80.739692 207.385635 \nL 80.992485 207.385635 \nL 80.992485 207.242394 \nL 81.245279 207.242394 \nL 81.245279 206.955912 \nL 81.498073 206.955912 \nL 81.498073 206.526188 \nL 81.750866 206.526188 \nL 81.750866 206.239706 \nL 82.256454 206.239706 \nL 82.256454 206.096465 \nL 82.509248 206.096465 \nL 82.509248 205.953224 \nL 83.014835 205.953224 \nL 83.014835 205.237018 \nL 83.267629 205.237018 \nL 83.267629 205.093777 \nL 83.520422 205.093777 \nL 83.520422 204.807295 \nL 83.773216 204.807295 \nL 83.773216 204.377572 \nL 84.02601 204.377572 \nL 84.02601 203.804607 \nL 85.037185 203.804607 \nL 85.037185 203.231643 \nL 85.542772 203.231643 \nL 85.542772 203.088402 \nL 85.795566 203.088402 \nL 85.795566 202.945161 \nL 86.04836 202.945161 \nL 86.04836 202.801919 \nL 86.301153 202.801919 \nL 86.301153 202.515437 \nL 86.553947 202.515437 \nL 86.553947 202.085714 \nL 86.806741 202.085714 \nL 86.806741 201.942473 \nL 87.312328 201.942473 \nL 87.312328 200.653303 \nL 87.817916 200.653303 \nL 87.817916 199.364133 \nL 88.323503 199.364133 \nL 88.323503 198.647927 \nL 89.081884 198.647927 \nL 89.081884 198.361445 \nL 89.587472 198.361445 \nL 89.587472 197.931722 \nL 89.840265 197.931722 \nL 89.840265 197.64524 \nL 90.345853 197.64524 \nL 90.345853 197.215516 \nL 90.598647 197.215516 \nL 90.598647 196.785793 \nL 90.85144 196.785793 \nL 90.85144 196.499311 \nL 91.357028 196.499311 \nL 91.357028 196.212829 \nL 91.609821 196.212829 \nL 91.609821 196.069587 \nL 92.115409 196.069587 \nL 92.115409 195.496623 \nL 92.368203 195.496623 \nL 92.368203 195.353382 \nL 92.620996 195.353382 \nL 92.620996 195.0669 \nL 92.87379 195.0669 \nL 92.87379 194.923659 \nL 94.89614 194.923659 \nL 94.89614 193.77773 \nL 95.148933 193.77773 \nL 95.148933 193.634489 \nL 95.401727 193.634489 \nL 95.401727 193.491248 \nL 95.907315 193.491248 \nL 95.907315 193.061524 \nL 96.160108 193.061524 \nL 96.160108 192.345319 \nL 96.91849 192.345319 \nL 96.91849 191.772354 \nL 97.676871 191.772354 \nL 97.676871 191.629113 \nL 98.182458 191.629113 \nL 98.182458 190.912908 \nL 98.435252 190.912908 \nL 98.435252 190.626425 \nL 99.193633 190.626425 \nL 99.193633 190.196702 \nL 99.446427 190.196702 \nL 99.446427 189.766979 \nL 100.204808 189.766979 \nL 100.204808 189.480497 \nL 100.457602 189.480497 \nL 100.457602 189.194014 \nL 100.963189 189.194014 \nL 100.963189 189.050773 \nL 101.72157 189.050773 \nL 101.72157 188.62105 \nL 102.732745 188.62105 \nL 102.732745 188.477809 \nL 103.238332 188.477809 \nL 103.238332 188.334568 \nL 103.74392 188.334568 \nL 103.74392 188.191327 \nL 103.996714 188.191327 \nL 103.996714 188.048085 \nL 104.249507 188.048085 \nL 104.249507 187.904844 \nL 104.502301 187.904844 \nL 104.502301 187.045398 \nL 104.755095 187.045398 \nL 104.755095 186.758916 \nL 105.007888 186.758916 \nL 105.007888 186.615674 \nL 105.76627 186.615674 \nL 105.76627 186.472433 \nL 106.019063 186.472433 \nL 106.019063 186.329192 \nL 106.524651 186.329192 \nL 106.524651 186.04271 \nL 107.535826 186.04271 \nL 107.535826 185.612987 \nL 107.788619 185.612987 \nL 107.788619 185.326504 \nL 108.041413 185.326504 \nL 108.041413 184.75354 \nL 108.294207 184.75354 \nL 108.294207 184.467058 \nL 108.547001 184.467058 \nL 108.547001 184.323817 \nL 109.305382 184.323817 \nL 109.305382 184.180576 \nL 109.558175 184.180576 \nL 109.558175 183.894093 \nL 109.810969 183.894093 \nL 109.810969 183.750852 \nL 110.063763 183.750852 \nL 110.063763 183.607611 \nL 110.316557 183.607611 \nL 110.316557 182.891406 \nL 110.56935 182.891406 \nL 110.56935 182.1752 \nL 110.822144 182.1752 \nL 110.822144 181.745477 \nL 111.327731 181.745477 \nL 111.327731 181.172512 \nL 111.580525 181.172512 \nL 111.580525 180.599548 \nL 111.833319 180.599548 \nL 111.833319 180.169825 \nL 112.5917 180.169825 \nL 112.5917 180.026583 \nL 113.602875 180.026583 \nL 113.602875 179.740101 \nL 113.855669 179.740101 \nL 113.855669 179.59686 \nL 114.108462 179.59686 \nL 114.108462 179.310378 \nL 115.119637 179.310378 \nL 115.119637 179.167137 \nL 115.625225 179.167137 \nL 115.625225 179.023896 \nL 115.878018 179.023896 \nL 115.878018 178.880655 \nL 116.130812 178.880655 \nL 116.130812 178.594172 \nL 116.383606 178.594172 \nL 116.383606 178.021208 \nL 117.394781 178.021208 \nL 117.394781 177.877967 \nL 117.900368 177.877967 \nL 117.900368 177.591485 \nL 118.153162 177.591485 \nL 118.153162 177.448244 \nL 118.405956 177.448244 \nL 118.405956 176.302315 \nL 118.911543 176.302315 \nL 118.911543 176.015833 \nL 119.41713 176.015833 \nL 119.41713 175.872591 \nL 119.922718 175.872591 \nL 119.922718 175.586109 \nL 120.681099 175.586109 \nL 120.681099 174.726663 \nL 120.933893 174.726663 \nL 120.933893 174.44018 \nL 121.43948 174.44018 \nL 121.43948 174.296939 \nL 122.197861 174.296939 \nL 122.197861 174.153698 \nL 122.450655 174.153698 \nL 122.450655 174.010457 \nL 122.703449 174.010457 \nL 122.703449 173.294251 \nL 122.956242 173.294251 \nL 122.956242 173.15101 \nL 123.46183 173.15101 \nL 123.46183 173.007769 \nL 123.967417 173.007769 \nL 123.967417 172.578046 \nL 124.220211 172.578046 \nL 124.220211 172.148323 \nL 124.978592 172.148323 \nL 124.978592 171.718599 \nL 125.231386 171.718599 \nL 125.231386 171.432117 \nL 125.48418 171.432117 \nL 125.48418 171.288876 \nL 125.989767 171.288876 \nL 125.989767 171.145635 \nL 126.748148 171.145635 \nL 126.748148 171.002394 \nL 127.506529 171.002394 \nL 127.506529 170.859153 \nL 127.759323 170.859153 \nL 127.759323 170.715912 \nL 128.517704 170.715912 \nL 128.517704 170.429429 \nL 128.770498 170.429429 \nL 128.770498 170.286188 \nL 129.781673 170.286188 \nL 129.781673 169.713224 \nL 130.28726 169.713224 \nL 130.28726 169.140259 \nL 130.540054 169.140259 \nL 130.540054 168.997018 \nL 131.298435 168.997018 \nL 131.298435 168.710536 \nL 131.551229 168.710536 \nL 131.551229 168.424054 \nL 131.804023 168.424054 \nL 131.804023 168.137572 \nL 132.056816 168.137572 \nL 132.056816 167.851089 \nL 132.30961 167.851089 \nL 132.30961 167.134884 \nL 132.562404 167.134884 \nL 132.562404 166.132196 \nL 133.320785 166.132196 \nL 133.320785 165.845714 \nL 133.826372 165.845714 \nL 133.826372 164.986267 \nL 134.079166 164.986267 \nL 134.079166 164.843026 \nL 134.33196 164.843026 \nL 134.33196 164.699785 \nL 134.837547 164.699785 \nL 134.837547 164.270062 \nL 135.090341 164.270062 \nL 135.090341 163.98358 \nL 135.595928 163.98358 \nL 135.595928 163.697097 \nL 136.607103 163.697097 \nL 136.607103 163.410615 \nL 136.859897 163.410615 \nL 136.859897 163.267374 \nL 137.112691 163.267374 \nL 137.112691 162.980892 \nL 137.365484 162.980892 \nL 137.365484 162.69441 \nL 137.871072 162.69441 \nL 137.871072 162.121445 \nL 138.629453 162.121445 \nL 138.629453 161.834963 \nL 138.882247 161.834963 \nL 138.882247 161.261999 \nL 139.13504 161.261999 \nL 139.13504 161.118757 \nL 139.640628 161.118757 \nL 139.640628 160.832275 \nL 140.146215 160.832275 \nL 140.146215 160.689034 \nL 140.651803 160.689034 \nL 140.651803 160.259311 \nL 141.662978 160.259311 \nL 141.662978 159.829587 \nL 141.915771 159.829587 \nL 141.915771 159.686346 \nL 142.421359 159.686346 \nL 142.421359 159.543105 \nL 142.674152 159.543105 \nL 142.674152 159.399864 \nL 142.926946 159.399864 \nL 142.926946 158.8269 \nL 143.17974 158.8269 \nL 143.17974 158.540417 \nL 143.432534 158.540417 \nL 143.432534 158.397176 \nL 143.685327 158.397176 \nL 143.685327 158.253935 \nL 143.938121 158.253935 \nL 143.938121 158.110694 \nL 144.190915 158.110694 \nL 144.190915 157.680971 \nL 144.696502 157.680971 \nL 144.696502 157.53773 \nL 145.20209 157.53773 \nL 145.20209 157.394489 \nL 145.454883 157.394489 \nL 145.454883 157.251248 \nL 145.707677 157.251248 \nL 145.707677 156.678283 \nL 146.466058 156.678283 \nL 146.466058 155.532354 \nL 146.718852 155.532354 \nL 146.718852 155.102631 \nL 146.971646 155.102631 \nL 146.971646 154.816149 \nL 147.477233 154.816149 \nL 147.477233 154.672908 \nL 147.730027 154.672908 \nL 147.730027 154.529667 \nL 148.235614 154.529667 \nL 148.235614 154.243184 \nL 148.488408 154.243184 \nL 148.488408 153.956702 \nL 148.741202 153.956702 \nL 148.741202 153.097255 \nL 148.993995 153.097255 \nL 148.993995 152.524291 \nL 149.246789 152.524291 \nL 149.246789 152.38105 \nL 150.763551 152.38105 \nL 150.763551 152.237809 \nL 151.774726 152.237809 \nL 151.774726 152.094568 \nL 152.280314 152.094568 \nL 152.280314 151.664844 \nL 152.533107 151.664844 \nL 152.533107 151.235121 \nL 153.797076 151.235121 \nL 153.797076 150.805398 \nL 154.302663 150.805398 \nL 154.302663 150.232433 \nL 154.808251 150.232433 \nL 154.808251 150.089192 \nL 155.313838 150.089192 \nL 155.313838 149.659469 \nL 155.819426 149.659469 \nL 155.819426 149.372987 \nL 156.830601 149.372987 \nL 156.830601 149.086504 \nL 157.083394 149.086504 \nL 157.083394 148.943263 \nL 158.094569 148.943263 \nL 158.094569 148.800022 \nL 158.347363 148.800022 \nL 158.347363 148.656781 \nL 158.600157 148.656781 \nL 158.600157 148.51354 \nL 158.85295 148.51354 \nL 158.85295 148.370299 \nL 159.105744 148.370299 \nL 159.105744 148.227058 \nL 159.358538 148.227058 \nL 159.358538 148.083817 \nL 160.369713 148.083817 \nL 160.369713 147.510852 \nL 161.128094 147.510852 \nL 161.128094 147.367611 \nL 161.380888 147.367611 \nL 161.380888 147.081129 \nL 161.633681 147.081129 \nL 161.633681 146.937888 \nL 162.139269 146.937888 \nL 162.139269 146.651406 \nL 162.392062 146.651406 \nL 162.392062 146.364923 \nL 164.161618 146.364923 \nL 164.161618 146.078441 \nL 165.172793 146.078441 \nL 165.172793 145.791959 \nL 165.678381 145.791959 \nL 165.678381 145.648718 \nL 165.931174 145.648718 \nL 165.931174 145.505477 \nL 166.436762 145.505477 \nL 166.436762 145.075753 \nL 166.689556 145.075753 \nL 166.689556 144.932512 \nL 167.195143 144.932512 \nL 167.195143 144.502789 \nL 167.447937 144.502789 \nL 167.447937 144.216307 \nL 167.700731 144.216307 \nL 167.700731 143.643342 \nL 169.72308 143.643342 \nL 169.72308 143.35686 \nL 169.975874 143.35686 \nL 169.975874 143.213619 \nL 170.228668 143.213619 \nL 170.228668 143.070378 \nL 170.481461 143.070378 \nL 170.481461 142.640655 \nL 170.987049 142.640655 \nL 170.987049 141.781208 \nL 171.239843 141.781208 \nL 171.239843 141.494726 \nL 171.998224 141.494726 \nL 171.998224 141.351485 \nL 172.251017 141.351485 \nL 172.251017 139.919074 \nL 173.009399 139.919074 \nL 173.009399 139.775833 \nL 173.262192 139.775833 \nL 173.262192 139.059627 \nL 173.514986 139.059627 \nL 173.514986 138.773145 \nL 173.76778 138.773145 \nL 173.76778 138.629904 \nL 174.020573 138.629904 \nL 174.020573 138.343421 \nL 174.273367 138.343421 \nL 174.273367 138.20018 \nL 174.526161 138.20018 \nL 174.526161 137.770457 \nL 175.284542 137.770457 \nL 175.284542 137.627216 \nL 175.537336 137.627216 \nL 175.537336 137.340734 \nL 175.790129 137.340734 \nL 175.790129 136.767769 \nL 176.042923 136.767769 \nL 176.042923 136.481287 \nL 176.295717 136.481287 \nL 176.295717 136.194805 \nL 176.548511 136.194805 \nL 176.548511 135.478599 \nL 176.801304 135.478599 \nL 176.801304 135.192117 \nL 177.054098 135.192117 \nL 177.054098 134.905635 \nL 177.559686 134.905635 \nL 177.559686 134.762394 \nL 177.812479 134.762394 \nL 177.812479 134.189429 \nL 178.318067 134.189429 \nL 178.318067 133.902947 \nL 179.076448 133.902947 \nL 179.076448 133.329983 \nL 179.329242 133.329983 \nL 179.329242 133.0435 \nL 179.582035 133.0435 \nL 179.582035 132.184054 \nL 179.834829 132.184054 \nL 179.834829 131.754331 \nL 180.087623 131.754331 \nL 180.087623 131.611089 \nL 180.340416 131.611089 \nL 180.340416 131.324607 \nL 180.846004 131.324607 \nL 180.846004 130.465161 \nL 181.098798 130.465161 \nL 181.098798 130.321919 \nL 181.604385 130.321919 \nL 181.604385 130.035437 \nL 182.109972 130.035437 \nL 182.109972 129.892196 \nL 182.61556 129.892196 \nL 182.61556 129.462473 \nL 182.868354 129.462473 \nL 182.868354 128.746267 \nL 183.373941 128.746267 \nL 183.373941 128.030062 \nL 183.626735 128.030062 \nL 183.626735 127.600338 \nL 184.132322 127.600338 \nL 184.132322 127.457097 \nL 184.63791 127.457097 \nL 184.63791 127.313856 \nL 184.890703 127.313856 \nL 184.890703 126.597651 \nL 185.143497 126.597651 \nL 185.143497 126.45441 \nL 185.396291 126.45441 \nL 185.396291 126.311168 \nL 185.901878 126.311168 \nL 185.901878 125.881445 \nL 186.660259 125.881445 \nL 186.660259 125.738204 \nL 186.913053 125.738204 \nL 186.913053 124.735516 \nL 187.418641 124.735516 \nL 187.418641 123.303105 \nL 188.177022 123.303105 \nL 188.177022 122.873382 \nL 188.682609 122.873382 \nL 188.682609 122.443659 \nL 189.188197 122.443659 \nL 189.188197 121.870694 \nL 189.44099 121.870694 \nL 189.44099 121.584212 \nL 189.693784 121.584212 \nL 189.693784 121.440971 \nL 189.946578 121.440971 \nL 189.946578 121.011248 \nL 190.199371 121.011248 \nL 190.199371 120.868006 \nL 190.452165 120.868006 \nL 190.452165 120.438283 \nL 190.957753 120.438283 \nL 190.957753 120.295042 \nL 191.210546 120.295042 \nL 191.210546 119.865319 \nL 191.716134 119.865319 \nL 191.716134 119.578836 \nL 192.221721 119.578836 \nL 192.221721 119.435595 \nL 192.727309 119.435595 \nL 192.727309 119.292354 \nL 194.496865 119.292354 \nL 194.496865 119.149113 \nL 195.508039 119.149113 \nL 195.508039 119.005872 \nL 196.519214 119.005872 \nL 196.519214 118.862631 \nL 197.530389 118.862631 \nL 197.530389 118.432908 \nL 198.28877 118.432908 \nL 198.28877 118.289667 \nL 198.541564 118.289667 \nL 198.541564 117.716702 \nL 199.047152 117.716702 \nL 199.047152 117.573461 \nL 199.299945 117.573461 \nL 199.299945 117.43022 \nL 199.805533 117.43022 \nL 199.805533 117.143738 \nL 200.058326 117.143738 \nL 200.058326 116.857255 \nL 201.575089 116.857255 \nL 201.575089 116.714014 \nL 201.827882 116.714014 \nL 201.827882 116.284291 \nL 202.080676 116.284291 \nL 202.080676 115.997809 \nL 202.33347 115.997809 \nL 202.33347 115.424844 \nL 203.091851 115.424844 \nL 203.091851 114.995121 \nL 203.597438 114.995121 \nL 203.597438 114.278916 \nL 204.103026 114.278916 \nL 204.103026 113.419469 \nL 204.35582 113.419469 \nL 204.35582 113.276228 \nL 204.608613 113.276228 \nL 204.608613 112.846504 \nL 204.861407 112.846504 \nL 204.861407 112.703263 \nL 205.114201 112.703263 \nL 205.114201 112.27354 \nL 205.619788 112.27354 \nL 205.619788 111.987058 \nL 206.125376 111.987058 \nL 206.125376 111.414093 \nL 206.378169 111.414093 \nL 206.378169 110.841129 \nL 206.630963 110.841129 \nL 206.630963 110.554647 \nL 206.883757 110.554647 \nL 206.883757 109.838441 \nL 207.136551 109.838441 \nL 207.136551 109.265477 \nL 207.642138 109.265477 \nL 207.642138 109.122236 \nL 207.894932 109.122236 \nL 207.894932 108.835753 \nL 208.653313 108.835753 \nL 208.653313 108.549271 \nL 208.906107 108.549271 \nL 208.906107 108.262789 \nL 209.1589 108.262789 \nL 209.1589 108.119548 \nL 209.917281 108.119548 \nL 209.917281 107.976307 \nL 210.170075 107.976307 \nL 210.170075 107.403342 \nL 210.675663 107.403342 \nL 210.675663 106.400655 \nL 211.434044 106.400655 \nL 211.434044 105.970931 \nL 211.686837 105.970931 \nL 211.686837 105.541208 \nL 211.939631 105.541208 \nL 211.939631 104.968244 \nL 212.445219 104.968244 \nL 212.445219 104.825002 \nL 212.950806 104.825002 \nL 212.950806 104.108797 \nL 213.2036 104.108797 \nL 213.2036 103.965556 \nL 213.709187 103.965556 \nL 213.709187 103.679074 \nL 214.214775 103.679074 \nL 214.214775 103.392591 \nL 214.467568 103.392591 \nL 214.467568 103.24935 \nL 214.973156 103.24935 \nL 214.973156 103.106109 \nL 215.225949 103.106109 \nL 215.225949 102.962868 \nL 215.731537 102.962868 \nL 215.731537 102.819627 \nL 216.742712 102.819627 \nL 216.742712 102.676386 \nL 217.501093 102.676386 \nL 217.501093 102.533145 \nL 217.753887 102.533145 \nL 217.753887 102.389904 \nL 218.512268 102.389904 \nL 218.512268 101.96018 \nL 219.017855 101.96018 \nL 219.017855 101.816939 \nL 219.270649 101.816939 \nL 219.270649 101.387216 \nL 220.02903 101.387216 \nL 220.02903 101.243975 \nL 221.040205 101.243975 \nL 221.040205 100.957493 \nL 222.556967 100.957493 \nL 222.556967 100.67101 \nL 223.062555 100.67101 \nL 223.062555 100.384528 \nL 223.315348 100.384528 \nL 223.315348 100.241287 \nL 223.820936 100.241287 \nL 223.820936 99.668323 \nL 224.326523 99.668323 \nL 224.326523 99.238599 \nL 224.579317 99.238599 \nL 224.579317 99.095358 \nL 225.084904 99.095358 \nL 225.084904 98.952117 \nL 226.096079 98.952117 \nL 226.096079 98.522394 \nL 226.601667 98.522394 \nL 226.601667 98.235912 \nL 227.360048 98.235912 \nL 227.360048 98.09267 \nL 228.118429 98.09267 \nL 228.118429 97.949429 \nL 229.129604 97.949429 \nL 229.129604 97.806188 \nL 229.635191 97.806188 \nL 229.635191 97.662947 \nL 231.151954 97.662947 \nL 231.151954 97.519706 \nL 231.404747 97.519706 \nL 231.404747 97.233224 \nL 232.163129 97.233224 \nL 232.163129 97.089983 \nL 232.415922 97.089983 \nL 232.415922 96.8035 \nL 232.92151 96.8035 \nL 232.92151 96.373777 \nL 233.174303 96.373777 \nL 233.174303 96.230536 \nL 233.679891 96.230536 \nL 233.679891 95.657572 \nL 233.932685 95.657572 \nL 233.932685 95.514331 \nL 234.185478 95.514331 \nL 234.185478 95.084607 \nL 234.691066 95.084607 \nL 234.691066 94.941366 \nL 234.943859 94.941366 \nL 234.943859 94.654884 \nL 235.449447 94.654884 \nL 235.449447 94.081919 \nL 235.702241 94.081919 \nL 235.702241 93.795437 \nL 235.955034 93.795437 \nL 235.955034 93.508955 \nL 236.207828 93.508955 \nL 236.207828 93.365714 \nL 236.460622 93.365714 \nL 236.460622 93.222473 \nL 236.713416 93.222473 \nL 236.713416 92.649508 \nL 237.72459 92.649508 \nL 237.72459 92.506267 \nL 238.482972 92.506267 \nL 238.482972 92.363026 \nL 239.241353 92.363026 \nL 239.241353 92.076544 \nL 239.999734 92.076544 \nL 239.999734 91.790062 \nL 240.252528 91.790062 \nL 240.252528 91.646821 \nL 240.758115 91.646821 \nL 240.758115 91.50358 \nL 241.010909 91.50358 \nL 241.010909 91.360338 \nL 241.263702 91.360338 \nL 241.263702 91.217097 \nL 241.516496 91.217097 \nL 241.516496 90.930615 \nL 242.780465 90.930615 \nL 242.780465 90.787374 \nL 243.286052 90.787374 \nL 243.286052 90.644133 \nL 244.044433 90.644133 \nL 244.044433 90.500892 \nL 244.297227 90.500892 \nL 244.297227 90.21441 \nL 244.802814 90.21441 \nL 244.802814 90.071168 \nL 245.308402 90.071168 \nL 245.308402 89.927927 \nL 246.319577 89.927927 \nL 246.319577 89.784686 \nL 247.077958 89.784686 \nL 247.077958 89.641445 \nL 247.330752 89.641445 \nL 247.330752 89.498204 \nL 247.836339 89.498204 \nL 247.836339 88.92524 \nL 248.089133 88.92524 \nL 248.089133 88.352275 \nL 248.341927 88.352275 \nL 248.341927 88.065793 \nL 249.353101 88.065793 \nL 249.353101 87.922552 \nL 250.364276 87.922552 \nL 250.364276 87.779311 \nL 251.375451 87.779311 \nL 251.375451 87.63607 \nL 251.881039 87.63607 \nL 251.881039 87.492829 \nL 252.133832 87.492829 \nL 252.133832 87.349587 \nL 252.63942 87.349587 \nL 252.63942 86.919864 \nL 253.903388 86.919864 \nL 253.903388 86.776623 \nL 254.156182 86.776623 \nL 254.156182 86.490141 \nL 254.914563 86.490141 \nL 254.914563 86.3469 \nL 255.167357 86.3469 \nL 255.167357 85.917176 \nL 255.925738 85.917176 \nL 255.925738 85.773935 \nL 256.431326 85.773935 \nL 256.431326 85.630694 \nL 256.684119 85.630694 \nL 256.684119 85.344212 \nL 256.936913 85.344212 \nL 256.936913 85.200971 \nL 257.189707 85.200971 \nL 257.189707 85.05773 \nL 257.4425 85.05773 \nL 257.4425 84.914489 \nL 257.695294 84.914489 \nL 257.695294 84.771248 \nL 258.200882 84.771248 \nL 258.200882 84.628006 \nL 258.453675 84.628006 \nL 258.453675 84.484765 \nL 258.959263 84.484765 \nL 258.959263 84.341524 \nL 259.212056 84.341524 \nL 259.212056 84.198283 \nL 259.717644 84.198283 \nL 259.717644 83.76856 \nL 259.970438 83.76856 \nL 259.970438 83.625319 \nL 260.223231 83.625319 \nL 260.223231 83.482078 \nL 260.476025 83.482078 \nL 260.476025 83.338836 \nL 260.981612 83.338836 \nL 260.981612 83.052354 \nL 261.234406 83.052354 \nL 261.234406 82.765872 \nL 261.992787 82.765872 \nL 261.992787 82.622631 \nL 262.498375 82.622631 \nL 262.498375 81.906425 \nL 262.751168 81.906425 \nL 262.751168 81.763184 \nL 263.003962 81.763184 \nL 263.003962 81.619943 \nL 264.520724 81.619943 \nL 264.520724 81.19022 \nL 265.026312 81.19022 \nL 265.026312 81.046979 \nL 265.279106 81.046979 \nL 265.279106 80.903738 \nL 265.784693 80.903738 \nL 265.784693 80.617255 \nL 266.543074 80.617255 \nL 266.543074 80.330773 \nL 267.301455 80.330773 \nL 267.301455 80.044291 \nL 267.807043 80.044291 \nL 267.807043 79.90105 \nL 268.059837 79.90105 \nL 268.059837 79.614568 \nL 268.565424 79.614568 \nL 268.565424 79.471327 \nL 268.818218 79.471327 \nL 268.818218 78.898362 \nL 269.071011 78.898362 \nL 269.071011 78.325398 \nL 269.323805 78.325398 \nL 269.323805 78.182157 \nL 269.829393 78.182157 \nL 269.829393 77.895674 \nL 270.840567 77.895674 \nL 270.840567 77.609192 \nL 271.093361 77.609192 \nL 271.093361 77.32271 \nL 271.346155 77.32271 \nL 271.346155 77.179469 \nL 271.598949 77.179469 \nL 271.598949 77.036228 \nL 271.851742 77.036228 \nL 271.851742 76.749746 \nL 272.104536 76.749746 \nL 272.104536 76.463263 \nL 272.610123 76.463263 \nL 272.610123 75.890299 \nL 273.368505 75.890299 \nL 273.368505 75.747058 \nL 273.621298 75.747058 \nL 273.621298 75.460576 \nL 273.874092 75.460576 \nL 273.874092 75.030852 \nL 274.632473 75.030852 \nL 274.632473 74.887611 \nL 275.138061 74.887611 \nL 275.138061 74.74437 \nL 275.390854 74.74437 \nL 275.390854 74.601129 \nL 275.643648 74.601129 \nL 275.643648 74.028165 \nL 276.149236 74.028165 \nL 276.149236 73.884923 \nL 276.402029 73.884923 \nL 276.402029 73.741682 \nL 276.907617 73.741682 \nL 276.907617 73.598441 \nL 277.16041 73.598441 \nL 277.16041 73.4552 \nL 277.665998 73.4552 \nL 277.665998 72.595753 \nL 278.171585 72.595753 \nL 278.171585 72.309271 \nL 279.435554 72.309271 \nL 279.435554 71.593066 \nL 280.193935 71.593066 \nL 280.193935 71.163342 \nL 281.457904 71.163342 \nL 281.457904 70.87686 \nL 282.469078 70.87686 \nL 282.469078 70.733619 \nL 283.480253 70.733619 \nL 283.480253 70.590378 \nL 283.733047 70.590378 \nL 283.733047 70.447137 \nL 283.985841 70.447137 \nL 283.985841 70.160655 \nL 284.997016 70.160655 \nL 284.997016 69.444449 \nL 285.249809 69.444449 \nL 285.249809 69.301208 \nL 286.008191 69.301208 \nL 286.008191 69.157967 \nL 286.513778 69.157967 \nL 286.513778 68.871485 \nL 287.272159 68.871485 \nL 287.272159 68.728244 \nL 287.524953 68.728244 \nL 287.524953 68.585002 \nL 287.777747 68.585002 \nL 287.777747 68.441761 \nL 288.283334 68.441761 \nL 288.283334 68.155279 \nL 289.041715 68.155279 \nL 289.041715 68.012038 \nL 289.294509 68.012038 \nL 289.294509 67.582315 \nL 289.547303 67.582315 \nL 289.547303 67.439074 \nL 290.305684 67.439074 \nL 290.305684 67.152591 \nL 291.064065 67.152591 \nL 291.064065 66.722868 \nL 291.316859 66.722868 \nL 291.316859 66.436386 \nL 291.569652 66.436386 \nL 291.569652 66.293145 \nL 291.822446 66.293145 \nL 291.822446 66.006663 \nL 292.07524 66.006663 \nL 292.07524 65.72018 \nL 292.328033 65.72018 \nL 292.328033 65.576939 \nL 292.580827 65.576939 \nL 292.580827 65.290457 \nL 292.833621 65.290457 \nL 292.833621 64.860734 \nL 293.339208 64.860734 \nL 293.339208 64.717493 \nL 294.097589 64.717493 \nL 294.097589 64.574251 \nL 294.603177 64.574251 \nL 294.603177 64.287769 \nL 294.855971 64.287769 \nL 294.855971 64.144528 \nL 295.361558 64.144528 \nL 295.361558 63.858046 \nL 296.119939 63.858046 \nL 296.119939 63.714805 \nL 297.131114 63.714805 \nL 297.131114 63.571564 \nL 297.383908 63.571564 \nL 297.383908 63.285082 \nL 297.889495 63.285082 \nL 297.889495 62.998599 \nL 298.395083 62.998599 \nL 298.395083 62.568876 \nL 298.90067 62.568876 \nL 298.90067 62.425635 \nL 299.153464 62.425635 \nL 299.153464 61.995912 \nL 299.911845 61.995912 \nL 299.911845 61.85267 \nL 300.670226 61.85267 \nL 300.670226 61.709429 \nL 300.92302 61.709429 \nL 300.92302 61.422947 \nL 301.681401 61.422947 \nL 301.681401 61.279706 \nL 301.934195 61.279706 \nL 301.934195 61.136465 \nL 302.439782 61.136465 \nL 302.439782 60.993224 \nL 302.692576 60.993224 \nL 302.692576 60.706742 \nL 302.94537 60.706742 \nL 302.94537 60.5635 \nL 303.198163 60.5635 \nL 303.198163 60.277018 \nL 303.450957 60.277018 \nL 303.450957 60.133777 \nL 303.703751 60.133777 \nL 303.703751 59.990536 \nL 303.956544 59.990536 \nL 303.956544 59.417572 \nL 304.462132 59.417572 \nL 304.462132 59.131089 \nL 304.714926 59.131089 \nL 304.714926 58.987848 \nL 305.220513 58.987848 \nL 305.220513 58.844607 \nL 306.484482 58.844607 \nL 306.484482 58.414884 \nL 307.74845 58.414884 \nL 307.74845 57.985161 \nL 308.001244 57.985161 \nL 308.001244 57.841919 \nL 309.518006 57.841919 \nL 309.518006 57.555437 \nL 310.023594 57.555437 \nL 310.023594 57.412196 \nL 310.276387 57.412196 \nL 310.276387 57.125714 \nL 310.781975 57.125714 \nL 310.781975 56.982473 \nL 311.540356 56.982473 \nL 311.540356 56.695991 \nL 311.79315 56.695991 \nL 311.79315 56.55275 \nL 312.298737 56.55275 \nL 312.298737 56.409508 \nL 312.804325 56.409508 \nL 312.804325 56.266267 \nL 313.057118 56.266267 \nL 313.057118 55.836544 \nL 313.309912 55.836544 \nL 313.309912 55.693303 \nL 313.562706 55.693303 \nL 313.562706 55.550062 \nL 314.321087 55.550062 \nL 314.321087 55.120338 \nL 315.585055 55.120338 \nL 315.585055 54.977097 \nL 315.837849 54.977097 \nL 315.837849 54.833856 \nL 316.849024 54.833856 \nL 316.849024 54.547374 \nL 317.101818 54.547374 \nL 317.101818 54.260892 \nL 318.61858 54.260892 \nL 318.61858 53.831168 \nL 318.871374 53.831168 \nL 318.871374 53.544686 \nL 319.124168 53.544686 \nL 319.124168 52.828481 \nL 319.629755 52.828481 \nL 319.629755 52.541999 \nL 320.388136 52.541999 \nL 320.388136 52.255516 \nL 320.64093 52.255516 \nL 320.64093 52.112275 \nL 321.399311 52.112275 \nL 321.399311 51.969034 \nL 321.904898 51.969034 \nL 321.904898 51.825793 \nL 322.157692 51.825793 \nL 322.157692 51.682552 \nL 322.410486 51.682552 \nL 322.410486 51.539311 \nL 322.66328 51.539311 \nL 322.66328 50.966346 \nL 323.168867 50.966346 \nL 323.168867 50.823105 \nL 324.685629 50.823105 \nL 324.685629 50.679864 \nL 324.938423 50.679864 \nL 324.938423 50.536623 \nL 325.696804 50.536623 \nL 325.696804 50.393382 \nL 326.202392 50.393382 \nL 326.202392 49.963659 \nL 327.46636 49.963659 \nL 327.46636 49.820417 \nL 328.224741 49.820417 \nL 328.224741 49.677176 \nL 328.730329 49.677176 \nL 328.730329 48.531248 \nL 328.983123 48.531248 \nL 328.983123 48.244765 \nL 329.235916 48.244765 \nL 329.235916 48.101524 \nL 329.994297 48.101524 \nL 329.994297 47.671801 \nL 331.258266 47.671801 \nL 331.258266 47.52856 \nL 331.763853 47.52856 \nL 331.763853 47.098836 \nL 332.522235 47.098836 \nL 332.522235 46.669113 \nL 332.775028 46.669113 \nL 332.775028 46.382631 \nL 333.027822 46.382631 \nL 333.027822 46.23939 \nL 333.533409 46.23939 \nL 333.533409 45.952908 \nL 333.786203 45.952908 \nL 333.786203 45.809667 \nL 334.544584 45.809667 \nL 334.544584 45.666425 \nL 334.797378 45.666425 \nL 334.797378 45.523184 \nL 335.302965 45.523184 \nL 335.302965 45.379943 \nL 335.555759 45.379943 \nL 335.555759 45.093461 \nL 335.808553 45.093461 \nL 335.808553 44.95022 \nL 336.061347 44.95022 \nL 336.061347 44.377255 \nL 336.31414 44.377255 \nL 336.31414 44.234014 \nL 337.072522 44.234014 \nL 337.072522 43.947532 \nL 337.578109 43.947532 \nL 337.578109 43.66105 \nL 338.083696 43.66105 \nL 338.083696 43.517809 \nL 338.33649 43.517809 \nL 338.33649 43.088085 \nL 339.347665 43.088085 \nL 339.347665 42.801603 \nL 339.600459 42.801603 \nL 339.600459 42.515121 \nL 340.35884 42.515121 \nL 340.35884 41.942157 \nL 341.117221 41.942157 \nL 341.117221 41.655674 \nL 341.622808 41.655674 \nL 341.622808 41.08271 \nL 341.875602 41.08271 \nL 341.875602 40.939469 \nL 342.128396 40.939469 \nL 342.128396 40.509746 \nL 342.886777 40.509746 \nL 342.886777 40.223263 \nL 343.139571 40.223263 \nL 343.139571 40.080022 \nL 343.897952 40.080022 \nL 343.897952 39.936781 \nL 344.403539 39.936781 \nL 344.403539 39.79354 \nL 345.920302 39.79354 \nL 345.920302 39.507058 \nL 346.173095 39.507058 \nL 346.173095 39.363817 \nL 346.425889 39.363817 \nL 346.425889 39.220576 \nL 346.678683 39.220576 \nL 346.678683 38.790852 \nL 347.689858 38.790852 \nL 347.689858 38.647611 \nL 348.448239 38.647611 \nL 348.448239 38.50437 \nL 349.459414 38.50437 \nL 349.459414 38.217888 \nL 349.712207 38.217888 \nL 349.712207 37.644923 \nL 349.965001 37.644923 \nL 349.965001 37.501682 \nL 350.217795 37.501682 \nL 350.217795 37.358441 \nL 350.723382 37.358441 \nL 350.723382 37.2152 \nL 350.976176 37.2152 \nL 350.976176 37.071959 \nL 352.492938 37.071959 \nL 352.492938 36.785477 \nL 352.998526 36.785477 \nL 352.998526 36.642236 \nL 353.504113 36.642236 \nL 353.504113 36.355753 \nL 355.020875 36.355753 \nL 355.020875 36.069271 \nL 355.273669 36.069271 \nL 355.273669 35.92603 \nL 355.779257 35.92603 \nL 355.779257 35.782789 \nL 356.790432 35.782789 \nL 356.790432 35.639548 \nL 358.0544 35.639548 \nL 358.0544 35.353066 \nL 359.065575 35.353066 \nL 359.065575 34.63686 \nL 359.823956 34.63686 \nL 359.823956 34.063896 \nL 360.07675 34.063896 \nL 360.07675 33.634172 \nL 360.329544 33.634172 \nL 360.329544 33.490931 \nL 360.582337 33.490931 \nL 360.582337 32.917967 \nL 361.340718 32.917967 \nL 361.340718 32.774726 \nL 361.593512 32.774726 \nL 361.593512 32.631485 \nL 361.846306 32.631485 \nL 361.846306 32.488244 \nL 362.0991 32.488244 \nL 362.0991 32.345002 \nL 362.857481 32.345002 \nL 362.857481 32.201761 \nL 363.363068 32.201761 \nL 363.363068 32.201761 \n\" style=\"fill:none;stroke:#ff8c00;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p07148f5b24)\" d=\"M 58.999432 229.874489 \nL 363.363068 32.201761 \n\" style=\"fill:none;stroke:#000080;stroke-dasharray:7.4,3.2;stroke-dashoffset:0;stroke-width:2;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 239.758125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 378.58125 239.758125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 239.758125 \nL 378.58125 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 378.58125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- Receiver operating characteristic example -->\n    <defs>\n     <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n    </defs>\n    <g transform=\"translate(83.51625 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"69.419922\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"130.943359\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"185.923828\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"247.447266\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"275.230469\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"334.410156\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"395.933594\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"437.046875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"468.833984\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"530.015625\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"593.492188\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"655.015625\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"696.128906\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"757.408203\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"796.617188\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"824.400391\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"887.779297\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"951.255859\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"983.042969\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1038.023438\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"1101.402344\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1162.681641\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1203.794922\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1265.074219\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1320.054688\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1359.263672\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1420.787109\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1461.900391\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1489.683594\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1541.783203\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1580.992188\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1608.775391\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1663.755859\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1695.542969\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1757.050781\" xlink:href=\"#DejaVuSans-120\"/>\n     <use x=\"1816.230469\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1877.509766\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1974.921875\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"2038.398438\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"2066.181641\" xlink:href=\"#DejaVuSans-101\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 216.08125 234.758125 \nL 371.58125 234.758125 \nQ 373.58125 234.758125 373.58125 232.758125 \nL 373.58125 219.08 \nQ 373.58125 217.08 371.58125 217.08 \nL 216.08125 217.08 \nQ 214.08125 217.08 214.08125 219.08 \nL 214.08125 232.758125 \nQ 214.08125 234.758125 216.08125 234.758125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_15\">\n     <path d=\"M 218.08125 225.178437 \nL 238.08125 225.178437 \n\" style=\"fill:none;stroke:#ff8c00;stroke-linecap:square;stroke-width:2;\"/>\n    </g>\n    <g id=\"line2d_16\"/>\n    <g id=\"text_16\">\n     <!-- ROC curve (area = 0.57) -->\n     <defs>\n      <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n      <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n      <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n      <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n      <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n     </defs>\n     <g transform=\"translate(246.08125 228.678437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"69.482422\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.193359\" xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"218.017578\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"249.804688\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"304.785156\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"368.164062\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"409.277344\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"468.457031\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"529.980469\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"561.767578\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"600.78125\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"662.060547\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"703.142578\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"764.666016\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"825.945312\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"857.732422\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"941.521484\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"973.308594\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"1036.931641\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"1068.71875\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"1132.341797\" xlink:href=\"#DejaVuSans-55\"/>\n      <use x=\"1195.964844\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p07148f5b24\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fpr, tpr,thred = metrics.roc_curve(y, oof_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_importance(models, feat_train_df):\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df['feature_importance'] = model.feature_importances_\n",
    "        _df['column'] = feat_train_df.columns\n",
    "        _df['fold'] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby('column')\\\n",
    "        .sum()[['feature_importance']]\\\n",
    "        .sort_values('feature_importance', ascending=False).index[:50]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(len(order) * .4, 7))\n",
    "    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    fig.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<Figure size 460.8x504 with 1 Axes>,\n <matplotlib.axes._subplots.AxesSubplot at 0x7f44c8d3d2b0>)"
     },
     "metadata": {},
     "execution_count": 232
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 460.8x504 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"497.039062pt\" version=\"1.1\" viewBox=\"0 0 452.7375 497.039062\" width=\"452.7375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 497.039062 \nL 452.7375 497.039062 \nL 452.7375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 47.24375 307.85 \nL 445.5375 307.85 \nL 445.5375 7.2 \nL 47.24375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m0dfade0047\" style=\"stroke:#46215f;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#46215f;stroke:#46215f;\" x=\"59.69043\" xlink:href=\"#m0dfade0047\" y=\"166.153438\"/>\n     <use style=\"fill:#46215f;stroke:#46215f;\" x=\"59.69043\" xlink:href=\"#m0dfade0047\" y=\"21.411256\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_1\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 59.612638 165.59283 \nL 59.768221 165.59283 \nL 59.768221 21.562188 \nL 59.612638 21.562188 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 59.534846 165.032223 \nL 59.846013 165.032223 \nL 59.846013 21.713121 \nL 59.534846 21.713121 \nz\n\" style=\"fill:#e5e0e8;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 59.379263 163.911007 \nL 60.001597 163.911007 \nL 60.001597 22.014987 \nL 59.379263 22.014987 \nz\n\" style=\"fill:#cabfd1;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 59.068096 161.668577 \nL 60.312764 161.668577 \nL 60.312764 22.618718 \nL 59.068096 22.618718 \nz\n\" style=\"fill:#b0a0bb;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 58.445762 157.183716 \nL 60.935098 157.183716 \nL 60.935098 23.826181 \nL 58.445762 23.826181 \nz\n\" style=\"fill:#9580a3;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 57.201094 148.213994 \nL 62.179766 148.213994 \nL 62.179766 26.241106 \nL 57.201094 26.241106 \nz\n\" style=\"fill:#7b618d;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 54.711758 128.894592 \nL 64.669102 128.894592 \nL 64.669102 31.224285 \nL 54.711758 31.224285 \nz\n\" style=\"fill:#604075;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 49.733086 91.022432 \nL 69.647773 91.022432 \nL 69.647773 45.023857 \nL 49.733086 45.023857 \nz\n\" style=\"fill:#46215f;\"/>\n   </g>\n   <g id=\"PathCollection_2\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"mdb29d8184d\" style=\"stroke:#4a3470;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#4a3470;stroke:#4a3470;\" x=\"84.583789\" xlink:href=\"#mdb29d8184d\" y=\"169.833324\"/>\n     <use style=\"fill:#4a3470;stroke:#4a3470;\" x=\"84.583789\" xlink:href=\"#mdb29d8184d\" y=\"61.890002\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_2\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 84.505997 169.143345 \nL 84.661581 169.143345 \nL 84.661581 61.933125 \nL 84.505997 61.933125 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 84.428206 168.453367 \nL 84.739373 168.453367 \nL 84.739373 61.976249 \nL 84.428206 61.976249 \nz\n\" style=\"fill:#e5e2eb;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 84.272622 167.073409 \nL 84.894956 167.073409 \nL 84.894956 62.062496 \nL 84.272622 62.062496 \nz\n\" style=\"fill:#cbc5d6;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 83.961455 164.313495 \nL 85.206123 164.313495 \nL 85.206123 62.234991 \nL 83.961455 62.234991 \nz\n\" style=\"fill:#b2a8c2;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 83.339121 158.793666 \nL 85.828457 158.793666 \nL 85.828457 62.57998 \nL 83.339121 62.57998 \nz\n\" style=\"fill:#978bad;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 82.094453 147.754008 \nL 87.073125 147.754008 \nL 87.073125 63.269959 \nL 82.094453 63.269959 \nz\n\" style=\"fill:#7e6e99;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 79.605117 130.427878 \nL 89.562461 130.427878 \nL 89.562461 66.949845 \nL 79.605117 66.949845 \nz\n\" style=\"fill:#645184;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 74.626445 129.047921 \nL 94.541133 129.047921 \nL 94.541133 87.342546 \nL 74.626445 87.342546 \nz\n\" style=\"fill:#4a3470;\"/>\n   </g>\n   <g id=\"PathCollection_3\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m721f377321\" style=\"stroke:#4b467b;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#4b467b;stroke:#4b467b;\" x=\"109.477148\" xlink:href=\"#m721f377321\" y=\"155.11378\"/>\n     <use style=\"fill:#4b467b;stroke:#4b467b;\" x=\"109.477148\" xlink:href=\"#m721f377321\" y=\"52.076972\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_3\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 109.399357 154.747229 \nL 109.55494 154.747229 \nL 109.55494 52.529771 \nL 109.399357 52.529771 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 109.321565 154.380678 \nL 109.632732 154.380678 \nL 109.632732 52.982569 \nL 109.321565 52.982569 \nz\n\" style=\"fill:#e6e5ec;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 109.165981 153.647575 \nL 109.788315 153.647575 \nL 109.788315 53.888166 \nL 109.165981 53.888166 \nz\n\" style=\"fill:#cbcad9;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 108.854814 152.181371 \nL 110.099482 152.181371 \nL 110.099482 55.69936 \nL 108.854814 55.69936 \nz\n\" style=\"fill:#b2b0c7;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 108.23248 149.248962 \nL 110.721816 149.248962 \nL 110.721816 59.321748 \nL 108.23248 59.321748 \nz\n\" style=\"fill:#9895b4;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 106.987813 143.384143 \nL 111.966484 143.384143 \nL 111.966484 66.566523 \nL 106.987813 66.566523 \nz\n\" style=\"fill:#7e7ba1;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 104.498477 133.034464 \nL 114.45582 133.034464 \nL 114.45582 81.056074 \nL 104.498477 81.056074 \nz\n\" style=\"fill:#64608e;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 99.519805 124.448063 \nL 119.434492 124.448063 \nL 119.434492 105.128662 \nL 99.519805 105.128662 \nz\n\" style=\"fill:#4b467b;\"/>\n   </g>\n   <g id=\"PathCollection_4\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"mc360137bd3\" style=\"stroke:#465380;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#465380;stroke:#465380;\" x=\"134.370508\" xlink:href=\"#mc360137bd3\" y=\"169.833324\"/>\n     <use style=\"fill:#465380;stroke:#465380;\" x=\"134.370508\" xlink:href=\"#mc360137bd3\" y=\"77.836174\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_4\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 134.292716 169.531458 \nL 134.4483 169.531458 \nL 134.4483 78.030231 \nL 134.292716 78.030231 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 134.214924 169.229593 \nL 134.526091 169.229593 \nL 134.526091 78.224287 \nL 134.214924 78.224287 \nz\n\" style=\"fill:#e5e7ed;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 134.059341 168.625861 \nL 134.681675 168.625861 \nL 134.681675 78.6124 \nL 134.059341 78.6124 \nz\n\" style=\"fill:#cacedb;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 133.748174 167.418399 \nL 134.992842 167.418399 \nL 134.992842 79.388626 \nL 133.748174 79.388626 \nz\n\" style=\"fill:#b0b6c9;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 133.12584 165.003474 \nL 135.615176 165.003474 \nL 135.615176 80.941078 \nL 133.12584 80.941078 \nz\n\" style=\"fill:#959db7;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 131.881172 160.173623 \nL 136.859844 160.173623 \nL 136.859844 84.045982 \nL 131.881172 84.045982 \nz\n\" style=\"fill:#7b84a5;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 129.391836 151.127237 \nL 139.34918 151.127237 \nL 139.34918 89.182489 \nL 129.391836 89.182489 \nz\n\" style=\"fill:#606b92;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 124.413164 140.394236 \nL 144.327852 140.394236 \nL 144.327852 94.702318 \nL 124.413164 94.702318 \nz\n\" style=\"fill:#465380;\"/>\n   </g>\n   <g id=\"PathCollection_5\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m904a3448de\" style=\"stroke:#405f82;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#405f82;stroke:#405f82;\" x=\"159.263867\" xlink:href=\"#m904a3448de\" y=\"164.926809\"/>\n     <use style=\"fill:#405f82;stroke:#405f82;\" x=\"159.263867\" xlink:href=\"#m904a3448de\" y=\"106.048633\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_5\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 159.186075 164.711191 \nL 159.341659 164.711191 \nL 159.341659 106.134881 \nL 159.186075 106.134881 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 159.108284 164.495573 \nL 159.419451 164.495573 \nL 159.419451 106.221128 \nL 159.108284 106.221128 \nz\n\" style=\"fill:#e4e8ed;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 158.9527 164.064336 \nL 159.575034 164.064336 \nL 159.575034 106.393623 \nL 158.9527 106.393623 \nz\n\" style=\"fill:#c8d1db;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 158.641533 163.201863 \nL 159.886201 163.201863 \nL 159.886201 106.738612 \nL 158.641533 106.738612 \nz\n\" style=\"fill:#adbaca;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 158.019199 161.476916 \nL 160.508535 161.476916 \nL 160.508535 107.428591 \nL 158.019199 107.428591 \nz\n\" style=\"fill:#92a3b7;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 156.774531 158.027023 \nL 161.753203 158.027023 \nL 161.753203 108.808548 \nL 156.774531 108.808548 \nz\n\" style=\"fill:#778da6;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 154.285195 151.740551 \nL 164.242539 151.740551 \nL 164.242539 112.94842 \nL 154.285195 112.94842 \nz\n\" style=\"fill:#5b7594;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 149.306523 143.767465 \nL 169.221211 143.767465 \nL 169.221211 127.514635 \nL 149.306523 127.514635 \nz\n\" style=\"fill:#405f82;\"/>\n   </g>\n   <g id=\"PathCollection_6\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"mf4b2436077\" style=\"stroke:#3a6982;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#3a6982;stroke:#3a6982;\" x=\"184.157227\" xlink:href=\"#mf4b2436077\" y=\"184.552868\"/>\n     <use style=\"fill:#3a6982;stroke:#3a6982;\" x=\"184.157227\" xlink:href=\"#mf4b2436077\" y=\"69.249773\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_6\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 184.079435 184.380373 \nL 184.235018 184.380373 \nL 184.235018 69.465392 \nL 184.079435 69.465392 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 184.001643 184.207879 \nL 184.31281 184.207879 \nL 184.31281 69.68101 \nL 184.001643 69.68101 \nz\n\" style=\"fill:#e3eaed;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 183.84606 183.862889 \nL 184.468394 183.862889 \nL 184.468394 70.112247 \nL 183.84606 70.112247 \nz\n\" style=\"fill:#c7d4db;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 183.534893 183.172911 \nL 184.779561 183.172911 \nL 184.779561 70.97472 \nL 183.534893 70.97472 \nz\n\" style=\"fill:#abbfca;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 182.912559 181.792953 \nL 185.401895 181.792953 \nL 185.401895 72.699667 \nL 182.912559 72.699667 \nz\n\" style=\"fill:#8ea9b7;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 181.667891 179.033039 \nL 186.646563 179.033039 \nL 186.646563 76.14956 \nL 181.667891 76.14956 \nz\n\" style=\"fill:#7394a6;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 179.178555 174.126524 \nL 189.135898 174.126524 \nL 189.135898 83.202675 \nL 179.178555 83.202675 \nz\n\" style=\"fill:#567e94;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 174.199883 169.833324 \nL 194.11457 169.833324 \nL 194.11457 101.142119 \nL 174.199883 101.142119 \nz\n\" style=\"fill:#3a6982;\"/>\n   </g>\n   <g id=\"PathCollection_7\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"mccb4781bef\" style=\"stroke:#357382;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#357382;stroke:#357382;\" x=\"209.050586\" xlink:href=\"#mccb4781bef\" y=\"207.858812\"/>\n     <use style=\"fill:#357382;stroke:#357382;\" x=\"209.050586\" xlink:href=\"#mccb4781bef\" y=\"129.354578\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_7\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 208.972794 207.643194 \nL 209.128378 207.643194 \nL 209.128378 129.419263 \nL 208.972794 129.419263 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 208.895002 207.427576 \nL 209.206169 207.427576 \nL 209.206169 129.483949 \nL 208.895002 129.483949 \nz\n\" style=\"fill:#e3ebed;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 208.739419 206.996339 \nL 209.361753 206.996339 \nL 209.361753 129.61332 \nL 208.739419 129.61332 \nz\n\" style=\"fill:#c5d7db;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 208.428252 206.133866 \nL 209.67292 206.133866 \nL 209.67292 129.872062 \nL 208.428252 129.872062 \nz\n\" style=\"fill:#a9c3c9;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 207.805918 204.408919 \nL 210.295254 204.408919 \nL 210.295254 130.389546 \nL 207.805918 130.389546 \nz\n\" style=\"fill:#8cafb7;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 206.56125 200.959026 \nL 211.539922 200.959026 \nL 211.539922 131.424514 \nL 206.56125 131.424514 \nz\n\" style=\"fill:#6f9ba5;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 204.071914 193.752583 \nL 214.029258 193.752583 \nL 214.029258 133.341121 \nL 204.071914 133.341121 \nz\n\" style=\"fill:#528793;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 199.093242 177.193096 \nL 219.00793 177.193096 \nL 219.00793 136.71435 \nL 199.093242 136.71435 \nz\n\" style=\"fill:#357382;\"/>\n   </g>\n   <g id=\"PathCollection_8\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m5f2db30126\" style=\"stroke:#307d80;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#307d80;stroke:#307d80;\" x=\"233.943945\" xlink:href=\"#m5f2db30126\" y=\"228.7115\"/>\n     <use style=\"fill:#307d80;stroke:#307d80;\" x=\"233.943945\" xlink:href=\"#m5f2db30126\" y=\"126.901321\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_8\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 233.866154 228.086207 \nL 234.021737 228.086207 \nL 234.021737 126.922882 \nL 233.866154 126.922882 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 233.788362 227.460913 \nL 234.099529 227.460913 \nL 234.099529 126.944444 \nL 233.788362 126.944444 \nz\n\" style=\"fill:#e2eded;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 233.632778 226.210327 \nL 234.255112 226.210327 \nL 234.255112 126.987568 \nL 233.632778 126.987568 \nz\n\" style=\"fill:#c4dadb;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 233.321611 223.709155 \nL 234.566279 223.709155 \nL 234.566279 127.073815 \nL 233.321611 127.073815 \nz\n\" style=\"fill:#a7c7c9;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 232.699277 218.70681 \nL 235.188613 218.70681 \nL 235.188613 127.24631 \nL 232.699277 127.24631 \nz\n\" style=\"fill:#89b4b6;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 231.454609 208.70212 \nL 236.433281 208.70212 \nL 236.433281 127.591299 \nL 231.454609 127.591299 \nz\n\" style=\"fill:#6ca2a5;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 228.965273 192.219297 \nL 238.922617 192.219297 \nL 238.922617 129.354578 \nL 228.965273 129.354578 \nz\n\" style=\"fill:#4e8f92;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 223.986602 184.859525 \nL 243.901289 184.859525 \nL 243.901289 137.940979 \nL 223.986602 137.940979 \nz\n\" style=\"fill:#307d80;\"/>\n   </g>\n   <g id=\"PathCollection_9\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m67b9c44082\" style=\"stroke:#2e887f;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#2e887f;stroke:#2e887f;\" x=\"258.837305\" xlink:href=\"#m67b9c44082\" y=\"223.804985\"/>\n     <use style=\"fill:#2e887f;stroke:#2e887f;\" x=\"258.837305\" xlink:href=\"#m67b9c44082\" y=\"128.127949\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_9\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 258.759513 223.503119 \nL 258.915096 223.503119 \nL 258.915096 128.214197 \nL 258.759513 128.214197 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 258.681721 223.201254 \nL 258.992888 223.201254 \nL 258.992888 128.300444 \nL 258.681721 128.300444 \nz\n\" style=\"fill:#e1eeed;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 258.526138 222.597522 \nL 259.148472 222.597522 \nL 259.148472 128.472939 \nL 258.526138 128.472939 \nz\n\" style=\"fill:#c3ddda;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 258.214971 221.39006 \nL 259.459639 221.39006 \nL 259.459639 128.817928 \nL 258.214971 128.817928 \nz\n\" style=\"fill:#a6ccc8;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 257.592637 218.975135 \nL 260.081973 218.975135 \nL 260.081973 129.507907 \nL 257.592637 129.507907 \nz\n\" style=\"fill:#87bbb6;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 256.347969 214.145284 \nL 261.326641 214.145284 \nL 261.326641 130.887864 \nL 256.347969 130.887864 \nz\n\" style=\"fill:#6aaaa4;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 253.858633 206.018869 \nL 263.815977 206.018869 \nL 263.815977 135.64105 \nL 253.858633 135.64105 \nz\n\" style=\"fill:#4b9991;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 248.879961 195.592526 \nL 268.794648 195.592526 \nL 268.794648 154.500466 \nL 248.879961 154.500466 \nz\n\" style=\"fill:#2e887f;\"/>\n   </g>\n   <g id=\"PathCollection_10\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m51855c61be\" style=\"stroke:#31957d;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#31957d;stroke:#31957d;\" x=\"283.730664\" xlink:href=\"#m51855c61be\" y=\"233.618014\"/>\n     <use style=\"fill:#31957d;stroke:#31957d;\" x=\"283.730664\" xlink:href=\"#m51855c61be\" y=\"109.728519\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_10\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 283.652872 233.574891 \nL 283.808456 233.574891 \nL 283.808456 109.836329 \nL 283.652872 109.836329 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 283.575081 233.531767 \nL 283.886248 233.531767 \nL 283.886248 109.944138 \nL 283.575081 109.944138 \nz\n\" style=\"fill:#e2f0ed;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 283.419497 233.44552 \nL 284.041831 233.44552 \nL 284.041831 110.159756 \nL 283.419497 110.159756 \nz\n\" style=\"fill:#c4e1da;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 283.10833 233.273025 \nL 284.352998 233.273025 \nL 284.352998 110.590993 \nL 283.10833 110.590993 \nz\n\" style=\"fill:#a7d2c7;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 282.485996 232.928036 \nL 284.975332 232.928036 \nL 284.975332 111.453466 \nL 282.485996 111.453466 \nz\n\" style=\"fill:#89c2b4;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 281.241328 232.238057 \nL 286.22 232.238057 \nL 286.22 113.178412 \nL 281.241328 113.178412 \nz\n\" style=\"fill:#6cb3a2;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 278.751992 231.011428 \nL 288.709336 231.011428 \nL 288.709336 118.928234 \nL 278.751992 118.928234 \nz\n\" style=\"fill:#4ea48f;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 273.77332 217.365185 \nL 293.688008 217.365185 \nL 293.688008 145.914065 \nL 273.77332 145.914065 \nz\n\" style=\"fill:#31957d;\"/>\n   </g>\n   <g id=\"PathCollection_11\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"md5e64a642a\" style=\"stroke:#3fa279;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#3fa279;stroke:#3fa279;\" x=\"308.624023\" xlink:href=\"#md5e64a642a\" y=\"247.11093\"/>\n     <use style=\"fill:#3fa279;stroke:#3fa279;\" x=\"308.624023\" xlink:href=\"#md5e64a642a\" y=\"90.102461\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_11\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 308.546232 246.916873 \nL 308.701815 246.916873 \nL 308.701815 90.943372 \nL 308.546232 90.943372 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 308.46844 246.722817 \nL 308.779607 246.722817 \nL 308.779607 91.784284 \nL 308.46844 91.784284 \nz\n\" style=\"fill:#e4f2ec;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 308.312856 246.334704 \nL 308.93519 246.334704 \nL 308.93519 93.466107 \nL 308.312856 93.466107 \nz\n\" style=\"fill:#c8e4d9;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 308.001689 245.558478 \nL 309.246357 245.558478 \nL 309.246357 96.829752 \nL 308.001689 96.829752 \nz\n\" style=\"fill:#add7c6;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 307.379355 244.006026 \nL 309.868691 244.006026 \nL 309.868691 103.557044 \nL 307.379355 103.557044 \nz\n\" style=\"fill:#91cab3;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 306.134688 240.901122 \nL 311.113359 240.901122 \nL 311.113359 117.011627 \nL 306.134688 117.011627 \nz\n\" style=\"fill:#76bda0;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 303.645352 231.9314 \nL 313.602695 231.9314 \nL 313.602695 140.547565 \nL 303.645352 140.547565 \nz\n\" style=\"fill:#5aaf8c;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 298.66668 200.49904 \nL 318.581367 200.49904 \nL 318.581367 159.713637 \nL 298.66668 159.713637 \nz\n\" style=\"fill:#3fa279;\"/>\n   </g>\n   <g id=\"PathCollection_12\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m8b6b4be6d3\" style=\"stroke:#55b174;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#55b174;stroke:#55b174;\" x=\"333.517383\" xlink:href=\"#m8b6b4be6d3\" y=\"209.085441\"/>\n     <use style=\"fill:#55b174;stroke:#55b174;\" x=\"333.517383\" xlink:href=\"#m8b6b4be6d3\" y=\"151.433894\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_12\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 333.439591 208.934508 \nL 333.595175 208.934508 \nL 333.595175 151.498579 \nL 333.439591 151.498579 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 333.361799 208.783575 \nL 333.672966 208.783575 \nL 333.672966 151.563265 \nL 333.361799 151.563265 \nz\n\" style=\"fill:#e7f4eb;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 333.206216 208.48171 \nL 333.82855 208.48171 \nL 333.82855 151.692636 \nL 333.206216 151.692636 \nz\n\" style=\"fill:#cee9d7;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 332.895049 207.877978 \nL 334.139717 207.877978 \nL 334.139717 151.951378 \nL 332.895049 151.951378 \nz\n\" style=\"fill:#b6ddc4;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 332.272715 206.670516 \nL 334.762051 206.670516 \nL 334.762051 152.468862 \nL 332.272715 152.468862 \nz\n\" style=\"fill:#9ed2b0;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 331.028047 204.255591 \nL 336.006719 204.255591 \nL 336.006719 153.50383 \nL 331.028047 153.50383 \nz\n\" style=\"fill:#86c79c;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 328.538711 200.345712 \nL 338.496055 200.345712 \nL 338.496055 156.953723 \nL 328.538711 156.953723 \nz\n\" style=\"fill:#6dbc88;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 323.560039 198.35244 \nL 343.474727 198.35244 \nL 343.474727 171.36661 \nL 323.560039 171.36661 \nz\n\" style=\"fill:#55b174;\"/>\n   </g>\n   <g id=\"PathCollection_13\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"m3d4cf3aae1\" style=\"stroke:#71be6b;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#71be6b;stroke:#71be6b;\" x=\"358.410742\" xlink:href=\"#m3d4cf3aae1\" y=\"245.884301\"/>\n     <use style=\"fill:#71be6b;stroke:#71be6b;\" x=\"358.410742\" xlink:href=\"#m3d4cf3aae1\" y=\"167.380067\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_13\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 358.33295 245.690244 \nL 358.488534 245.690244 \nL 358.488534 167.725056 \nL 358.33295 167.725056 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 358.255159 245.496188 \nL 358.566326 245.496188 \nL 358.566326 168.070045 \nL 358.255159 168.070045 \nz\n\" style=\"fill:#ebf6ea;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 358.099575 245.108075 \nL 358.721909 245.108075 \nL 358.721909 168.760024 \nL 358.099575 168.760024 \nz\n\" style=\"fill:#d6ecd5;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 357.788408 244.331849 \nL 359.033076 244.331849 \nL 359.033076 170.139981 \nL 357.788408 170.139981 \nz\n\" style=\"fill:#c2e3c0;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 357.166074 242.779397 \nL 359.65541 242.779397 \nL 359.65541 172.899896 \nL 357.166074 172.899896 \nz\n\" style=\"fill:#aedaab;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 355.921406 239.674493 \nL 360.900078 239.674493 \nL 360.900078 178.419725 \nL 355.921406 178.419725 \nz\n\" style=\"fill:#99d096;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 353.43207 232.851371 \nL 363.389414 232.851371 \nL 363.389414 187.619439 \nL 353.43207 187.619439 \nz\n\" style=\"fill:#85c780;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 348.453398 218.285156 \nL 368.368086 218.285156 \nL 368.368086 192.525954 \nL 348.453398 192.525954 \nz\n\" style=\"fill:#71be6b;\"/>\n   </g>\n   <g id=\"PathCollection_14\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"mc32310db54\" style=\"stroke:#8ac35a;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#8ac35a;stroke:#8ac35a;\" x=\"383.304102\" xlink:href=\"#mc32310db54\" y=\"255.69733\"/>\n     <use style=\"fill:#8ac35a;stroke:#8ac35a;\" x=\"383.304102\" xlink:href=\"#mc32310db54\" y=\"184.552868\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_14\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 383.22631 255.093599 \nL 383.381893 255.093599 \nL 383.381893 184.639115 \nL 383.22631 184.639115 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 383.148518 254.489868 \nL 383.459685 254.489868 \nL 383.459685 184.725362 \nL 383.148518 184.725362 \nz\n\" style=\"fill:#eef7e8;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 382.992935 253.282405 \nL 383.615269 253.282405 \nL 383.615269 184.897857 \nL 382.992935 184.897857 \nz\n\" style=\"fill:#deeed0;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 382.681768 250.86748 \nL 383.926436 250.86748 \nL 383.926436 185.242846 \nL 382.681768 185.242846 \nz\n\" style=\"fill:#cde6b8;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 382.059434 246.03763 \nL 384.54877 246.03763 \nL 384.54877 185.932825 \nL 382.059434 185.932825 \nz\n\" style=\"fill:#bcdda0;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 380.814766 236.377929 \nL 385.793438 236.377929 \nL 385.793438 187.312782 \nL 380.814766 187.312782 \nz\n\" style=\"fill:#acd589;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 378.32543 220.891742 \nL 388.282773 220.891742 \nL 388.282773 191.299325 \nL 378.32543 191.299325 \nz\n\" style=\"fill:#9bcc71;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 373.346758 217.365185 \nL 393.261445 217.365185 \nL 393.261445 204.792241 \nL 373.346758 204.792241 \nz\n\" style=\"fill:#8ac35a;\"/>\n   </g>\n   <g id=\"PathCollection_15\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"mf8cd8678b3\" style=\"stroke:#a5c744;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#a5c744;stroke:#a5c744;\" x=\"408.197461\" xlink:href=\"#mf8cd8678b3\" y=\"291.269562\"/>\n     <use style=\"fill:#a5c744;stroke:#a5c744;\" x=\"408.197461\" xlink:href=\"#mf8cd8678b3\" y=\"174.739839\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_15\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 408.119669 290.450212 \nL 408.275253 290.450212 \nL 408.275253 174.847648 \nL 408.119669 174.847648 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 408.041877 289.630862 \nL 408.353044 289.630862 \nL 408.353044 174.955457 \nL 408.041877 174.955457 \nz\n\" style=\"fill:#f2f7e5;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 407.886294 287.992163 \nL 408.508628 287.992163 \nL 408.508628 175.171075 \nL 407.886294 175.171075 \nz\n\" style=\"fill:#e5efca;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 407.575127 284.714765 \nL 408.819795 284.714765 \nL 408.819795 175.602312 \nL 407.575127 175.602312 \nz\n\" style=\"fill:#d9e7af;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 406.952793 278.159968 \nL 409.442129 278.159968 \nL 409.442129 176.464785 \nL 406.952793 176.464785 \nz\n\" style=\"fill:#ccdf94;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 405.708125 265.050374 \nL 410.686797 265.050374 \nL 410.686797 178.189732 \nL 405.708125 178.189732 \nz\n\" style=\"fill:#bfd77a;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 403.218789 244.657672 \nL 413.176133 244.657672 \nL 413.176133 184.092882 \nL 403.218789 184.092882 \nz\n\" style=\"fill:#b2cf5f;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 398.240117 244.044358 \nL 418.154805 244.044358 \nL 418.154805 207.552155 \nL 398.240117 207.552155 \nz\n\" style=\"fill:#a5c744;\"/>\n   </g>\n   <g id=\"PathCollection_16\">\n    <defs>\n     <path d=\"M -0 4.242641 \nL 2.545584 0 \nL 0 -4.242641 \nL -2.545584 -0 \nz\n\" id=\"ma01372379a\" style=\"stroke:#c1c933;\"/>\n    </defs>\n    <g clip-path=\"url(#p5adc665e59)\">\n     <use style=\"fill:#c1c933;stroke:#c1c933;\" x=\"433.09082\" xlink:href=\"#ma01372379a\" y=\"259.377216\"/>\n     <use style=\"fill:#c1c933;stroke:#c1c933;\" x=\"433.09082\" xlink:href=\"#ma01372379a\" y=\"164.926809\"/>\n    </g>\n   </g>\n   <g id=\"PatchCollection_16\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 433.013029 259.312531 \nL 433.168612 259.312531 \nL 433.168612 165.616788 \nL 433.013029 165.616788 \nz\n\" style=\"fill:#ffffff;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 432.935237 259.247845 \nL 433.246404 259.247845 \nL 433.246404 166.306766 \nL 432.935237 166.306766 \nz\n\" style=\"fill:#f6f7e2;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 432.779653 259.118474 \nL 433.401987 259.118474 \nL 433.401987 167.686724 \nL 432.779653 167.686724 \nz\n\" style=\"fill:#edf0c4;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 432.468486 258.859732 \nL 433.713154 258.859732 \nL 433.713154 170.446638 \nL 432.468486 170.446638 \nz\n\" style=\"fill:#e5e8a8;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 431.846152 258.342248 \nL 434.335488 258.342248 \nL 434.335488 175.966467 \nL 431.846152 175.966467 \nz\n\" style=\"fill:#dce08a;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 430.601484 257.30728 \nL 435.580156 257.30728 \nL 435.580156 187.006125 \nL 430.601484 187.006125 \nz\n\" style=\"fill:#d3d96d;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 428.112148 255.084016 \nL 438.069492 255.084016 \nL 438.069492 204.332255 \nL 428.112148 204.332255 \nz\n\" style=\"fill:#cad14f;\"/>\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 423.133477 250.484158 \nL 443.048164 250.484158 \nL 443.048164 208.16547 \nL 423.133477 208.16547 \nz\n\" style=\"fill:#c1c933;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1ddab8f4a6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.69043\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- last_week_home_OPS_ratio -->\n      <defs>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n       <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n       <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      </defs>\n      <g transform=\"translate(62.310742 450.817187)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"606.494141\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"667.675781\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"765.087891\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"826.611328\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"876.611328\" xlink:href=\"#DejaVuSans-79\"/>\n       <use x=\"955.322266\" xlink:href=\"#DejaVuSans-80\"/>\n       <use x=\"1015.625\" xlink:href=\"#DejaVuSans-83\"/>\n       <use x=\"1079.101562\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1129.101562\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1170.214844\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1231.494141\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1270.703125\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"1298.486328\" xlink:href=\"#DejaVuSans-111\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.583789\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- last_year_home_st_ERA_ratio -->\n      <defs>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      </defs>\n      <g transform=\"translate(87.204102 460.834375)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"289.550781\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"351.074219\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"412.353516\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"453.466797\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"503.466797\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"566.845703\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"628.027344\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"725.439453\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"786.962891\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"836.962891\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"889.0625\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"928.271484\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"978.271484\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1041.455078\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1110.875\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"1179.283203\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1229.283203\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1270.396484\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1331.675781\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1370.884766\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"1398.667969\" xlink:href=\"#DejaVuSans-111\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.477148\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- last_week_away_team_OPS -->\n      <g transform=\"translate(112.097461 451.70625)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"604.394531\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"686.181641\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"747.460938\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"806.640625\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"856.640625\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"895.849609\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"957.373047\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1018.652344\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"1116.064453\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1166.064453\" xlink:href=\"#DejaVuSans-79\"/>\n       <use x=\"1244.775391\" xlink:href=\"#DejaVuSans-80\"/>\n       <use x=\"1305.078125\" xlink:href=\"#DejaVuSans-83\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"134.370508\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- last_3_week_away_st_ERA -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(136.99082 446.107812)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"293.994141\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"343.994141\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"425.78125\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"487.304688\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"548.828125\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"606.738281\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"656.738281\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"718.017578\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"799.804688\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"861.083984\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"920.263672\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"970.263672\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"1022.363281\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1061.572266\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1111.572266\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1174.755859\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1244.175781\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"159.263867\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- last_3_week_home_st_ERA_ratio -->\n      <g transform=\"translate(161.88418 476.160937)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"293.994141\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"343.994141\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"425.78125\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"487.304688\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"548.828125\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"606.738281\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"656.738281\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"720.117188\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"781.298828\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"878.710938\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"940.234375\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"990.234375\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"1042.333984\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1081.542969\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1131.542969\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1194.726562\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1264.146484\" xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"1332.554688\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1382.554688\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1423.667969\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1484.947266\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1524.15625\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"1551.939453\" xlink:href=\"#DejaVuSans-111\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"184.157227\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- last_3_week_home_st_ERA -->\n      <g transform=\"translate(186.777539 448.104687)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"293.994141\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"343.994141\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"425.78125\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"487.304688\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"548.828125\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"606.738281\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"656.738281\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"720.117188\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"781.298828\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"878.710938\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"940.234375\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"990.234375\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"1042.333984\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1081.542969\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1131.542969\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1194.726562\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1264.146484\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"209.050586\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- last_week_home_win_rate_ratio -->\n      <defs>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <g transform=\"translate(211.670898 473.173437)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"606.494141\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"667.675781\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"765.087891\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"826.611328\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"876.611328\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"958.398438\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"986.181641\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"1049.560547\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1099.560547\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1140.673828\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1201.953125\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1241.162109\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"1302.685547\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1352.685547\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1393.798828\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1455.078125\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1494.287109\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"1522.070312\" xlink:href=\"#DejaVuSans-111\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"233.943945\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- last_week_home_re_ERA -->\n      <g transform=\"translate(236.564258 437.873437)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"606.494141\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"667.675781\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"765.087891\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"826.611328\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"876.611328\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"917.693359\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"979.216797\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1029.216797\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1092.400391\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1161.820312\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"258.837305\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- last_week_home_team_OPS -->\n      <g transform=\"translate(261.457617 453.703125)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"606.494141\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"667.675781\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"765.087891\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"826.611328\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"876.611328\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"915.820312\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"977.34375\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1038.623047\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"1136.035156\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1186.035156\" xlink:href=\"#DejaVuSans-79\"/>\n       <use x=\"1264.746094\" xlink:href=\"#DejaVuSans-80\"/>\n       <use x=\"1325.048828\" xlink:href=\"#DejaVuSans-83\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"283.730664\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- last_year_home_st_ERA -->\n      <g transform=\"translate(286.350977 432.778125)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"289.550781\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"351.074219\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"412.353516\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"453.466797\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"503.466797\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"566.845703\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"628.027344\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"725.439453\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"786.962891\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"836.962891\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"889.0625\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"928.271484\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"978.271484\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1041.455078\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1110.875\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"308.624023\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- last_week_away_re_ERA -->\n      <g transform=\"translate(311.244336 435.876562)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"604.394531\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"686.181641\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"747.460938\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"806.640625\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"856.640625\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"897.722656\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"959.246094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1009.246094\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1072.429688\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1141.849609\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.517383\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- last_week_home_runs_ave -->\n      <defs>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      </defs>\n      <g transform=\"translate(336.137695 447.70625)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"606.494141\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"667.675781\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"765.087891\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"826.611328\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"876.611328\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"917.724609\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"981.103516\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"1044.482422\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"1096.582031\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1146.582031\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1207.861328\" xlink:href=\"#DejaVuSans-118\"/>\n       <use x=\"1267.041016\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"358.410742\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- last_year_away_st_ERA -->\n      <g transform=\"translate(361.031055 430.78125)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"289.550781\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"351.074219\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"412.353516\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"453.466797\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"503.466797\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"564.746094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"646.533203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"707.8125\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"766.992188\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"816.992188\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"869.091797\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"908.300781\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"958.300781\" xlink:href=\"#DejaVuSans-69\"/>\n       <use x=\"1021.484375\" xlink:href=\"#DejaVuSans-82\"/>\n       <use x=\"1090.904297\" xlink:href=\"#DejaVuSans-65\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"383.304102\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- last_year_away_win_rate -->\n      <g transform=\"translate(385.924414 439.15625)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"289.550781\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"351.074219\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"412.353516\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"453.466797\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"503.466797\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"564.746094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"646.533203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"707.8125\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"766.992188\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"816.992188\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"898.779297\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"926.5625\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"989.941406\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1039.941406\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1081.054688\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1142.333984\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1181.542969\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"408.197461\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- last_year_home_win_rate -->\n      <g transform=\"translate(410.817773 441.153125)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"289.550781\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"351.074219\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"412.353516\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"453.466797\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"503.466797\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"566.845703\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"628.027344\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"725.439453\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"786.962891\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"836.962891\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"918.75\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"946.533203\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"1009.912109\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1059.912109\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"1101.025391\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1162.304688\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"1201.513672\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"433.09082\" xlink:href=\"#m1ddab8f4a6\" y=\"307.85\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- last_week_away_runs_ave -->\n      <g transform=\"translate(435.711133 445.709375)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"89.0625\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"141.162109\" xlink:href=\"#DejaVuSans-116\"/>\n       <use x=\"180.371094\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"230.371094\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"312.158203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"373.681641\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"435.205078\" xlink:href=\"#DejaVuSans-107\"/>\n       <use x=\"493.115234\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"543.115234\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"604.394531\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"686.181641\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"747.460938\" xlink:href=\"#DejaVuSans-121\"/>\n       <use x=\"806.640625\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"856.640625\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"897.753906\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"961.132812\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"1024.511719\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"1076.611328\" xlink:href=\"#DejaVuSans-95\"/>\n       <use x=\"1126.611328\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"1187.890625\" xlink:href=\"#DejaVuSans-118\"/>\n       <use x=\"1247.070312\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- column -->\n     <defs>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n     </defs>\n     <g transform=\"translate(227.985156 487.759375)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"54.980469\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"116.162109\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"143.945312\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"207.324219\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"304.736328\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_17\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m63d665166b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.24375\" xlink:href=\"#m63d665166b\" y=\"261.830474\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(21.15625 265.629692)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.24375\" xlink:href=\"#m63d665166b\" y=\"200.49904\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 250 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(21.15625 204.298259)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.24375\" xlink:href=\"#m63d665166b\" y=\"139.167607\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 300 -->\n      <g transform=\"translate(21.15625 142.966826)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.24375\" xlink:href=\"#m63d665166b\" y=\"77.836174\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 350 -->\n      <g transform=\"translate(21.15625 81.635393)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.24375\" xlink:href=\"#m63d665166b\" y=\"16.504741\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(21.15625 20.30396)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_23\">\n     <!-- feature_importance -->\n     <defs>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     </defs>\n     <g transform=\"translate(14.798438 206.752344)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"35.205078\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"96.728516\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"158.007812\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"197.216797\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"260.595703\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"301.677734\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"363.201172\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"413.201172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"440.984375\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"538.396484\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"601.873047\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"663.054688\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"704.167969\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"743.376953\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"804.65625\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"868.035156\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"923.015625\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_22\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 49.733086 62.503316 \nL 69.647773 62.503316 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 74.626445 114.635034 \nL 94.541133 114.635034 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 99.519805 119.541549 \nL 119.434492 119.541549 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 124.413164 120.768177 \nL 144.327852 120.768177 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 149.306523 136.101036 \nL 169.221211 136.101036 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 174.199883 161.246923 \nL 194.11457 161.246923 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 199.093242 157.567037 \nL 219.00793 157.567037 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 223.986602 166.766752 \nL 243.901289 166.766752 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 248.879961 167.380067 \nL 268.794648 167.380067 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 273.77332 168.606695 \nL 293.688008 168.606695 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 298.66668 179.646353 \nL 318.581367 179.646353 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 323.560039 188.846068 \nL 343.474727 188.846068 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 348.453398 206.632184 \nL 368.368086 206.632184 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 373.346758 211.538698 \nL 393.261445 211.538698 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 398.240117 226.871557 \nL 418.154805 226.871557 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p5adc665e59)\" d=\"M 423.133477 226.258242 \nL 443.048164 226.258242 \n\" style=\"fill:none;stroke:#262626;stroke-linecap:square;stroke-opacity:0.45;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 47.24375 307.85 \nL 47.24375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 445.5375 307.85 \nL 445.5375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 47.24375 307.85 \nL 445.5375 307.85 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 47.24375 7.2 \nL 445.5375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5adc665e59\">\n   <rect height=\"300.65\" width=\"398.29375\" x=\"47.24375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "visualize_importance(models, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "5       1\n",
       "6       1\n",
       "7       1\n",
       "8       1\n",
       "10      1\n",
       "11      1\n",
       "12      1\n",
       "13      1\n",
       "14      1\n",
       "15      0\n",
       "16      0\n",
       "17      0\n",
       "18      0\n",
       "19      0\n",
       "20      1\n",
       "21      1\n",
       "22      1\n",
       "23      0\n",
       "24      1\n",
       "25      1\n",
       "26      0\n",
       "27      0\n",
       "28      0\n",
       "29      1\n",
       "31      0\n",
       "       ..\n",
       "1732    1\n",
       "1733    1\n",
       "1734    0\n",
       "1735    1\n",
       "1736    0\n",
       "1737    1\n",
       "1738    1\n",
       "1739    1\n",
       "1740    1\n",
       "1741    1\n",
       "1742    0\n",
       "1743    0\n",
       "1744    0\n",
       "1745    0\n",
       "1746    1\n",
       "1747    0\n",
       "1748    0\n",
       "1749    0\n",
       "1750    0\n",
       "1751    1\n",
       "1752    1\n",
       "1753    1\n",
       "1754    0\n",
       "1755    0\n",
       "1756    1\n",
       "1757    1\n",
       "1758    0\n",
       "1759    1\n",
       "1760    1\n",
       "1761    1\n",
       "Name: home_victory, Length: 1727, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_result_train2 = game_result_train.copy()\n",
    "game_result_train2['home_victory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0]\tvalidation_0-logloss:0.69238\tvalidation_1-logloss:0.69307\n[50]\tvalidation_0-logloss:0.65440\tvalidation_1-logloss:0.69170\n[100]\tvalidation_0-logloss:0.62294\tvalidation_1-logloss:0.69214\n[150]\tvalidation_0-logloss:0.59556\tvalidation_1-logloss:0.69281\n[200]\tvalidation_0-logloss:0.57300\tvalidation_1-logloss:0.69508\n[250]\tvalidation_0-logloss:0.55172\tvalidation_1-logloss:0.69930\n[299]\tvalidation_0-logloss:0.53451\tvalidation_1-logloss:0.70026\n[0]\tvalidation_0-logloss:0.69235\tvalidation_1-logloss:0.69331\n[50]\tvalidation_0-logloss:0.65366\tvalidation_1-logloss:0.68954\n[100]\tvalidation_0-logloss:0.62166\tvalidation_1-logloss:0.68814\n[150]\tvalidation_0-logloss:0.59574\tvalidation_1-logloss:0.68838\n[200]\tvalidation_0-logloss:0.57129\tvalidation_1-logloss:0.69069\n[250]\tvalidation_0-logloss:0.55054\tvalidation_1-logloss:0.69234\n[299]\tvalidation_0-logloss:0.53325\tvalidation_1-logloss:0.69383\n[0]\tvalidation_0-logloss:0.69246\tvalidation_1-logloss:0.69337\n[50]\tvalidation_0-logloss:0.65466\tvalidation_1-logloss:0.68549\n[100]\tvalidation_0-logloss:0.62338\tvalidation_1-logloss:0.68124\n[150]\tvalidation_0-logloss:0.59688\tvalidation_1-logloss:0.67707\n[200]\tvalidation_0-logloss:0.57394\tvalidation_1-logloss:0.67465\n[250]\tvalidation_0-logloss:0.55366\tvalidation_1-logloss:0.67212\n[299]\tvalidation_0-logloss:0.53716\tvalidation_1-logloss:0.67011\n[0]\tvalidation_0-logloss:0.69234\tvalidation_1-logloss:0.69327\n[50]\tvalidation_0-logloss:0.65514\tvalidation_1-logloss:0.68887\n[100]\tvalidation_0-logloss:0.62338\tvalidation_1-logloss:0.68883\n[150]\tvalidation_0-logloss:0.59864\tvalidation_1-logloss:0.68936\n[200]\tvalidation_0-logloss:0.57556\tvalidation_1-logloss:0.69218\n[250]\tvalidation_0-logloss:0.55448\tvalidation_1-logloss:0.69356\n[299]\tvalidation_0-logloss:0.53749\tvalidation_1-logloss:0.69449\n[0]\tvalidation_0-logloss:0.69239\tvalidation_1-logloss:0.69305\n[50]\tvalidation_0-logloss:0.65575\tvalidation_1-logloss:0.68563\n[100]\tvalidation_0-logloss:0.62663\tvalidation_1-logloss:0.68055\n[150]\tvalidation_0-logloss:0.60155\tvalidation_1-logloss:0.67750\n[200]\tvalidation_0-logloss:0.57881\tvalidation_1-logloss:0.67621\n[250]\tvalidation_0-logloss:0.55956\tvalidation_1-logloss:0.67619\n[299]\tvalidation_0-logloss:0.54222\tvalidation_1-logloss:0.67540\n[0]\tvalidation_0-logloss:0.69235\tvalidation_1-logloss:0.69298\n[50]\tvalidation_0-logloss:0.65543\tvalidation_1-logloss:0.68659\n[100]\tvalidation_0-logloss:0.62455\tvalidation_1-logloss:0.68103\n[150]\tvalidation_0-logloss:0.59891\tvalidation_1-logloss:0.67976\n[200]\tvalidation_0-logloss:0.57660\tvalidation_1-logloss:0.67705\n[250]\tvalidation_0-logloss:0.55532\tvalidation_1-logloss:0.67618\n[299]\tvalidation_0-logloss:0.53704\tvalidation_1-logloss:0.67567\n[0]\tvalidation_0-logloss:0.69233\tvalidation_1-logloss:0.69310\n[50]\tvalidation_0-logloss:0.65477\tvalidation_1-logloss:0.68774\n[100]\tvalidation_0-logloss:0.62452\tvalidation_1-logloss:0.68518\n[150]\tvalidation_0-logloss:0.59875\tvalidation_1-logloss:0.68193\n[200]\tvalidation_0-logloss:0.57707\tvalidation_1-logloss:0.68189\n[250]\tvalidation_0-logloss:0.55640\tvalidation_1-logloss:0.68285\n[299]\tvalidation_0-logloss:0.53913\tvalidation_1-logloss:0.68248\n[0]\tvalidation_0-logloss:0.69242\tvalidation_1-logloss:0.69293\n[50]\tvalidation_0-logloss:0.65581\tvalidation_1-logloss:0.68546\n[100]\tvalidation_0-logloss:0.62555\tvalidation_1-logloss:0.68085\n[150]\tvalidation_0-logloss:0.60010\tvalidation_1-logloss:0.67434\n[200]\tvalidation_0-logloss:0.57710\tvalidation_1-logloss:0.67145\n[250]\tvalidation_0-logloss:0.55602\tvalidation_1-logloss:0.66991\n[299]\tvalidation_0-logloss:0.53786\tvalidation_1-logloss:0.67110\n[0]\tvalidation_0-logloss:0.69243\tvalidation_1-logloss:0.69297\n[50]\tvalidation_0-logloss:0.65671\tvalidation_1-logloss:0.68572\n[100]\tvalidation_0-logloss:0.62764\tvalidation_1-logloss:0.67963\n[150]\tvalidation_0-logloss:0.60170\tvalidation_1-logloss:0.67625\n[200]\tvalidation_0-logloss:0.57877\tvalidation_1-logloss:0.67249\n[250]\tvalidation_0-logloss:0.55878\tvalidation_1-logloss:0.67021\n[299]\tvalidation_0-logloss:0.54183\tvalidation_1-logloss:0.67010\n[0]\tvalidation_0-logloss:0.69227\tvalidation_1-logloss:0.69309\n[50]\tvalidation_0-logloss:0.65649\tvalidation_1-logloss:0.68544\n[100]\tvalidation_0-logloss:0.62611\tvalidation_1-logloss:0.68095\n[150]\tvalidation_0-logloss:0.59980\tvalidation_1-logloss:0.67595\n[200]\tvalidation_0-logloss:0.57742\tvalidation_1-logloss:0.67452\n[250]\tvalidation_0-logloss:0.55758\tvalidation_1-logloss:0.67215\n[299]\tvalidation_0-logloss:0.53984\tvalidation_1-logloss:0.67272\n"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "K = 10\n",
    "kf =  StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "models = []\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'eval_metric': 'logloss',\n",
    "    'n_estimators': 300,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.0001,\n",
    "    'min_child_weight': 5,\n",
    "    'colsample_bytree': 0.2,\n",
    "    'subsample': 1.0,\n",
    "    \"seed\":42,\n",
    "    'max_depth': 7\n",
    "}\n",
    "oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "oof_pred_ans = np.zeros_like(y, dtype=np.float)\n",
    "for train_index, test_index in kf.split(X,y):\n",
    "    x_train, x_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "    clf.fit(x_train, y_train\n",
    "            ,eval_set=[(x_train,y_train),(x_test, y_test)]\n",
    "            ,verbose=50)\n",
    "    pred_i = clf.predict_proba(x_test)[:, 1]\n",
    "    oof_pred[test_index] = pred_i\n",
    "    oof_pred_ans[test_index] = clf.predict(x_test)\n",
    "    models.append(clf)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.580459157879532\n0.33746130030959753\n0.5588235294117647\n"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.roc_auc_score(y, oof_pred)\n",
    "print(score)\n",
    "print(np.sum(oof_pred_ans == 0) / np.sum(oof_pred_ans >= 0))\n",
    "print(metrics.accuracy_score(y, oof_pred_ans, normalize=True, sample_weight=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}